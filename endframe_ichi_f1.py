import os
import sys
sys.path.append(os.path.abspath(os.path.realpath(os.path.join(os.path.dirname(__file__), './submodules/FramePack'))))

# WindowsÁí∞Â¢É„Åß loopÂÜçÁîüÊôÇ„Å´ [WinError 10054] „ÅÆ warning „ÅåÂá∫„Çã„ÅÆ„ÇíÂõûÈÅø„Åô„ÇãË®≠ÂÆö
import asyncio
if sys.platform in ('win32', 'cygwin'):
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

from diffusers_helper.hf_login import login

import os
import random
import time
import subprocess
import traceback  # „É≠„Ç∞Âá∫ÂäõÁî®
# „ÇØ„É≠„Çπ„Éó„É©„ÉÉ„Éà„Éï„Ç©„Éº„É†ÂØæÂøú„ÅÆ„Åü„ÇÅ„ÅÆÊù°‰ª∂‰ªò„Åç„Ç§„É≥„Éù„Éº„Éà
import yaml
import zipfile

import argparse

# PNG„É°„Çø„Éá„Éº„ÇøÂá¶ÁêÜ„É¢„Ç∏„É•„Éº„É´„ÅÆ„Ç§„É≥„Éù„Éº„Éà
import sys
sys.path.append(os.path.abspath(os.path.dirname(__file__)))
from eichi_utils.png_metadata import (
    embed_metadata_to_png, extract_metadata_from_png, extract_metadata_from_numpy_array,
    PROMPT_KEY, SEED_KEY, SECTION_PROMPT_KEY, SECTION_NUMBER_KEY
)

parser = argparse.ArgumentParser()
parser.add_argument('--share', action='store_true')
parser.add_argument("--server", type=str, default='127.0.0.1')
parser.add_argument("--port", type=int, default=8001)
parser.add_argument("--inbrowser", action='store_true')
parser.add_argument("--lang", type=str, default='ja', help="Language: ja, zh-tw, en")
args = parser.parse_args()

# Load translations from JSON files
from locales.i18n_extended import (set_lang, translate)
set_lang(args.lang)

try:
    import winsound
    HAS_WINSOUND = True
except ImportError:
    HAS_WINSOUND = False
import json
import traceback
from datetime import datetime, timedelta

if 'HF_HOME' not in os.environ:
    os.environ['HF_HOME'] = os.path.abspath(os.path.realpath(os.path.join(os.path.dirname(__file__), './hf_download')))
    print(translate("HF_HOME„ÇíË®≠ÂÆö: {0}").format(os.environ['HF_HOME']))
else:
    print(translate("Êó¢Â≠ò„ÅÆHF_HOME„Çí‰ΩøÁî®: {0}").format(os.environ['HF_HOME']))
temp_dir = "./temp_for_zip_section_info"

# LoRA„Çµ„Éù„Éº„Éà„ÅÆÁ¢∫Ë™ç
has_lora_support = False
try:
    import lora_utils
    has_lora_support = True
except ImportError:
    print(translate("LoRA„Çµ„Éù„Éº„Éà„ÅåÁÑ°Âäπ„Åß„ÅôÔºàlora_utils„É¢„Ç∏„É•„Éº„É´„Åå„Ç§„É≥„Çπ„Éà„Éº„É´„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„ÇìÔºâ"))

# Ë®≠ÂÆöÁÆ°ÁêÜ„ÅÆ„Ç§„É≥„Éù„Éº„Éà„Å®Ë™≠„ÅøËæº„Åø
from eichi_utils.settings_manager import load_app_settings_f1
saved_app_settings = load_app_settings_f1()

# Ë™≠„ÅøËæº„Çì„Å†Ë®≠ÂÆö„Çí„É≠„Ç∞„Å´Âá∫Âäõ
if saved_app_settings:
    pass
else:
    print(translate(" ‰øùÂ≠ò„Åï„Çå„ÅüË®≠ÂÆö„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„ÄÇ„Éá„Éï„Ç©„É´„ÉàË®≠ÂÆö„Çí‰ΩøÁî®„Åó„Åæ„Åô"))

# Ë®≠ÂÆö„É¢„Ç∏„É•„Éº„É´„Çí„Ç§„É≥„Éù„Éº„ÉàÔºà„É≠„Éº„Ç´„É´„É¢„Ç∏„É•„Éº„É´Ôºâ
import os.path
from eichi_utils.video_mode_settings import (
    VIDEO_MODE_SETTINGS, get_video_modes, get_video_seconds, get_important_keyframes,
    get_copy_targets, get_max_keyframes_count, get_total_sections, generate_keyframe_guide_html,
    handle_mode_length_change, process_keyframe_change, MODE_TYPE_NORMAL
    # F1„É¢„Éº„Éâ„Åß„ÅØ‰∏çË¶Å„Å™Ê©üËÉΩ„ÅÆ„Ç§„É≥„Éù„Éº„Éà„ÇíÂâäÈô§
)

# Ë®≠ÂÆöÁÆ°ÁêÜ„É¢„Ç∏„É•„Éº„É´„Çí„Ç§„É≥„Éù„Éº„Éà
from eichi_utils.settings_manager import (
    get_settings_file_path,
    get_output_folder_path,
    initialize_settings,
    load_settings,
    save_settings,
    open_output_folder
)

# „É≠„Ç∞ÁÆ°ÁêÜ„É¢„Ç∏„É•„Éº„É´„Çí„Ç§„É≥„Éù„Éº„Éà
from eichi_utils.log_manager import (
    enable_logging, disable_logging, is_logging_enabled, 
    get_log_folder, set_log_folder, open_log_folder,
    get_default_log_settings, load_log_settings, apply_log_settings
)

# „Éó„É™„Çª„ÉÉ„ÉàÁÆ°ÁêÜ„É¢„Ç∏„É•„Éº„É´„Çí„Ç§„É≥„Éù„Éº„Éà
from eichi_utils.preset_manager import (
    initialize_presets,
    load_presets,
    get_default_startup_prompt,
    save_preset,
    delete_preset
)

# „Ç≠„Éº„Éï„É¨„Éº„É†Âá¶ÁêÜ„É¢„Ç∏„É•„Éº„É´„Çí„Ç§„É≥„Éù„Éº„Éà
from eichi_utils.keyframe_handler import (
    ui_to_code_index,
    code_to_ui_index,
    unified_keyframe_change_handler,
    unified_input_image_change_handler
)

# Êã°Âºµ„Ç≠„Éº„Éï„É¨„Éº„É†Âá¶ÁêÜ„É¢„Ç∏„É•„Éº„É´„Çí„Ç§„É≥„Éù„Éº„Éà
from eichi_utils.keyframe_handler_extended import extended_mode_length_change_handler
import gradio as gr
# UIÈñ¢ÈÄ£„É¢„Ç∏„É•„Éº„É´„ÅÆ„Ç§„É≥„Éù„Éº„Éà
from eichi_utils.ui_styles import get_app_css
import torch
import einops
import safetensors.torch as sf
import numpy as np
import math

from PIL import Image
from diffusers import AutoencoderKLHunyuanVideo
from transformers import LlamaModel, CLIPTextModel, LlamaTokenizerFast, CLIPTokenizer
from diffusers_helper.hunyuan import encode_prompt_conds, vae_decode, vae_encode, vae_decode_fake
from diffusers_helper.utils import save_bcthw_as_mp4, crop_or_pad_yield_mask, soft_append_bcthw, resize_and_center_crop, state_dict_weighted_merge, state_dict_offset_merge, generate_timestamp
from diffusers_helper.models.hunyuan_video_packed import HunyuanVideoTransformer3DModelPacked
from diffusers_helper.pipelines.k_diffusion_hunyuan import sample_hunyuan
from diffusers_helper.memory import cpu, gpu, get_cuda_free_memory_gb, move_model_to_device_with_memory_preservation, offload_model_from_device_for_memory_preservation, fake_diffusers_current_device, DynamicSwapInstaller, unload_complete_models, load_model_as_complete
from diffusers_helper.thread_utils import AsyncStream, async_run
from diffusers_helper.gradio.progress_bar import make_progress_bar_css, make_progress_bar_html
from transformers import SiglipImageProcessor, SiglipVisionModel
from diffusers_helper.clip_vision import hf_clip_vision_encode
from diffusers_helper.bucket_tools import find_nearest_bucket

from eichi_utils.transformer_manager import TransformerManager
from eichi_utils.text_encoder_manager import TextEncoderManager

from eichi_utils.config_queue_manager import ConfigQueueManager

from pathlib import Path

current_ui_components = {}



free_mem_gb = get_cuda_free_memory_gb(gpu)
high_vram = free_mem_gb > 100

print(translate('Free VRAM {0} GB').format(free_mem_gb))
print(translate('High-VRAM Mode: {0}').format(high_vram))

# „É¢„Éá„É´„Çí‰∏¶Âàó„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åó„Å¶„Åä„Åè
from eichi_utils.model_downloader import ModelDownloader
ModelDownloader().download_f1()

# „Ç∞„É≠„Éº„Éê„É´„Å™„É¢„Éá„É´Áä∂ÊÖãÁÆ°ÁêÜ„Ç§„É≥„Çπ„Çø„É≥„Çπ„Çí‰ΩúÊàê
# F1„É¢„Éº„Éâ„Åß„ÅØuse_f1_model=True„ÇíÊåáÂÆö
transformer_manager = TransformerManager(device=gpu, high_vram_mode=high_vram, use_f1_model=True)
text_encoder_manager = TextEncoderManager(device=gpu, high_vram_mode=high_vram)

# ==============================================================================
# CONFIG QUEUE SYSTEM - MAIN INTEGRATION
# ==============================================================================


# Queue processing state tracking
config_queue_manager = None  # Initialized later in main code
current_loaded_config = None  # Currently loaded config name
queue_processing_active = False  # Global processing state flag
current_processing_config_name = None  # For video file naming
current_batch_progress = {"current": 0, "total": 0}  # Batch progress tracking
queue_ui_settings = None  # Captured UI settings for queue processing
pending_lora_config_data = None  # For delayed LoRA configuration loading

# Configuration constants for queue display
CONST_queued_shown_count = 5  # Number of queued items shown in status
CONST_latest_finish_count = 2  # Number of completed items shown in status

# Language-independent constants for LoRA config storage
LORA_MODE_DIRECTORY = "directory_selection"
LORA_MODE_UPLOAD = "file_upload"
LORA_NONE_OPTION = "none_option"

try:
    tokenizer = LlamaTokenizerFast.from_pretrained("hunyuanvideo-community/HunyuanVideo", subfolder='tokenizer')
    tokenizer_2 = CLIPTokenizer.from_pretrained("hunyuanvideo-community/HunyuanVideo", subfolder='tokenizer_2')
    vae = AutoencoderKLHunyuanVideo.from_pretrained("hunyuanvideo-community/HunyuanVideo", subfolder='vae', torch_dtype=torch.float16).cpu()

    # text_encoder„Å®text_encoder_2„ÅÆÂàùÊúüÂåñ
    if not text_encoder_manager.ensure_text_encoder_state():
        raise Exception(translate("text_encoder„Å®text_encoder_2„ÅÆÂàùÊúüÂåñ„Å´Â§±Êïó„Åó„Åæ„Åó„Åü"))
    text_encoder, text_encoder_2 = text_encoder_manager.get_text_encoders()

    # transformer„ÅÆÂàùÊúüÂåñ
    transformer_manager.ensure_download_models()
    transformer = transformer_manager.get_transformer()  # ‰ªÆÊÉ≥„Éá„Éê„Ç§„Çπ‰∏ä„ÅÆtransformer„ÇíÂèñÂæó

    # ‰ªñ„ÅÆ„É¢„Éá„É´„ÅÆË™≠„ÅøËæº„Åø
    feature_extractor = SiglipImageProcessor.from_pretrained("lllyasviel/flux_redux_bfl", subfolder='feature_extractor')
    image_encoder = SiglipVisionModel.from_pretrained("lllyasviel/flux_redux_bfl", subfolder='image_encoder', torch_dtype=torch.float16).cpu()
except Exception as e:
    print(translate("„É¢„Éá„É´Ë™≠„ÅøËæº„Åø„Ç®„É©„Éº: {0}").format(e))
    print(translate("„Éó„É≠„Ç∞„É©„É†„ÇíÁµÇ‰∫Ü„Åó„Åæ„Åô..."))
    import sys
    sys.exit(1)

vae.eval()
image_encoder.eval()

if not high_vram:
    vae.enable_slicing()
    vae.enable_tiling()

vae.to(dtype=torch.float16)
image_encoder.to(dtype=torch.float16)

vae.requires_grad_(False)
image_encoder.requires_grad_(False)

if not high_vram:
    # DynamicSwapInstaller is same as huggingface's enable_sequential_offload but 3x faster
    DynamicSwapInstaller.install_model(transformer, device=gpu) # „ÇØ„É©„Çπ„ÇíÊìç‰Ωú„Åô„Çã„ÅÆ„Åß‰ªÆÊÉ≥„Éá„Éê„Ç§„Çπ‰∏ä„ÅÆtransformer„Åß„ÇÇOK
else:
    image_encoder.to(gpu)
    vae.to(gpu)

stream = AsyncStream()

# Ë®≠ÂÆöÁÆ°ÁêÜ„É¢„Ç∏„É•„Éº„É´„Çí„Ç§„É≥„Éù„Éº„Éà
from eichi_utils.settings_manager import (
    get_settings_file_path,
    get_output_folder_path,
    initialize_settings,
    load_settings,
    save_settings,
    open_output_folder
)

# „Éï„Ç©„É´„ÉÄÊßãÈÄ†„ÇíÂÖà„Å´ÂÆöÁæ©
webui_folder = os.path.dirname(os.path.abspath(__file__))

# Ë®≠ÂÆö‰øùÂ≠òÁî®„Éï„Ç©„É´„ÉÄ„ÅÆË®≠ÂÆö
settings_folder = os.path.join(webui_folder, 'settings')
os.makedirs(settings_folder, exist_ok=True)

# Ë®≠ÂÆö„Éï„Ç°„Ç§„É´ÂàùÊúüÂåñ
initialize_settings()

# LoRA„Éó„É™„Çª„ÉÉ„Éà„ÅÆÂàùÊúüÂåñ
from eichi_utils.lora_preset_manager import initialize_lora_presets
initialize_lora_presets()

# „Éô„Éº„Çπ„Éë„Çπ„ÇíÂÆöÁæ©
base_path = os.path.dirname(os.path.abspath(__file__))

# Ë®≠ÂÆö„Åã„ÇâÂá∫Âäõ„Éï„Ç©„É´„ÉÄ„ÇíÂèñÂæó
app_settings = load_settings()
output_folder_name = app_settings.get('output_folder', 'outputs')
print(translate("Ë®≠ÂÆö„Åã„ÇâÂá∫Âäõ„Éï„Ç©„É´„ÉÄ„ÇíË™≠„ÅøËæº„Åø: {0}").format(output_folder_name))

# „É≠„Ç∞Ë®≠ÂÆö„ÇíË™≠„ÅøËæº„ÅøÈÅ©Áî®
log_settings = app_settings.get('log_settings', get_default_log_settings())
print(translate("„É≠„Ç∞Ë®≠ÂÆö„ÇíË™≠„ÅøËæº„Åø: ÊúâÂäπ={0}, „Éï„Ç©„É´„ÉÄ={1}").format(
    log_settings.get('log_enabled', False), 
    log_settings.get('log_folder', 'logs')
))
if log_settings.get('log_enabled', False):
    # ÁèæÂú®„ÅÆ„Éï„Ç°„Ç§„É´Âêç„ÇíÊ∏°„Åô
    enable_logging(log_settings.get('log_folder', 'logs'), source_name="endframe_ichi_f1")
    print(translate("„É≠„Ç∞Âá∫Âäõ„ÇíÊúâÂäπÂåñ„Åó„Åæ„Åó„Åü"))

# „Ç≠„É•„ÉºÈñ¢ÈÄ£„ÅÆ„Ç∞„É≠„Éº„Éê„É´Â§âÊï∞
queue_enabled = False  # „Ç≠„É•„ÉºÊ©üËÉΩ„ÅÆÊúâÂäπ/ÁÑ°Âäπ„Éï„É©„Ç∞
queue_type = "prompt"  # „Ç≠„É•„Éº„ÅÆ„Çø„Ç§„ÉóÔºà"prompt" „Åæ„Åü„ÅØ "image"Ôºâ
prompt_queue_file_path = None  # „Éó„É≠„É≥„Éó„Éà„Ç≠„É•„Éº„ÅÆ„Éï„Ç°„Ç§„É´„Éë„Çπ
image_queue_files = []  # „Ç§„É°„Éº„Ç∏„Ç≠„É•„Éº„ÅÆ„Éï„Ç°„Ç§„É´„É™„Çπ„Éà
input_folder_name_value = app_settings.get('input_folder', 'inputs')  # ÂÖ•Âäõ„Éï„Ç©„É´„ÉÄÂêç„ÅÆË®≠ÂÆöÂÄ§

# ÂÖ•Âäõ„Éï„Ç©„É´„ÉÄ„ÇÇÂ≠òÂú®Á¢∫Ë™çÔºà‰ΩúÊàê„ÅØ„Éú„Çø„É≥Êäº‰∏ãÊôÇ„ÅÆ„ÅøÔºâ
input_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), input_folder_name_value)
print(translate("Ë®≠ÂÆö„Åã„ÇâÂÖ•Âäõ„Éï„Ç©„É´„ÉÄ„ÇíË™≠„ÅøËæº„Åø: {0}").format(input_folder_name_value))

# Âá∫Âäõ„Éï„Ç©„É´„ÉÄ„ÅÆ„Éï„É´„Éë„Çπ„ÇíÁîüÊàê
outputs_folder = get_output_folder_path(output_folder_name)
os.makedirs(outputs_folder, exist_ok=True)

# ==============================================================================
# CORE QUEUE CONFIGURATION FUNCTIONS
# ==============================================================================

def get_current_ui_settings_for_queue():

    global current_ui_components
    
    try:
        settings = {}
        
        # Helper function to safely get component values with explicit type conversion
        def get_component_value(component_name, default_value, value_type=None):
            if component_name in current_ui_components:
                component = current_ui_components[component_name]
                if hasattr(component, 'value'):
                    value = component.value
                    print(translate("üîç Getting {0}: {1} (type: {2})").format(component_name, value, type(value)))
                    # Type conversion if specified
                    if value_type == bool:
                        return bool(value)
                    elif value_type == int:
                        try:
                            return int(float(value)) if value is not None else default_value
                        except (ValueError, TypeError):
                            print(translate("‚ö†Ô∏è Error converting {0} to int: {1}, using default: {2}").format(component_name, value, default_value))
                            return default_value
                    elif value_type == float:
                        try:
                            return float(value) if value is not None else default_value
                        except (ValueError, TypeError):
                            print(translate("‚ö†Ô∏è Error converting {0} to float: {1}, using default: {2}").format(component_name, value, default_value))
                            return default_value
                    else:
                        return value if value is not None else default_value
                else:
                    print(translate("‚ö†Ô∏è Component {0} has no value attribute").format(component_name))
                    return default_value
            else:
                print(translate("‚ö†Ô∏è Component {0} not found in registered components").format(component_name))
                return default_value
        
        # ===== DURATION SETTINGS - DETAILED LOGGING =====
        
        # Get slider value with detailed logging
        total_second_length_value = get_component_value('total_second_length', 1, int)
        settings['total_second_length'] = max(1, total_second_length_value)
        
        # Get radio value for comparison
        length_radio_value = get_component_value('length_radio', translate("1Áßí"))
        
        print(translate("üïí Duration settings for queue:"))
        print(translate("   length_radio: '{0}' (for reference only)").format(length_radio_value))
        print(translate("   total_second_length slider: {0}s ‚Üí final: {1}s").format(total_second_length_value, settings['total_second_length']))
        
        # Determine duration source for logging
        duration_source = "total_second_length"  # Since we're prioritizing the slider
        
        # Frame size settings
        frame_size_setting = get_component_value('frame_size_radio', translate("1Áßí (33„Éï„É¨„Éº„É†)"))
        settings['frame_size_setting'] = frame_size_setting
        
        # Convert frame size to latent_window_size
        if frame_size_setting == translate("0.5Áßí (17„Éï„É¨„Éº„É†)"):
            settings['latent_window_size'] = 4.5
        else:
            settings['latent_window_size'] = 9
        
        print(translate("üé¨ Frame settings: {0} ‚Üí latent_window_size={1}").format(frame_size_setting, settings['latent_window_size']))
        
        # ===== QUALITY SETTINGS =====
        settings['steps'] = get_component_value('steps', 25, int)
        settings['cfg'] = get_component_value('cfg', 1.0, float)
        settings['gs'] = get_component_value('gs', 10, float)
        settings['rs'] = get_component_value('rs', 0.0, float)
        settings['resolution'] = get_component_value('resolution', 640, int)
        settings['mp4_crf'] = get_component_value('mp4_crf', 16, int)
        
        # ===== GENERATION SETTINGS =====
        base_seed = get_component_value('seed', 1, int)
        use_random_seed = get_component_value('use_random_seed', False, bool)
        
        if use_random_seed:
            import random
            settings['seed'] = random.randint(0, 2**32 - 1)
            print(translate("üé≤ Generated new random seed for queue item: {0}").format(settings['seed']))
        else:
            settings['seed'] = base_seed
            
        settings['use_random_seed'] = False  # Always False for queue processing since we handle it above
        settings['use_teacache'] = get_component_value('use_teacache', True, bool)
        settings['image_strength'] = get_component_value('image_strength', 1.0, float)
        settings['fp8_optimization'] = get_component_value('fp8_optimization', True, bool)
        
        # ===== BATCH COUNT - EXPLICIT HANDLING =====
        batch_count_raw = get_component_value('batch_count', 1, int)
        # Ensure it's definitely an integer and within valid range
        batch_count_final = max(1, min(int(batch_count_raw), 100))
        settings['batch_count'] = batch_count_final
        
        print(translate("üî¢ Batch count processing: raw={0} (type: {1}) ‚Üí final={2} (type: {3})").format(batch_count_raw, type(batch_count_raw), batch_count_final, type(batch_count_final)))
        
        
        # ===== SYSTEM SETTINGS =====
        settings['gpu_memory_preservation'] = get_component_value('gpu_memory_preservation', 6.0, float)
        
        # ===== OUTPUT SETTINGS =====
        settings['keep_section_videos'] = get_component_value('keep_section_videos', False, bool)
        settings['save_section_frames'] = get_component_value('save_section_frames', False, bool)
        settings['save_tensor_data'] = get_component_value('save_tensor_data', False, bool)
        settings['frame_save_mode'] = get_component_value('frame_save_mode', translate("‰øùÂ≠ò„Åó„Å™„ÅÑ"))
        settings['output_dir'] = get_component_value('output_dir', "outputs")
        settings['alarm_on_completion'] = get_component_value('alarm_on_completion', False, bool)
        
        # ===== F1 MODE SETTINGS =====
        settings['all_padding_value'] = get_component_value('all_padding_value', 1.0, float)
        settings['use_all_padding'] = get_component_value('use_all_padding', False, bool)
        
        # ===== FIXED VALUES FOR QUEUE PROCESSING =====
        settings['n_prompt'] = ""  # Ignored in F1 mode
        settings['tensor_data_input'] = None  # Not supported in queue
        settings['use_queue'] = False
        settings['prompt_queue_file'] = None
        settings['batch_count'] = 1
        settings['save_settings_on_start'] = False
        
        print(translate("üìã Queue settings summary:"))
        print(translate("   Duration: {0}s ({1}), Frames: {2}").format(settings['total_second_length'], duration_source, frame_size_setting))
        print(translate("   Quality: steps={0}, CFG={1}, Distilled={2}").format(settings['steps'], settings['cfg'], settings['gs']))
        print(translate("   Output: resolution={0}, CRF={1}").format(settings['resolution'], settings['mp4_crf']))
        print(translate("   Generation: seed={0} (random: {1})").format(settings['seed'], use_random_seed))
        print(translate("   Performance: TeaCache={0}, FP8={1}").format(settings['use_teacache'], settings['fp8_optimization']))
        
        return settings
        
    except Exception as e:
        print(translate("‚ùå Error getting current UI settings: {0}").format(e))
        import traceback
        traceback.print_exc()
        
        # Return safe defaults
        return {
            'total_second_length': 1,
            'latent_window_size': 9,
            'frame_size_setting': translate("1Áßí (33„Éï„É¨„Éº„É†)"),
            'steps': 25,
            'cfg': 1.0,
            'gs': 10,
            'rs': 0.0,
            'resolution': 640,
            'mp4_crf': 16,
            'seed': 1,
            'use_random_seed': False,
            'use_teacache': True,
            'image_strength': 1.0,
            'fp8_optimization': True,
            'gpu_memory_preservation': 6.0,
            'keep_section_videos': False,
            'save_section_frames': False,
            'save_tensor_data': False,
            'frame_save_mode': translate("‰øùÂ≠ò„Åó„Å™„ÅÑ"),
            'output_dir': "outputs",
            'alarm_on_completion': False,
            'all_padding_value': 1.0,
            'use_all_padding': False,
            'n_prompt': "",
            'tensor_data_input': None,
            'use_queue': False,
            'prompt_queue_file': None,
            'batch_count': 1,
            'save_settings_on_start': False
        }

def cancel_operation_handler():
    return (
        "‚ùå " + translate("Operation cancelled"),
        gr.update(),  # Don't change config dropdown
        gr.update(),  # Don't change queue status
        gr.update(visible=False),  # Hide confirmation group
        None  # Clear operation data
    )

def merged_refresh_handler_standardized():
    try:
        if config_queue_manager is None:
            return translate("‚ùå Config queue manager not initialized"), gr.update(), gr.update()
        
        # Refresh config list
        available_configs = config_queue_manager.get_available_configs()
        
        # Get enhanced queue status with auto-correction
        queue_status = config_queue_manager.get_queue_status()
        
        # Auto-correction logic (same as before)
        global queue_processing_active
        manager_processing = config_queue_manager.is_processing
        has_current_work = bool(queue_status.get('current_config'))
        has_queued_items = queue_status.get('queue_count', 0) > 0
        
        needs_correction = False
        
        if manager_processing and not has_current_work and not has_queued_items:
            print(translate("üîß Merged refresh: Manager processing but no work - correcting"))
            config_queue_manager.is_processing = False
            config_queue_manager.current_config = None
            queue_processing_active = False
            needs_correction = True
        
        if queue_processing_active and not manager_processing:
            print(translate("üîß Merged refresh: Global active but manager idle - syncing"))
            queue_processing_active = False
            needs_correction = True
        
        if needs_correction:
            queue_status = config_queue_manager.get_queue_status()
        
        # Use the same enhanced formatting function for consistency
        status_text = format_queue_status_with_batch_progress(queue_status)
        
        # Add correction note if needed
        if needs_correction:
            status_text += "\nüîß Auto-corrected processing state"
        
        queue_count = queue_status['queue_count']
        print(translate("üîÑ Merged refresh completed: {0} configs, {1} queued").format(len(available_configs), queue_count))
        
        return (
            translate("‚úÖ Refreshed: {0} configs, {1} queued").format(len(available_configs), queue_count),
            gr.update(choices=available_configs),
            gr.update(value=status_text)
        )
        
    except Exception as e:
        return translate("‚ùå Error during refresh: {0}").format(str(e)), gr.update(), gr.update()

# ==============================================================================
# QUEUE CONTROL HANDLERS
# ==============================================================================

def queue_config_handler_with_confirmation(config_dropdown):

    global current_loaded_config
    
    config_name = current_loaded_config or config_dropdown
    if not config_name:
        return "‚ùå Error: No config loaded", gr.update(), gr.update(visible=False), None
    
    if config_queue_manager is None:
        return translate("‚ùå Error: Config queue manager not initialized"), gr.update(), gr.update(visible=False), None

    # Check if already in queue
    queue_file = os.path.join(config_queue_manager.queue_dir, f"{config_name}.json")
    
    if os.path.exists(queue_file):
        # Store operation details for confirmation
        operation_data = {
            'type': 'queue_overwrite',
            'config_name': config_name
        }
        
        # Show confirmation message
        confirmation_msg = f"""
        <div style="padding: 20px; border-radius: 10px; background-color: #fff3cd; border: 1px solid #ffeaa7; margin: 10px 0;">
            <h3 style="color: #856404; margin: 0 0 10px 0;">‚ö†Ô∏è {translate('Queue Overwrite Confirmation')}</h3>
            <p style="margin: 10px 0;">{translate('Config "{0}" is already in the queue. Do you want to overwrite it with the current config settings?').format(config_name)}</p>
            <p style="margin: 10px 0; font-weight: bold; color: #856404;">
                {translate('This will replace the queued config with your current settings.')}
            </p>
        </div>
        """
        
        return (
            confirmation_msg,
            gr.update(),
            gr.update(visible=True),  # Show confirmation group
            operation_data  # Store operation data
        )
    else:
        # Not in queue, proceed normally
        success, message = config_queue_manager.queue_config(config_name)
        
        if success:
            queue_status = config_queue_manager.get_queue_status()
            status_text = format_queue_status_with_batch_progress(queue_status)
            return f"‚úÖ {message}", gr.update(value=status_text), gr.update(visible=False), None
        else:
            return f"‚ùå {message}", gr.update(), gr.update(visible=False), None
  
def stop_queue_processing_handler_fixed():

    global queue_processing_active

    if config_queue_manager is None:
        return translate("‚ùå Error: Config queue manager not initialized"), gr.update(), gr.update(visible=True), gr.update(visible=True)
    
    if not queue_processing_active and not config_queue_manager.is_processing:
        return translate("‚ùå Queue processing is not running"), gr.update(), gr.update(visible=True), gr.update(visible=True)
    
    print(translate("üõë Stopping queue processing..."))
    
    success, message = config_queue_manager.stop_queue_processing()
    
    if success:
        # Force reset both flags
        queue_processing_active = False
        config_queue_manager.is_processing = False
        config_queue_manager.current_config = None
        print(translate("‚úÖ Queue processing stopped and flags reset"))
        
    queue_status = config_queue_manager.get_queue_status()
    status_text = format_queue_status_with_batch_progress(queue_status)
    
    # Return with visibility restored
    return message, gr.update(value=status_text), gr.update(visible=True), gr.update(visible=True)

def clear_queue_handler():

    if config_queue_manager is None:  # ‚Üê Add this line
        return translate("Error: Config queue manager not initialized/clear_queue_handler"), gr.update(), gr.update()    
    
    success, message = config_queue_manager.clear_queue()
    
    queue_status = config_queue_manager.get_queue_status()
    status_text = format_queue_status_with_batch_progress(queue_status)
    
    return message, gr.update(value=status_text)

# ==============================================================================
# CONFIG FILE OPERATIONS (SAVE/LOAD/DELETE)
# ==============================================================================

def save_current_config_handler_v3(config_name_input, add_timestamp, input_image, prompt, use_lora, lora_mode, 
                                  lora_dropdown1, lora_dropdown2, lora_dropdown3, lora_files, 
                                  lora_files2, lora_files3, lora_scales_text):

    global current_loaded_config
    
    try:
        # Validate inputs
        if not input_image:
            return translate("‚ùå Error: No image selected"), gr.update(), gr.update(), gr.update(visible=False), None
            
        if not prompt or not prompt.strip():
            return translate("‚ùå Error: No prompt entered"), gr.update(), gr.update(), gr.update(visible=False), None
        
        if config_queue_manager is None:
            return translate("‚ùå Error: Config queue manager not initialized"), gr.update(), gr.update(), gr.update(visible=False), None
        
        # Get the config name to use
        config_name_to_use = config_name_input.strip() if config_name_input and config_name_input.strip() else ""
        
        # Check if config already exists (only relevant when NOT adding timestamp)
        will_overwrite = False
        if config_name_to_use and not add_timestamp:
            will_overwrite = config_queue_manager.config_exists(config_name_to_use)
        
        if will_overwrite:
            # Store operation details for confirmation
            operation_data = {
                'type': 'overwrite_exact',
                'config_name': config_name_to_use,
                'config_name_input': config_name_input,
                'add_timestamp': add_timestamp,
                'input_image': input_image,
                'prompt': prompt,
                'use_lora': use_lora,
                'lora_mode': lora_mode,
                'lora_dropdown1': lora_dropdown1,
                'lora_dropdown2': lora_dropdown2,
                'lora_dropdown3': lora_dropdown3,
                'lora_files': lora_files,
                'lora_files2': lora_files2,
                'lora_files3': lora_files3,
                'lora_scales_text': lora_scales_text
            }
            
            # Show confirmation message
            confirmation_msg = f"""
            <div style="padding: 20px; border-radius: 10px; background-color: #fff3cd; border: 1px solid #ffeaa7; margin: 10px 0;">
                <h3 style="color: #856404; margin: 0 0 10px 0;">‚ö†Ô∏è {translate('Overwrite Confirmation')}</h3>
                <p style="margin: 10px 0;">{translate('Config file "{0}.json" already exists. Do you want to overwrite it?').format(config_name_to_use)}</p>
                <p style="margin: 10px 0; font-weight: bold; color: #856404;">
                    {translate('Use the buttons above to confirm or cancel the operation.')}
                </p>
            </div>
            """
            
            return (
                confirmation_msg, 
                gr.update(), 
                gr.update(), 
                gr.update(visible=True),  # Show confirmation group
                operation_data  # Store operation data
            )
        
        # No overwrite needed - proceed directly
        return perform_save_operation_v3(
            config_name_input, add_timestamp, input_image, prompt, use_lora, lora_mode,
            lora_dropdown1, lora_dropdown2, lora_dropdown3, lora_files,
            lora_files2, lora_files3, lora_scales_text
        )
        
    except Exception as e:
        return f"‚ùå Error saving config: {str(e)}", gr.update(), gr.update(), gr.update(visible=False), None
    
def perform_save_operation_v3(config_name_input, add_timestamp, input_image, prompt, use_lora, lora_mode,
                            lora_dropdown1, lora_dropdown2, lora_dropdown3, lora_files,
                            lora_files2, lora_files3, lora_scales_text):

    global current_loaded_config

    try:
        config_name_to_use = config_name_input.strip() if config_name_input and config_name_input.strip() else ""
        
        # Get LoRA settings (with language-independent storage and auto-conversion)
        lora_settings = get_current_lora_settings(
            use_lora, lora_mode, lora_dropdown1, lora_dropdown2, lora_dropdown3,
            lora_files, lora_files2, lora_files3, lora_scales_text
        )
        
        # Save config with timestamp option
        success, message = config_queue_manager.save_config_with_timestamp_option(
            config_name_to_use, input_image, prompt, lora_settings, add_timestamp, other_params=None
        )
        
        if success:
            # FIXED: Parse the message to extract ONLY the config name, not system messages
            # Expected message format: "Config saved: {actual_name}"
            actual_config_name = config_name_to_use
            
            # Parse the clean config name from the success message
            if ": " in message:
                # Extract everything after "Config saved: "
                temp_name = message.split(": ")[1].strip()
                # Remove any system messages that might have been concatenated
                if " (" in temp_name:
                    # Remove everything from the first parenthesis onwards
                    actual_config_name = temp_name.split(" (")[0].strip()
                else:
                    actual_config_name = temp_name
            
            # VALIDATION: Ensure the config name is clean and doesn't contain system messages
            if "(" in actual_config_name or ")" in actual_config_name:
                print(translate(f"‚ö†Ô∏è Warning: Config name contains parentheses, cleaning: '{actual_config_name}'"))
                # Remove everything from the first parenthesis onwards
                actual_config_name = actual_config_name.split("(")[0].strip()
            
            print(translate("‚úÖ Config saved successfully: '{0}' (from message: '{1}')").format(actual_config_name, message))
            
            current_loaded_config = actual_config_name
            
            # CRITICAL FIX: Always refresh the available configs list after saving
            # This ensures the dropdown includes any newly created configs (including case variations)
            available_configs = config_queue_manager.get_available_configs()
            
            # CASE-SENSITIVE CHECK: Ensure the actual_config_name exists in the choices
            # This handles case-sensitive file systems where "abc" and "ABC" are different
            if actual_config_name not in available_configs:
                print(translate("‚ö†Ô∏è Warning: Saved config '{0}' not found in available configs").format(actual_config_name))
                print(translate("   Available configs: {0}...").format(available_configs[:10]))  # Show first 10 for debugging
                
                # Try case-insensitive search as fallback
                for config in available_configs:
                    if config.lower() == actual_config_name.lower():
                        print(f"   Found case-insensitive match: '{config}'")
                        actual_config_name = config
                        current_loaded_config = config
                        break
            
            queue_status = config_queue_manager.get_queue_status()
            status_text = format_queue_status_with_batch_progress(queue_status)
            
            # Format user message with timestamp info - keep it separate from config name
            user_message = ""
            if add_timestamp and config_name_to_use:
                user_message = f"‚úÖ {translate('Config saved with timestamp')}: {actual_config_name}.json"
            elif not add_timestamp and config_name_to_use:
                user_message = f"‚úÖ {translate('Config saved with exact name')}: {actual_config_name}.json"
            else:
                user_message = f"‚úÖ {translate('Config saved with auto-generated name')}: {actual_config_name}.json"
            
            # Add LoRA conversion notification if applicable (but separate from config name)
            if lora_settings.get("lora_mode_key") == LORA_MODE_DIRECTORY and lora_settings.get("lora_files"):
                # If we have LoRA files, show what was configured
                lora_files_list = lora_settings.get("lora_files", [])
                if lora_files_list:
                    filenames = [os.path.basename(path) for path in lora_files_list]
                    user_message += translate("\nüì¶ LoRA files configured: {0}").format(', '.join(filenames))
            
            return (
                user_message,
                gr.update(choices=available_configs, value=actual_config_name),  # Use CLEAN config name with refreshed choices
                gr.update(value=status_text),
                gr.update(visible=False),  # Hide confirmation group
                None  # Clear operation data
            )
        else:
            return translate("‚ùå {0}").format(message), gr.update(), gr.update(), gr.update(visible=False), None
            
    except Exception as e:
        return translate("‚ùå Error saving config: {0}").format(str(e)), gr.update(), gr.update(), gr.update(visible=False), None

def load_config_with_delayed_lora_application_fixed(config_name):

    global pending_lora_config_data, current_loaded_config
    
    if not config_name or config_queue_manager is None:
        return [
            translate("Error: Config queue manager not initialized"),
            gr.update(), gr.update(), gr.update(), gr.update(), 
            gr.update(), gr.update(), gr.update(), gr.update(), gr.update()
        ]
    
    success, config_data, message = config_queue_manager.load_config_for_editing(config_name)
    
    if success:
        current_loaded_config = config_name
        
        image_path = config_data.get('image_path')
        prompt = config_data.get('prompt', '')
        lora_settings = config_data.get('lora_settings', {})
        
        use_lora = lora_settings.get('use_lora', False)
        
        # Convert language-independent key to current language
        lora_mode_key = lora_settings.get('lora_mode_key')
        if lora_mode_key:
            lora_mode = get_lora_mode_text(lora_mode_key)
        else:
            # Fallback: try to convert old language-dependent format
            old_lora_mode = lora_settings.get('lora_mode', translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû"))
            lora_mode = translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû")  # Safe default
            print(translate("üì¶ Config uses old language-dependent format, using default: {0}").format(lora_mode))
        
        print(translate("üìÇ Loading config: {0}").format(config_name))
        print(translate("    use_lora: {0}, lora_mode: {1}").format(use_lora, lora_mode))
        
        # Handle LoRA configuration (now always directory mode due to auto-conversion)
        if use_lora and lora_mode == translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû"):
            # FIXED: Always scan directory first to ensure we have current choices
            choices = scan_lora_directory()
            print(translate("üì¶ Scanned LoRA directory, found {0} choices").format(len(choices)))
            
            # Try new format first (language-independent)
            lora_dropdown_files = lora_settings.get('lora_dropdown_files')
            if lora_dropdown_files:
                print(translate("üì¶ Using language-independent format"))
                lora_dropdown_values = []
                
                for dropdown_file in lora_dropdown_files[:3]:
                    if dropdown_file == LORA_NONE_OPTION:
                        # FIXED: Handle both constants and string literals
                        lora_dropdown_values.append(translate("„Å™„Åó"))
                    elif dropdown_file in choices:
                        lora_dropdown_values.append(dropdown_file)
                    else:
                        # FIXED: If file not found, use "none" instead of the missing file
                        print(translate("‚ö†Ô∏è LoRA file not found in directory: {0}, using 'none'").format(dropdown_file))
                        lora_dropdown_values.append(translate("„Å™„Åó"))
                
                # Pad with "none" if needed
                while len(lora_dropdown_values) < 3:
                    lora_dropdown_values.append(translate("„Å™„Åó"))
                
                applied_files = [f for f in lora_dropdown_values if f != translate("„Å™„Åó")]
                
            else:
                # Fallback: use old format (file paths)
                print(translate("üì¶ Using fallback file path method"))
                lora_files = lora_settings.get('lora_files', [])
                if lora_files:
                    # FIXED: Use enhanced validation in apply_lora_config_to_dropdowns
                    choices, lora_dropdown_values, applied_files = apply_lora_config_to_dropdowns_safe(lora_files, choices)
                else:
                    lora_dropdown_values = [translate("„Å™„Åó")] * 3
                    applied_files = []
            
            if applied_files:
                print(translate("‚úÖ Applied LoRA files: {0}").format(applied_files))
                
                # Store for potential reuse
                pending_lora_config_data = {
                    'files': lora_settings.get('lora_files', []),
                    'scales': lora_settings.get('lora_scales', '0.8,0.8,0.8'),
                    'mode': lora_mode,
                    'config_name': config_name,
                    'applied_values': lora_dropdown_values
                }
                
                # FIXED: Return with proper choices and values, ensuring all values are in choices
                return [
                    translate("‚úÖ Loaded config: {0} (LoRA: {1}, Str: {2})").format(config_name, ', '.join(applied_files), lora_settings.get('lora_scales')),
                    gr.update(value=image_path if image_path and os.path.exists(image_path) else None),
                    gr.update(value=prompt),
                    gr.update(value=use_lora),
                    gr.update(value=lora_mode),
                    gr.update(value=lora_settings.get('lora_scales', '0.8,0.8,0.8')),
                    gr.update(choices=choices, value=lora_dropdown_values[0] if lora_dropdown_values[0] in choices else choices[0]),
                    gr.update(choices=choices, value=lora_dropdown_values[1] if lora_dropdown_values[1] in choices else choices[0]),
                    gr.update(choices=choices, value=lora_dropdown_values[2] if lora_dropdown_values[2] in choices else choices[0]),
                    gr.update(value=config_name)
                ]
            else:
                print(translate("üì¶ Config has LoRA enabled but no files"))
                pending_lora_config_data = None
                # FIXED: Still return proper choices to avoid warnings
                return [
                    translate("‚úÖ Loaded config: {0}").format(config_name),
                    gr.update(value=image_path if image_path and os.path.exists(image_path) else None),
                    gr.update(value=prompt),
                    gr.update(value=use_lora),
                    gr.update(value=lora_mode),
                    gr.update(value=lora_settings.get('lora_scales', '0.8,0.8,0.8')),
                    gr.update(choices=choices, value=choices[0] if choices else translate("„Å™„Åó")),
                    gr.update(choices=choices, value=choices[0] if choices else translate("„Å™„Åó")),
                    gr.update(choices=choices, value=choices[0] if choices else translate("„Å™„Åó")),
                    gr.update(value=config_name)
                ]
        else:
            print(translate("üì¶ No LoRA configuration needed"))
            pending_lora_config_data = None

        # Default return for non-LoRA configs
        return [
            translate("‚úÖ Loaded config: {0}").format(config_name),
            gr.update(value=image_path if image_path and os.path.exists(image_path) else None),
            gr.update(value=prompt),
            gr.update(value=use_lora),
            gr.update(value=lora_mode),
            gr.update(value=lora_settings.get('lora_scales', '0.8,0.8,0.8')),
            gr.update(), gr.update(), gr.update(),
            gr.update(value=config_name)
        ]
    else:
        pending_lora_config_data = None
        return [
            translate("‚ùå {0}").format(message), gr.update(), gr.update(), gr.update(), gr.update(),
            gr.update(), gr.update(), gr.update(), gr.update(), gr.update()
        ]

def delete_config_handler_v2(config_dropdown):

    if not config_dropdown:
        return translate("‚ùå No config selected for deletion"), gr.update(), gr.update(), gr.update(visible=False), None
    
    if config_queue_manager is None:
        return translate("‚ùå Config queue manager not initialized"), gr.update(), gr.update(), gr.update(visible=False), None
    
    # Check if file actually exists
    if not config_queue_manager.config_exists(config_dropdown):
        available_configs = config_queue_manager.get_available_configs()
        queue_status = config_queue_manager.get_queue_status()
        status_text = format_queue_status_with_batch_progress(queue_status)
        
        return (
            f"‚ùå Config file {config_dropdown}.json not found (refreshing list)", 
            gr.update(choices=available_configs, value=None), 
            gr.update(value=status_text), 
            gr.update(visible=False),
            None
        )
    
    # Store operation details for confirmation
    operation_data = {
        'type': 'delete',
        'config_name': config_dropdown
    }
    
    # Show confirmation message
    confirmation_msg = f"""
    <div style="padding: 20px; border-radius: 10px; background-color: #f8d7da; border: 1px solid #f5c6cb; margin: 10px 0;">
        <h3 style="color: #721c24; margin: 0 0 10px 0;">üóëÔ∏è {translate('Delete Confirmation')}</h3>
        <p style="margin: 10px 0;">{translate('Are you sure you want to delete config file "{0}.json"? This action cannot be undone.').format(config_dropdown)}</p>
        <p style="margin: 10px 0; font-weight: bold; color: #721c24;">
            {translate('Use the buttons below to confirm or cancel the operation.')}
        </p>
    </div>
    """
    
    return (
        confirmation_msg,
        gr.update(),
        gr.update(),
        gr.update(visible=True),  # Show confirmation group
        operation_data  # Store operation data - ONLY 5 VALUES RETURNED
    )

# ==============================================================================
# INTEGRATION WITH MANUAL GENERATION SYSTEM
# ==============================================================================

def validate_and_process_with_queue_check(*args):

    global queue_processing_active
    
    # Check if queue processing is active
    if queue_processing_active or (config_queue_manager and config_queue_manager.is_processing):
        # Return error message with button states (7 outputs to match start_button.click)
        yield (
            gr.skip(),  # result_video
            gr.update(visible=False),  # preview_image
            "Cannot start manual generation: Queue processing is active",  # progress_desc
            '<div style="color: red;">Queue processing is running. Please wait for completion or stop the queue.</div>',  # progress_bar
            gr.update(interactive=False, value=translate("ÈòüÂàóÂ§ÑÁêÜ‰∏≠ÔºåÊâãÂä®ÁîüÊàêÂ∑≤Á¶ÅÁî®")),  # start_button
            gr.update(interactive=False),  # end_button
            gr.update(interactive=False, value=translate("ÈòüÂàóÂ§ÑÁêÜ‰∏≠...")),  # queue_start_button
            gr.update()  # seed
        )
        return
        
    # If no queue processing, proceed with normal validation
    for result in validate_and_process(*args):
        # result is a tuple: (video, preview, desc, progress, start_btn, end_btn, seed)
        if len(result) >= 6:
            video, preview, desc, progress, start_btn, end_btn = result[:6]
            seed_update = result[6] if len(result) > 6 else gr.update()
            
            # During manual generation, manage queue start button state
            if isinstance(start_btn, dict) and not start_btn.get('interactive', True):
                # Manual generation is running, disable queue start
                queue_start_state = gr.update(interactive=False, value=translate("ÊâãÂä®ÁîüÊàê‰∏≠..."))
            else:
                # Manual generation finished, re-enable queue start
                queue_start_state = gr.update(interactive=True, value=translate("‚ñ∂Ô∏è Start Queue"))
            
            # Return 8 outputs to match the expected outputs
            yield (video, preview, desc, progress, start_btn, end_btn, queue_start_state, seed_update)
        else:
            # Fallback for unexpected result format
            yield result + (gr.update(),) * (8 - len(result))

def end_process_enhanced():

    global stream
    global batch_stopped

    batch_stopped = True
    print(translate("ÂÅúÊ≠¢„Éú„Çø„É≥„ÅåÊäº„Åï„Çå„Åæ„Åó„Åü„ÄÇ„Éê„ÉÉ„ÉÅÂá¶ÁêÜ„ÇíÂÅúÊ≠¢„Åó„Åæ„Åô..."))
    stream.input_queue.push('end')

    # Return updated button states
    return (
        gr.update(value=translate("ÂÅúÊ≠¢Âá¶ÁêÜ‰∏≠...")),  # End button (temporary message)
        gr.update(interactive=True, value=translate("‚ñ∂Ô∏è Start Queue"))  # Re-enable queue start
    )
  
# ==============================================================================
# UI CREATION AND EVENT SETUP
# ==============================================================================

def create_enhanced_config_queue_ui():
    
    with gr.Group():
        gr.Markdown(f"### " + translate("Config Queue System"))
        
        with gr.Row():
            with gr.Column(scale=2):
                config_name_input = gr.Textbox(
                    label=translate("Config Name (optional)"),
                    placeholder=translate("Leave blank for auto-generation"),
                    value="",
                    info=translate("Use existing name to overwrite, or new name to create")
                )
            with gr.Column(scale=1):
                # LOAD SAVED SETTING FOR TIMESTAMP CHECKBOX
                saved_settings = load_app_settings_f1()
                default_add_timestamp = saved_settings.get("add_timestamp_to_config", True)
                
                add_timestamp_to_config = gr.Checkbox(
                    label=translate("Add timestamp to config name"),
                    value=default_add_timestamp,  # Use saved setting
                    info=translate("Uncheck to use exact input name (may overwrite existing)")
                )

                save_config_btn = gr.Button(
                    value=translate("üíæ Save Config"),
                    variant="primary"
                )
                
        # Config selection with merged refresh
        with gr.Row():
            with gr.Column(scale=2):
                available_configs = config_queue_manager.get_available_configs()
                if not available_configs:
                    available_configs = [translate("No configs available")]
                
                config_dropdown = gr.Dropdown(
                    label=translate("Select Config"),
                    choices=available_configs,
                    value=None,
                    allow_custom_value=False,
                    info=translate("Select a config file to load, queue, or delete")
                )
            with gr.Column(scale=1):
                with gr.Row():
                    load_config_btn = gr.Button(value=translate("üìÇ Load"), variant="secondary", scale=1)
                    delete_config_btn = gr.Button(value=translate("üóëÔ∏è Delete"), variant="secondary", scale=1)
                    merged_refresh_btn = gr.Button(value=translate("üîÑ Refresh All"), variant="secondary", scale=1)
        
      
        # Queue control buttons with enhanced start
        with gr.Row():
            with gr.Column(scale=1):
                queue_config_btn = gr.Button(value=translate("üìã Queue Config"), variant="primary")
            with gr.Column(scale=1):
                clear_queue_btn = gr.Button(value=translate("üóëÔ∏è Clear Queue"), variant="secondary")
            with gr.Column(scale=1):
                enhanced_start_queue_btn = gr.Button(value=translate("‚ñ∂Ô∏è Start Queue"), variant="primary")
            with gr.Column(scale=1):
                stop_queue_btn = gr.Button(value=translate("‚èπÔ∏è Stop Queue"), variant="secondary")


        # Messages
        config_message = gr.Markdown("")

        # State and confirmation (same as before)
        pending_operation = gr.State(None)
        with gr.Group(visible=False) as confirmation_group:
            confirmation_html = gr.HTML("")
            with gr.Row():
                confirm_btn = gr.Button(translate("‚úÖ Confirm"), variant="primary", scale=1)
                cancel_btn = gr.Button(translate("‚ùå Cancel"), variant="secondary", scale=1)

        # Enhanced status display
        queue_status_display = gr.Textbox(
            label=translate("Queue & Config Status"),
            value="",
            lines=10,
            interactive=False
        )

        
    # Initialize status
    try:
        initial_status = config_queue_manager.get_queue_status()
        initial_status_text = format_queue_status_with_batch_progress(initial_status)
        queue_status_display.value = initial_status_text
    except Exception as e:
        queue_status_display.value = translate("Status: Ready")
    
    return {
        'config_name_input': config_name_input,
        'add_timestamp_to_config': add_timestamp_to_config,
        'save_config_btn': save_config_btn,
        'config_dropdown': config_dropdown,
        'load_config_btn': load_config_btn,
        'delete_config_btn': delete_config_btn,
        'merged_refresh_btn': merged_refresh_btn,  # Changed from separate buttons
        'pending_operation': pending_operation,
        'confirmation_group': confirmation_group,
        'confirmation_html': confirmation_html,
        'confirm_btn': confirm_btn,
        'cancel_btn': cancel_btn,
        'queue_config_btn': queue_config_btn,
        'enhanced_start_queue_btn': enhanced_start_queue_btn,  # Enhanced start button
        'stop_queue_btn': stop_queue_btn,
        'clear_queue_btn': clear_queue_btn,
        'queue_status_display': queue_status_display,
        'config_message': config_message
    }

def setup_enhanced_config_queue_events(components, ui_components):
    
    # Config management events (unchanged)
    components['load_config_btn'].click(
        fn=load_config_with_delayed_lora_application_fixed,
        inputs=[components['config_dropdown']],
        outputs=[
            components['config_message'],
            ui_components['input_image'],
            ui_components['prompt'],
            ui_components['use_lora'],
            ui_components['lora_mode'],
            ui_components['lora_scales_text'],
            ui_components['lora_dropdown1'],
            ui_components['lora_dropdown2'],
            ui_components['lora_dropdown3'],
            components['config_name_input']
        ]
    )
    
    components['save_config_btn'].click(
        fn=save_current_config_handler_v3,
        inputs=[
            components['config_name_input'],
            components['add_timestamp_to_config'],  # NEW INPUT
            ui_components['input_image'],
            ui_components['prompt'],
            ui_components['use_lora'],
            ui_components['lora_mode'],
            ui_components['lora_dropdown1'],
            ui_components['lora_dropdown2'],
            ui_components['lora_dropdown3'],
            ui_components['lora_files'],
            ui_components['lora_files2'],
            ui_components['lora_files3'],
            ui_components['lora_scales_text']
        ],
        outputs=[
            components['config_message'],
            components['config_dropdown'],
            components['queue_status_display'],
            components['confirmation_group'],
            components['pending_operation']
        ]
    )
    
    
    components['delete_config_btn'].click(
        fn=delete_config_handler_v2,
        inputs=[components['config_dropdown']],
        outputs=[
            components['config_message'],
            components['config_dropdown'],
            components['queue_status_display'],
            components['confirmation_group'],
            components['pending_operation']
        ]
    )
    
    # Confirmation handlers (unchanged)
    components['confirm_btn'].click(
        fn=confirm_operation_handler_fixed,
        inputs=[components['pending_operation']],
        outputs=[
            components['config_message'],
            components['config_dropdown'],
            components['queue_status_display'],
            components['confirmation_group'],
            components['pending_operation'],
            components['config_name_input']
        ]
    )
    
    components['cancel_btn'].click(
        fn=cancel_operation_handler,
        inputs=[],
        outputs=[
            components['config_message'],
            components['config_dropdown'],
            components['queue_status_display'],
            components['confirmation_group'],
            components['pending_operation']
        ]
    )

    # UPDATED QUEUE START HANDLER - Now includes batch_count
    components['enhanced_start_queue_btn'].click(
        fn=start_queue_processing_with_current_ui_values,
        inputs=[
            # Duration settings - both controls  
            length_radio, total_second_length,
            # Frame settings
            frame_size_radio,
            # Quality settings
            steps, cfg, gs, rs, resolution, mp4_crf,
            # Generation settings
            seed, use_random_seed, use_teacache, image_strength, fp8_optimization,
            # System settings
            gpu_memory_preservation,
            # Output settings
            keep_section_videos, save_section_frames, save_tensor_data,
            frame_save_mode, output_dir, alarm_on_completion,
            # F1 mode settings
            all_padding_value, use_all_padding,
            # ADD BATCH COUNT INPUT
            batch_count
        ],
        outputs=[
            components['config_message'],
            components['queue_status_display'],
            ui_components['progress_desc'],
            ui_components['progress_bar'],
            ui_components['preview_image'],
            ui_components['result_video'],
            start_button,  # ADD: Manual start button state
            end_button     # ADD: Manual end button state  
        ]
    )
    
    components['queue_config_btn'].click(
        fn=queue_config_handler_with_confirmation,
        inputs=[components['config_dropdown']],
        outputs=[
            components['config_message'],
            components['queue_status_display'],
            components['confirmation_group'],    
            components['pending_operation']  
        ]
    )
    
    components['stop_queue_btn'].click(
        fn=stop_queue_processing_handler_fixed,
        inputs=[],
        outputs=[
            components['config_message'],
            components['queue_status_display'],
            ui_components['preview_image'],
            ui_components['result_video'] 
        ]
    )
    
    components['clear_queue_btn'].click(
        fn=clear_queue_handler,
        inputs=[],
        outputs=[
            components['config_message'],
            components['queue_status_display']
        ]
    )
        
    # Merged refresh button (unchanged)
    components['merged_refresh_btn'].click(
        fn=merged_refresh_handler_standardized,
        inputs=[],
        outputs=[
            components['config_message'],
            components['config_dropdown'],
            components['queue_status_display']
        ]
    )

def setup_periodic_queue_status_check():

    import threading
    import time
    
    def periodic_check():
        while True:
            try:
                time.sleep(10)  # Check every 10 seconds (reduced from 30 for better responsiveness)
                
                if config_queue_manager and hasattr(config_queue_manager, 'is_processing'):
                    status = config_queue_manager.get_queue_status()
                    
                    # Check for stuck state
                    if (config_queue_manager.is_processing and 
                        status.get('queue_count', 0) == 0 and 
                        not status.get('current_config') and
                        not status.get('processing')):
                        
                        print(translate("üîß Periodic check: Detected stuck queue state - auto-correcting"))
                        globals()['queue_processing_active'] = False
                        config_queue_manager.is_processing = False
                        config_queue_manager.current_config = None
                        
                        # Log the correction for debugging
                        print(translate("üîß Periodic correction applied at {0}").format(time.strftime('%H:%M:%S')))
                        
            except Exception as e:
                print(translate("Periodic queue check error: {0}").format(e))
    
    # Start the periodic check thread
    check_thread = threading.Thread(target=periodic_check, daemon=True)
    check_thread.start()
    print(translate("üîß Started periodic queue status checker (10s intervals)"))


# ==============================================================================
# UTILITY FUNCTIONS
# ==============================================================================

def get_lora_mode_text(lora_mode_key):

    if lora_mode_key == LORA_MODE_DIRECTORY:
        return translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû")
    elif lora_mode_key == LORA_MODE_UPLOAD:
        return translate("„Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ")
    else:
        return translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû")  # Default fallback

def get_current_lora_settings(use_lora, lora_mode, lora_dropdown1, lora_dropdown2, lora_dropdown3, 
                             lora_files, lora_files2, lora_files3, lora_scales_text):

    lora_settings = {
        "use_lora": use_lora,
        "lora_scales": lora_scales_text
    }
    
    if not use_lora:
        lora_settings["lora_mode_key"] = LORA_MODE_DIRECTORY
        lora_settings["lora_files"] = []
        return lora_settings
    
    if lora_mode == translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû"):
        # Directory selection mode - handle normally
        print(translate("üìÅ Saving config: Directory selection mode"))
        lora_settings["lora_mode_key"] = LORA_MODE_DIRECTORY
        
        lora_paths = []
        lora_dropdown_files = []
        
        for dropdown in [lora_dropdown1, lora_dropdown2, lora_dropdown3]:
            if dropdown and dropdown != translate("„Å™„Åó"):
                lora_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'lora')
                lora_path = os.path.join(lora_dir, dropdown)
                if os.path.exists(lora_path):
                    lora_paths.append(lora_path)
                    lora_dropdown_files.append(dropdown)
                else:
                    lora_dropdown_files.append(LORA_NONE_OPTION)
            else:
                lora_dropdown_files.append(LORA_NONE_OPTION)
        
        lora_settings["lora_files"] = lora_paths
        lora_settings["lora_dropdown_files"] = lora_dropdown_files
        
    else:  # File upload mode - AUTO-CONVERT to directory mode
        print(translate("üìÅ Saving config: Converting file uploads to directory mode"))
        
        import shutil
        lora_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'lora')
        os.makedirs(lora_dir, exist_ok=True)
        
        lora_paths = []
        lora_dropdown_files = []
        copied_files = []
        
        for lora_file in [lora_files, lora_files2, lora_files3]:
            if lora_file and hasattr(lora_file, 'name'):
                try:
                    src_path = lora_file.name
                    original_filename = os.path.basename(src_path)
                    dest_path = os.path.join(lora_dir, original_filename)
                    
                    # Handle filename conflicts
                    if os.path.exists(dest_path):
                        if os.path.getsize(src_path) == os.path.getsize(dest_path):
                            print(translate("   üìÑ File already exists (same size): {0}").format(original_filename))
                        else:
                            name, ext = os.path.splitext(original_filename)
                            counter = 1
                            while os.path.exists(dest_path):
                                new_filename = f"{name}_copy{counter}{ext}"
                                dest_path = os.path.join(lora_dir, new_filename)
                                counter += 1
                            original_filename = os.path.basename(dest_path)
                            print(translate("   üìÑ Renamed to avoid conflict: {0}").format(original_filename))
                    
                    if not os.path.exists(dest_path):
                        shutil.copy2(src_path, dest_path)
                        print(translate("   ‚úÖ Copied LoRA file: {0}").format(original_filename))
                        copied_files.append(original_filename)
                    else:
                        print(translate("   ‚úÖ Using existing file: {0}").format(original_filename))
                        copied_files.append(original_filename)
                    
                    lora_paths.append(dest_path)
                    lora_dropdown_files.append(original_filename)
                    
                except Exception as e:
                    print(translate("   ‚ùå Error copying LoRA file {0}: {1}").format(lora_file.name, e))
                    continue
        
        while len(lora_dropdown_files) < 3:
            lora_dropdown_files.append(LORA_NONE_OPTION)
        
        lora_settings["lora_mode_key"] = LORA_MODE_DIRECTORY  # AUTO-CONVERTED
        lora_settings["lora_files"] = lora_paths
        lora_settings["lora_dropdown_files"] = lora_dropdown_files
        # REMOVED: Don't store conversion info in lora_settings to avoid message contamination
        
        if copied_files:
            print(translate("   üì¶ Auto-converted file uploads: {0}").format(', '.join(copied_files)))
    
    return lora_settings

def apply_lora_config_to_dropdowns_safe(lora_files, existing_choices=None):
    
    # Use provided choices or scan fresh
    if existing_choices is None:
        choices = scan_lora_directory()
        print(translate("üîÑ Fresh scan found {0} choices: {1}...").format(len(choices), choices[:5]))
    else:
        choices = existing_choices
        print(translate("üîÑ Using provided choices: {0} choices").format(len(choices)))
    
    # Initialize dropdown values
    lora_dropdown_values = [translate("„Å™„Åó"), translate("„Å™„Åó"), translate("„Å™„Åó")]
    applied_files = []
    
    # Apply each LoRA file
    for i, lora_path in enumerate(lora_files[:3]):
        if lora_path and os.path.exists(lora_path):
            lora_filename = os.path.basename(lora_path)
            
            # Check if filename exists in choices
            if lora_filename in choices:
                lora_dropdown_values[i] = lora_filename
                applied_files.append(lora_filename)
                print(translate("  ‚úÖ Applied LoRA {0}: {1}").format(i+1, lora_filename))
            else:
                print(translate("  ‚ùå LoRA file not found in directory: {0}").format(lora_filename))
                print(translate("      Available choices: {0}...").format(choices[:10]))  # Show first 10 for debugging
                # Keep default "„Å™„Åó" value instead of setting invalid value
        else:
            print(translate("  ‚ö†Ô∏è LoRA {0} file not found or invalid: {1}").format(i+1, lora_path))
    
    # Validate all values are in choices before returning
    for i, value in enumerate(lora_dropdown_values):
        if value not in choices:
            print(translate("  üîß Correcting invalid dropdown value: {0} -> {1}").format(value, choices[0]))
            lora_dropdown_values[i] = choices[0] if choices else translate("„Å™„Åó")
    
    return choices, lora_dropdown_values, applied_files

def scan_lora_directory():

    lora_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'lora')
    choices = []
    
    # „Éá„Ç£„É¨„ÇØ„Éà„É™„ÅåÂ≠òÂú®„Åó„Å™„ÅÑÂ†¥Âêà„ÅØ‰ΩúÊàê
    if not os.path.exists(lora_dir):
        os.makedirs(lora_dir, exist_ok=True)
        print(translate("LoRA„Éá„Ç£„É¨„ÇØ„Éà„É™„ÅåÂ≠òÂú®„Åó„Å™„Åã„Å£„Åü„Åü„ÇÅ‰ΩúÊàê„Åó„Åæ„Åó„Åü: {0}").format(lora_dir))
    
    # „Éá„Ç£„É¨„ÇØ„Éà„É™ÂÜÖ„ÅÆ„Éï„Ç°„Ç§„É´„Çí„É™„Çπ„Éà„Ç¢„ÉÉ„Éó
    try:
        for filename in os.listdir(lora_dir):
            if filename.endswith(('.safetensors', '.pt', '.bin')):
                # Validate file is readable
                file_path = os.path.join(lora_dir, filename)
                if os.path.isfile(file_path) and os.access(file_path, os.R_OK):
                    choices.append(filename)
    except Exception as e:
        print(translate("Error scanning LoRA directory: {0}").format(e))
    
    # Á©∫„ÅÆÈÅ∏ÊäûËÇ¢„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØ"„Å™„Åó"„ÇíËøΩÂä†
    choices = sorted(choices)
    
    # „Å™„Åó„ÅÆÈÅ∏ÊäûËÇ¢„ÇíÊúÄÂàù„Å´ËøΩÂä†
    none_choice = translate("„Å™„Åó")
    if none_choice not in choices:
        choices.insert(0, none_choice)
    
    # ÈáçË¶Å: „Åô„Åπ„Å¶„ÅÆÈÅ∏ÊäûËÇ¢„ÅåÁ¢∫ÂÆü„Å´ÊñáÂ≠óÂàóÂûã„Åß„ÅÇ„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç
    validated_choices = []
    for choice in choices:
        if isinstance(choice, str) and choice.strip():
            validated_choices.append(choice)
        else:
            print(translate("‚ö†Ô∏è Skipping invalid choice: {0} (type: {1})").format(choice,type(choice)))
    
    # Ensure we always have at least the "none" option
    if not validated_choices:
        validated_choices = [translate("„Å™„Åó")]
    
    print(translate("üîç Scanned LoRA directory: found {0} files + none option").format(len(validated_choices)-1))
    return validated_choices

# ==============================================================================
# INITIALIZATION AND STARTUP
# ==============================================================================

# Initialize config queue manager with error handling
try:
    config_queue_manager = ConfigQueueManager(os.path.dirname(os.path.abspath(__file__)))
    print(translate("Config queue manager initialized successfully"))
except Exception as e:
    print(translate("Error initializing config queue manager: {0}").format(e))
    config_queue_manager = None
# Setup monitoring systems if manager available
if config_queue_manager is not None:
    setup_periodic_queue_status_check()

# ==============================================================================
# QUEUE PROCESSING FUNCTIONS
# ==============================================================================

def start_queue_processing_with_current_ui_values(
    # Duration settings - both controls
    length_radio, total_second_length,
    # Frame settings
    frame_size_radio,
    # Quality settings  
    steps, cfg, gs, rs, resolution, mp4_crf,
    # Generation settings
    seed, use_random_seed, use_teacache, image_strength, fp8_optimization,
    # System settings
    gpu_memory_preservation,
    # Output settings
    keep_section_videos, save_section_frames, save_tensor_data, 
    frame_save_mode, output_dir, alarm_on_completion,
    # F1 mode settings
    all_padding_value, use_all_padding,
    # Batch count parameter
    batch_count
):
    
    if config_queue_manager is None:
        yield (
            translate("‚ùå Config queue manager not initialized"),  # 1. markdown (config_message)
            gr.update(),                                 # 2. textbox (queue_status_display)
            gr.update(),                                 # 3. markdown (progress_desc)
            gr.update(),                                 # 4. html (progress_bar)
            gr.update(visible=False),                    # 5. image (preview_image) - HIDE
            gr.update(visible=False),                    # 6. video (result_video) - HIDE
            gr.update(interactive=True),                 # 7. button (manual start_button)
            gr.update(interactive=False)                 # 8. button (manual end_button)
        )
        return
    
    queue_status = config_queue_manager.get_queue_status()
    has_items = queue_status.get('queue_count', 0) > 0
    
    if not has_items:
        yield (
            "‚ùå No items in queue",                      # 1. markdown (config_message)
            gr.update(),                                 # 2. textbox (queue_status_display)
            gr.update(),                                 # 3. markdown (progress_desc)
            gr.update(),                                 # 4. html (progress_bar)
            gr.update(visible=False),                    # 5. image (preview_image) - HIDE
            gr.update(visible=False),                    # 6. video (result_video) - HIDE
            gr.update(interactive=True),                 # 7. button (manual start_button)
            gr.update(interactive=False)                 # 8. button (manual end_button)
        )
        return
    
    
    # Store settings globally for queue worker to access
    global queue_ui_settings
    queue_ui_settings = {
        'total_second_length': max(1, int(total_second_length)),
        'length_radio': length_radio,
        'frame_size_setting': frame_size_radio,
        'latent_window_size': 4.5 if frame_size_radio == translate("0.5Áßí (17„Éï„É¨„Éº„É†)") else 9,
        'steps': int(steps),
        'cfg': float(cfg),
        'gs': float(gs), 
        'rs': float(rs),
        'resolution': int(resolution),
        'mp4_crf': int(mp4_crf),
        'seed': int(seed),
        'use_random_seed': bool(use_random_seed),
        'use_teacache': bool(use_teacache),
        'image_strength': float(image_strength),
        'fp8_optimization': bool(fp8_optimization),
        'gpu_memory_preservation': float(gpu_memory_preservation),
        'keep_section_videos': bool(keep_section_videos),
        'save_section_frames': bool(save_section_frames),
        'save_tensor_data': bool(save_tensor_data),
        'frame_save_mode': frame_save_mode,
        'output_dir': output_dir,
        'alarm_on_completion': bool(alarm_on_completion),
        'all_padding_value': float(all_padding_value),
        'use_all_padding': bool(use_all_padding),
        'batch_count': max(1, int(batch_count)),
        'n_prompt': "",
        'tensor_data_input': None,
        'use_queue': False,
        'prompt_queue_file': None,
        'save_settings_on_start': False
    }
    
    total_expected_videos = queue_status['queue_count'] * queue_ui_settings['batch_count']
    print(translate("üìã Queue starting: {0} configs √ó {1} batches = {2} total videos").format(queue_status['queue_count'], queue_ui_settings['batch_count'], total_expected_videos))
    
    # Start processing with batch-aware processor
    success, message = config_queue_manager.start_queue_processing(process_config_item_with_batch_support)
    
    if not success:
        yield (
            translate("‚ùå Failed to start: {0}").format(message),            # 1. markdown (config_message)
            gr.update(),                                 # 2. textbox (queue_status_display)
            gr.update(),                                 # 3. markdown (progress_desc)
            gr.update(),                                 # 4. html (progress_bar)
            gr.update(visible=False),                    # 5. image (preview_image) - HIDE
            gr.update(visible=False),                    # 6. video (result_video) - HIDE
            gr.update(interactive=True),                 # 7. button (manual start_button)
            gr.update(interactive=False)                 # 8. button (manual end_button)
        )
        return
    
    global queue_processing_active
    queue_processing_active = True
    initial_count = has_items
    
    # Return initial status with queue processing UI
    yield (
        translate("‚úÖ Queue started ({0} configs √ó {1} batches = {2} videos)").format(queue_status['queue_count'], queue_ui_settings['batch_count'], total_expected_videos),  # 1. markdown (config_message)
        gr.update(value=format_queue_status_with_batch_progress(queue_status)),  # 2. textbox (queue_status_display)
        translate("Queue processing started: {0} total videos to generate...").format(total_expected_videos),  # 3. markdown (progress_desc)
        f'<div style="color: blue; font-weight: bold;">{translate("üìã Queue processing active - Progress UI disabled. Check console for details.")}</div>',  # 4. html (progress_bar)
        gr.update(visible=False),                        # 5. image (preview_image) - HIDE
        gr.update(visible=False),                        # 6. video (result_video) - HIDE
        gr.update(interactive=False, value=translate("ÈòüÂàóÂ§ÑÁêÜ‰∏≠...")),  # 7. button (manual start_button)
        gr.update(interactive=False)                     # 8. button (manual end_button)
    )
    
    # Monitor with periodic updates using batch progress
    import time
    last_count = initial_count
    last_current_config = None
    
    while queue_processing_active:
        time.sleep(3.0)
        
        try:
            status = config_queue_manager.get_queue_status()
            current_count = status['queue_count']
            is_processing = status['is_processing']
            current_config = status.get('current_config')
            
            # Get batch progress for enhanced status
            batch_progress = current_batch_progress.copy()
            
            if current_count != last_count or current_config != last_current_config:
                # Calculate remaining videos with batch progress
                unprocessed_config_count = current_count
                if current_config and batch_progress['total'] > 0:
                    # Current config is being processed, subtract completed batches
                    current_config_remaining_batches = batch_progress['total'] - batch_progress['current']
                    remaining_videos = current_config_remaining_batches + (unprocessed_config_count * queue_ui_settings['batch_count'])
                else:
                    remaining_videos = unprocessed_config_count * queue_ui_settings['batch_count']
                
                if current_config:
                    if batch_progress['total'] > 0:
                        batch_info = translate("Batch {0}/{1}").format(batch_progress['current'], batch_progress['total'])
                        status_msg = translate("üìã Processing: {0} ({1}) - {2} videos remaining").format(current_config, batch_info, remaining_videos)
                        desc_msg = translate("Processing {0} - {1} - {2} videos remaining").format(current_config, batch_info, remaining_videos)
                    else:
                        status_msg = translate("üìã Processing: {0} - {1} videos remaining").format(current_config, remaining_videos)
                        desc_msg = translate("Processing {0} - {1} videos remaining").format(current_config, remaining_videos)
                else:
                    status_msg = translate("üìã Queue: {0} videos remaining").format(remaining_videos)
                    desc_msg = translate("Queue processing... {0} videos remaining").format(remaining_videos)
                
                yield (
                    status_msg,                              # 1. markdown (config_message)
                    gr.update(value=format_queue_status_with_batch_progress(status)),  # 2. textbox (queue_status_display)
                    desc_msg,                                # 3. markdown (progress_desc)
                    f'<div style="color: blue; font-weight: bold;">üìã {translate("Queue processing active - Progress UI disabled. Check console for details.")}</div>',  # 4. html (progress_bar)
                    gr.update(visible=False),                # 5. image (preview_image) - HIDE
                    gr.update(visible=False),                # 6. video (result_video) - HIDE
                    gr.update(interactive=False, value=translate("ÈòüÂàóÂ§ÑÁêÜ‰∏≠...")),  # 7. button (manual start_button)
                    gr.update(interactive=False)             # 8. button (manual end_button)
                )
                
                last_count = current_count
                last_current_config = current_config
            
            if not is_processing and current_count == 0:
                print(translate("‚úÖ Queue processing completed"))
                yield (
                    translate("‚úÖ Queue completed - All {0} videos processed successfully").format(total_expected_videos),  # 1. markdown (config_message)
                    gr.update(value=format_queue_status_with_batch_progress(status)),  # 2. textbox (queue_status_display)
                    "All queue items and batches have been processed",  # 3. markdown (progress_desc)
                    '<div style="color: green; font-weight: bold;">‚úÖ Queue processing completed</div>',  # 4. html (progress_bar)
                    gr.update(visible=True),  # preview_image - RESTORE VISIBILITY
                    gr.update(visible=True),  # result_video - RESTORE VISIBILITY
                    gr.update(interactive=True, value=translate("Start Generation")),  # 7. button (manual start_button) - RE-ENABLE
                    gr.update(interactive=False)             # 8. button (manual end_button)
                )
                break
                
        except Exception as e:
            print(translate("‚ùå Queue monitoring error: {0}").format(e))
            continue
    
    queue_processing_active = False
    print(translate("üèÅ Queue processing monitor finished"))

# def process_config_item_with_batch_support(config_data):

#     global queue_ui_settings, current_processing_config_name, current_batch_progress
    
#     try:
#         config_name = config_data.get('config_name', 'unknown_config')
#         print(translate("üé¨ Processing config: {0}").format(config_name))
        
#         # Validate that image exists for generation
#         image_path = config_data.get('image_path')
#         if not image_path or not os.path.exists(image_path):
#             print(translate("‚ùå Cannot generate video: Image missing for config {0}").format(config_name))
#             print(translate("    Expected path: {0}").format(image_path))
#             return False
        
#         print(translate("‚úÖ Image validated: {0}").format(os.path.basename(image_path)))
            
#         # Get batch count from UI settings with debug logging
#         batch_count_raw = queue_ui_settings.get('batch_count', 1)
        
#         # Ensure batch_count is definitely an integer
#         if isinstance(batch_count_raw, bool):
#             print(translate("‚ö†Ô∏è Warning: batch_count is boolean ({0}), converting to integer").format(batch_count_raw))
#             batch_count = 1 if batch_count_raw else 1
#         else:
#             try:
#                 batch_count = int(batch_count_raw)
#             except (ValueError, TypeError):
#                 print(translate("‚ö†Ô∏è Warning: Could not convert batch_count to int: {0} (type: {1})").format(batch_count_raw, type(batch_count_raw)))
#                 batch_count = 1
        
#         batch_count = max(1, min(batch_count, 100))  # Ensure valid range
        
        
#         # Set the current config name for worker function to use
#         current_processing_config_name = config_name
        
#         # Initialize batch progress with validated integer
#         current_batch_progress = {"current": 0, "total": batch_count}
#         print(translate("üìä Initialized batch progress: {0}").format(current_batch_progress))
        
#         # Use the stored UI settings
#         if queue_ui_settings is None:
#             print(translate("‚ùå No UI settings available - using defaults"))
#             queue_ui_settings = get_current_ui_settings_for_queue()
        
#         current_ui_settings = queue_ui_settings
        
#         print(translate("üïí Using duration from UI: {0}s").format(current_ui_settings['total_second_length']))
        
#         # Extract config data
#         image_path = config_data['image_path']
#         prompt = config_data['prompt']
#         lora_settings = config_data['lora_settings']
        
#         # Handle LoRA configuration (existing code...)
#         use_lora = lora_settings.get('use_lora', False)
#         lora_mode_key = lora_settings.get('lora_mode_key')
#         if lora_mode_key:
#             lora_mode = get_lora_mode_text(lora_mode_key)
#         else:
#             old_lora_mode = lora_settings.get('lora_mode')
#             if old_lora_mode:
#                 if '„Éá„Ç£„É¨„ÇØ„Éà„É™' in old_lora_mode or 'directory' in old_lora_mode.lower() or 'ÁõÆÈåÑ' in old_lora_mode:
#                     lora_mode = translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû")
#                 elif '„Éï„Ç°„Ç§„É´' in old_lora_mode or 'file' in old_lora_mode.lower() or 'Ê™îÊ°à' in old_lora_mode:
#                     lora_mode = translate("„Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ")
#                 else:
#                     lora_mode = translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû")
#             else:
#                 lora_mode = translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû")
            
#         lora_scales_text = lora_settings.get('lora_scales', '0.8,0.8,0.8')
        
#         # Initialize LoRA parameters (existing code...)
#         lora_files_obj = None
#         lora_files2_obj = None
#         lora_files3_obj = None
#         lora_dropdown1_val = None
#         lora_dropdown2_val = None
#         lora_dropdown3_val = None
        
#         if use_lora:
#             lora_files_list = lora_settings.get('lora_files', [])
            
#             if lora_mode == translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû"):
#                 lora_dropdown_files = lora_settings.get('lora_dropdown_files')
#                 if lora_dropdown_files:
#                     lora_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'lora')
                    
#                     for i, dropdown_file in enumerate(lora_dropdown_files[:3]):
#                         if dropdown_file != "none_option":
#                             lora_file_path = os.path.join(lora_dir, dropdown_file)
#                             if os.path.exists(lora_file_path):
#                                 if i == 0:
#                                     lora_dropdown1_val = dropdown_file
#                                 elif i == 1:
#                                     lora_dropdown2_val = dropdown_file
#                                 elif i == 2:
#                                     lora_dropdown3_val = dropdown_file
#                 else:
#                     if lora_files_list:
#                         if len(lora_files_list) > 0 and lora_files_list[0] and os.path.exists(lora_files_list[0]):
#                             lora_dropdown1_val = os.path.basename(lora_files_list[0])
#                         if len(lora_files_list) > 1 and lora_files_list[1] and os.path.exists(lora_files_list[1]):
#                             lora_dropdown2_val = os.path.basename(lora_files_list[1])
#                         if len(lora_files_list) > 2 and lora_files_list[2] and os.path.exists(lora_files_list[2]):
#                             lora_dropdown3_val = os.path.basename(lora_files_list[2])
#             else:
#                 if lora_files_list:
#                     if len(lora_files_list) > 0 and os.path.exists(lora_files_list[0]):
#                         lora_files_obj = type('MockFile', (), {'name': lora_files_list[0]})()
#                     if len(lora_files_list) > 1 and os.path.exists(lora_files_list[1]):
#                         lora_files2_obj = type('MockFile', (), {'name': lora_files_list[1]})()
#                     if len(lora_files_list) > 2 and os.path.exists(lora_files_list[2]):
#                         lora_files3_obj = type('MockFile', (), {'name': lora_files_list[2]})()
        
#         print(translate("üéØ Calling process() with config: {0}, batch_count: {1}, duration: {2}s").format(config_name, batch_count, current_ui_settings['total_second_length']))
        
#         def process_with_batch_tracking(*args):
#             """Simplified wrapper - let process() handle its own batch logic"""
#             print(translate("üìä Starting video generation for config: {0}").format(config_name))
            
#             # Initialize batch progress - we're starting the entire config processing
#             update_batch_progress(0, batch_count)
            
#             # Just consume the process generator without trying to intercept individual results
#             for result in process(*args):
#                 # Don't try to detect "batch completion" here - process() handles batch logic internally
#                 yield result
            
#             # When the generator is fully consumed, the entire config (all batches) is complete
#             update_batch_progress(batch_count, batch_count)
#             print(translate("‚úÖ All {0} batch(es) completed for config: {1}").format(batch_count, config_name))
        
#         # Call the enhanced process function with batch tracking
#         result_generator = process_with_batch_tracking(
#             image_path,  # input_image
#             prompt,  # prompt
#             current_ui_settings['n_prompt'],  # n_prompt
#             current_ui_settings['seed'],  # seed
#             current_ui_settings['total_second_length'],  # total_second_length
#             current_ui_settings['latent_window_size'],  # latent_window_size
#             current_ui_settings['steps'],  # steps
#             current_ui_settings['cfg'],  # cfg
#             current_ui_settings['gs'],  # gs
#             current_ui_settings['rs'],  # rs
#             current_ui_settings['gpu_memory_preservation'],  # gpu_memory_preservation
#             current_ui_settings['use_teacache'],  # use_teacache
#             current_ui_settings['use_random_seed'],  # use_random_seed
#             current_ui_settings['mp4_crf'],  # mp4_crf
#             current_ui_settings['all_padding_value'],  # all_padding_value
#             current_ui_settings['image_strength'],  # image_strength
#             current_ui_settings['frame_size_setting'],  # frame_size_setting
#             current_ui_settings['keep_section_videos'],  # keep_section_videos
#             lora_files_obj,  # lora_files
#             lora_files2_obj,  # lora_files2
#             lora_files3_obj,  # lora_files3
#             lora_scales_text,  # lora_scales_text
#             current_ui_settings['output_dir'],  # output_dir
#             current_ui_settings['save_section_frames'],  # save_section_frames
#             current_ui_settings['use_all_padding'],  # use_all_padding
#             use_lora,  # use_lora
#             lora_mode,  # lora_mode
#             lora_dropdown1_val,  # lora_dropdown1
#             lora_dropdown2_val,  # lora_dropdown2
#             lora_dropdown3_val,  # lora_dropdown3
#             current_ui_settings['save_tensor_data'],  # save_tensor_data
#             [[None, None, ""] for _ in range(50)],  # section_settings (F1 specific)
#             current_ui_settings['tensor_data_input'],  # tensor_data_input
#             current_ui_settings['fp8_optimization'],  # fp8_optimization
#             current_ui_settings['resolution'],  # resolution
#             batch_count,  # USE VALIDATED batch_count DIRECTLY
#             current_ui_settings['frame_save_mode'],  # frame_save_mode
#             current_ui_settings['use_queue'],  # use_queue
#             current_ui_settings['prompt_queue_file'],  # prompt_queue_file
#             current_ui_settings['save_settings_on_start'],  # save_settings_on_start
#             current_ui_settings['alarm_on_completion']  # alarm_on_completion
#         )
        
#         # Consume the generator - RESTORED
#         step_count = 0
#         for result in result_generator:
#             step_count += 1
        
#         # Reset batch progress when done
#         current_batch_progress = {"current": 0, "total": 0}
        
#         return True
        
#     except Exception as e:
#         print(translate("‚ùå Config processing error: {0}").format(e))
#         import traceback
#         traceback.print_exc()
#         return False
#     finally:
#         # Clear the config name when done
#         current_processing_config_name = None
#         current_batch_progress = {"current": 0, "total": 0}
#         return True

def process_config_item_with_batch_support(config_data):
    global queue_ui_settings, current_processing_config_name, current_batch_progress
    
    try:
        config_name = config_data.get('config_name', 'unknown_config')
        print(translate("üé¨ Processing config: {0}").format(config_name))
        
        # FIXED: Use ConfigQueueManager's load_config_for_generation method
        # This method handles path resolution for both images and LoRA files
        if config_queue_manager is None:
            print(translate("‚ùå Config queue manager not available"))
            return False
            
        # Load config with proper path resolution
        success, resolved_config_data, message = config_queue_manager.load_config_for_generation(config_name)
        
        if not success:
            print(translate("‚ùå Cannot load config for generation: {0}").format(config_name))
            print(translate("    Error: {0}").format(message))
            return False
            
        # Use the resolved config data instead of the original
        config_data = resolved_config_data
        
        # Validate that image exists for generation (now with resolved path)
        image_path = config_data.get('image_path')
        if not image_path or not os.path.exists(image_path):
            print(translate("‚ùå Cannot generate video: Image missing for config {0}").format(config_name))
            print(translate("    Expected path: {0}").format(image_path))
            return False
        
        print(translate("‚úÖ Image validated: {0}").format(os.path.basename(image_path)))
            
        # Get batch count from UI settings with debug logging
        batch_count_raw = queue_ui_settings.get('batch_count', 1)
        
        # Ensure batch_count is definitely an integer
        if isinstance(batch_count_raw, bool):
            print(translate("‚ö†Ô∏è Warning: batch_count is boolean ({0}), converting to integer").format(batch_count_raw))
            batch_count = 1 if batch_count_raw else 1
        else:
            try:
                batch_count = int(batch_count_raw)
            except (ValueError, TypeError):
                print(translate("‚ö†Ô∏è Warning: Could not convert batch_count to int: {0} (type: {1})").format(batch_count_raw, type(batch_count_raw)))
                batch_count = 1
        
        batch_count = max(1, min(batch_count, 100))  # Ensure valid range
        
        # Set the current config name for worker function to use
        current_processing_config_name = config_name
        
        # Initialize batch progress with validated integer
        current_batch_progress = {"current": 0, "total": batch_count}
        print(translate("üìä Initialized batch progress: {0}").format(current_batch_progress))
        
        # Use the stored UI settings
        if queue_ui_settings is None:
            print(translate("‚ùå No UI settings available - using defaults"))
            queue_ui_settings = get_current_ui_settings_for_queue()
        
        current_ui_settings = queue_ui_settings
        
        print(translate("üïí Using duration from UI: {0}s").format(current_ui_settings['total_second_length']))
        
        # Extract config data (now using resolved paths)
        image_path = config_data['image_path']  # Already resolved
        prompt = config_data['prompt']
        lora_settings = config_data['lora_settings']
        
        # Handle LoRA configuration with resolved paths
        use_lora = lora_settings.get('use_lora', False)
        lora_mode_key = lora_settings.get('lora_mode_key')
        if lora_mode_key:
            lora_mode = get_lora_mode_text(lora_mode_key)
        else:
            old_lora_mode = lora_settings.get('lora_mode')
            if old_lora_mode:
                if '„Éá„Ç£„É¨„ÇØ„Éà„É™' in old_lora_mode or 'directory' in old_lora_mode.lower() or 'ÁõÆÈåÑ' in old_lora_mode:
                    lora_mode = translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû")
                elif '„Éï„Ç°„Ç§„É´' in old_lora_mode or 'file' in old_lora_mode.lower() or 'Ê™îÊ°à' in old_lora_mode:
                    lora_mode = translate("„Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ")
                else:
                    lora_mode = translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû")
            else:
                lora_mode = translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû")
            
        lora_scales_text = lora_settings.get('lora_scales', '0.8,0.8,0.8')
        
        # Initialize LoRA parameters
        lora_files_obj = None
        lora_files2_obj = None
        lora_files3_obj = None
        lora_dropdown1_val = None
        lora_dropdown2_val = None
        lora_dropdown3_val = None
        
        if use_lora:
            # LoRA files are now resolved to absolute paths by load_config_for_generation
            lora_files_list = lora_settings.get('lora_files', [])
            
            if lora_mode == translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû"):
                lora_dropdown_files = lora_settings.get('lora_dropdown_files')
                if lora_dropdown_files:
                    # Use the resolved absolute paths, but extract filenames for dropdown values
                    for i, lora_file_path in enumerate(lora_files_list[:3]):
                        if lora_file_path and os.path.exists(lora_file_path):
                            filename = os.path.basename(lora_file_path)
                            if i == 0:
                                lora_dropdown1_val = filename
                            elif i == 1:
                                lora_dropdown2_val = filename
                            elif i == 2:
                                lora_dropdown3_val = filename
                else:
                    # Fallback: extract filenames from resolved paths
                    if lora_files_list:
                        if len(lora_files_list) > 0 and lora_files_list[0] and os.path.exists(lora_files_list[0]):
                            lora_dropdown1_val = os.path.basename(lora_files_list[0])
                        if len(lora_files_list) > 1 and lora_files_list[1] and os.path.exists(lora_files_list[1]):
                            lora_dropdown2_val = os.path.basename(lora_files_list[1])
                        if len(lora_files_list) > 2 and lora_files_list[2] and os.path.exists(lora_files_list[2]):
                            lora_dropdown3_val = os.path.basename(lora_files_list[2])
            else:
                # File upload mode - create mock file objects from resolved paths
                if lora_files_list:
                    if len(lora_files_list) > 0 and os.path.exists(lora_files_list[0]):
                        lora_files_obj = type('MockFile', (), {'name': lora_files_list[0]})()
                    if len(lora_files_list) > 1 and os.path.exists(lora_files_list[1]):
                        lora_files2_obj = type('MockFile', (), {'name': lora_files_list[1]})()
                    if len(lora_files_list) > 2 and os.path.exists(lora_files_list[2]):
                        lora_files3_obj = type('MockFile', (), {'name': lora_files_list[2]})()
        
        print(translate("üéØ Calling process() with config: {0}, batch_count: {1}, duration: {2}s").format(config_name, batch_count, current_ui_settings['total_second_length']))
        
        def process_with_batch_tracking(*args):
            """Simplified wrapper - let process() handle its own batch logic"""
            print(translate("üìä Starting video generation for config: {0}").format(config_name))
            
            # Initialize batch progress - we're starting the entire config processing
            update_batch_progress(0, batch_count)
            
            # Just consume the process generator without trying to intercept individual results
            for result in process(*args):
                # Don't try to detect "batch completion" here - process() handles batch logic internally
                yield result
            
            # When the generator is fully consumed, the entire config (all batches) is complete
            update_batch_progress(batch_count, batch_count)
            print(translate("‚úÖ All {0} batch(es) completed for config: {1}").format(batch_count, config_name))
        
        # Call the enhanced process function with batch tracking
        result_generator = process_with_batch_tracking(
            image_path,  # input_image (now resolved)
            prompt,  # prompt
            current_ui_settings['n_prompt'],  # n_prompt
            current_ui_settings['seed'],  # seed
            current_ui_settings['total_second_length'],  # total_second_length
            current_ui_settings['latent_window_size'],  # latent_window_size
            current_ui_settings['steps'],  # steps
            current_ui_settings['cfg'],  # cfg
            current_ui_settings['gs'],  # gs
            current_ui_settings['rs'],  # rs
            current_ui_settings['gpu_memory_preservation'],  # gpu_memory_preservation
            current_ui_settings['use_teacache'],  # use_teacache
            current_ui_settings['use_random_seed'],  # use_random_seed
            current_ui_settings['mp4_crf'],  # mp4_crf
            current_ui_settings['all_padding_value'],  # all_padding_value
            current_ui_settings['image_strength'],  # image_strength
            current_ui_settings['frame_size_setting'],  # frame_size_setting
            current_ui_settings['keep_section_videos'],  # keep_section_videos
            lora_files_obj,  # lora_files
            lora_files2_obj,  # lora_files2
            lora_files3_obj,  # lora_files3
            lora_scales_text,  # lora_scales_text
            current_ui_settings['output_dir'],  # output_dir
            current_ui_settings['save_section_frames'],  # save_section_frames
            current_ui_settings['use_all_padding'],  # use_all_padding
            use_lora,  # use_lora
            lora_mode,  # lora_mode
            lora_dropdown1_val,  # lora_dropdown1
            lora_dropdown2_val,  # lora_dropdown2
            lora_dropdown3_val,  # lora_dropdown3
            current_ui_settings['save_tensor_data'],  # save_tensor_data
            [[None, None, ""] for _ in range(50)],  # section_settings (F1 specific)
            current_ui_settings['tensor_data_input'],  # tensor_data_input
            current_ui_settings['fp8_optimization'],  # fp8_optimization
            current_ui_settings['resolution'],  # resolution
            batch_count,  # USE VALIDATED batch_count DIRECTLY
            current_ui_settings['frame_save_mode'],  # frame_save_mode
            current_ui_settings['use_queue'],  # use_queue
            current_ui_settings['prompt_queue_file'],  # prompt_queue_file
            current_ui_settings['save_settings_on_start'],  # save_settings_on_start
            current_ui_settings['alarm_on_completion']  # alarm_on_completion
        )
        
        # Consume the generator
        step_count = 0
        for result in result_generator:
            step_count += 1
        
        # Reset batch progress when done
        current_batch_progress = {"current": 0, "total": 0}
        
        return True
        
    except Exception as e:
        print(translate("‚ùå Config processing error: {0}").format(e))
        import traceback
        traceback.print_exc()
        return False
    finally:
        # Clear the config name when done
        current_processing_config_name = None
        current_batch_progress = {"current": 0, "total": 0}
        return True

# ==============================================================================
# QUEUE STATUS AND MONITORING
# ==============================================================================

def format_queue_status_with_batch_progress(status):

    if "error" in status:
        return f"‚ùå Error: {status['error']}"
   
    lines = []
   
    # Processing status with batch information
    if status['is_processing']:
        lines.append(translate("üîÑ Status: PROCESSING"))
       
        current_config = status.get('current_config')
        batch_progress = status.get('batch_progress', {"current": 0, "total": 0})
        configs_remaining = status.get('configs_remaining', 0)  # Use the field from status
       
        if current_config:
            if batch_progress['total'] > 0:
                batch_info = translate("Batch ({0}/{1})").format(batch_progress['current'], batch_progress['total'])
                queue_info = translate("{0} file(s) in queue").format(configs_remaining)
                lines.append(translate("üìπ Processing: {0}, {1}, {2}").format(current_config, batch_info, queue_info))
            else:
                lines.append(translate("üìπ Processing: {0}, {1} file(s) in queue").format(current_config, configs_remaining))
        elif status.get('processing'):
            lines.append(translate("üìπ Current: {0}").format(status['processing']))
    else:
        lines.append(translate("‚è∏Ô∏è Status: IDLE"))
   
    # Queue information
    queue_count = status['queue_count']
    lines.append(translate("üìã Queue: {0} items").format(queue_count))
   
    # Available configs count
    try:
        if config_queue_manager:
            available_configs = config_queue_manager.get_available_configs()
            lines.append(translate("üìÅ Configs: {0} available").format(len(available_configs)))
    except:
        pass
   
    # Pending items (limited display)
    if status['queued']:
        lines.append(translate("‚è≥ Pending:"))
        for i, config in enumerate(status['queued'][:CONST_queued_shown_count]):
            lines.append(f"   {i+1}. {config}")
        if len(status['queued']) > CONST_queued_shown_count:
            lines.append(translate("   ... and {0} more").format(len(status['queued']) - CONST_queued_shown_count))
   
    # Recently completed (newest first)
    if status['completed']:
        lines.append(translate("‚úÖ Recently completed: {0} (newest first)").format(len(status['completed'])))
        for config in status['completed'][:CONST_latest_finish_count]:
            lines.append(f"   ‚úì {config}")
   
    # Timestamp
    from datetime import datetime
    timestamp = datetime.now().strftime("%H:%M:%S")
    lines.append(translate("üïí Last updated: {0}").format(timestamp))
   
    return "\n".join(lines)

def update_batch_progress(current_batch, total_batches):

    global current_batch_progress
    current_batch_progress = {"current": current_batch, "total": total_batches}
    print(translate("üìä Batch progress updated: {0}/{1}").format(current_batch, total_batches))

# ==============================================================================
# CONFIRMATION SYSTEM AND UI EVENT HANDLERS
# ==============================================================================

def confirm_operation_handler_fixed(operation_data):

    if not operation_data:
        return "‚ùå No pending operation", gr.update(), gr.update(), gr.update(visible=False), None, gr.update()
    
    try:
        if operation_data['type'] == 'overwrite' or operation_data['type'] == 'overwrite_exact':
            # Existing config save overwrite logic
            result = perform_save_operation_v3(
                operation_data['config_name_input'],
                operation_data['add_timestamp'],
                operation_data['input_image'],
                operation_data['prompt'],
                operation_data['use_lora'],
                operation_data['lora_mode'],
                operation_data['lora_dropdown1'],
                operation_data['lora_dropdown2'],
                operation_data['lora_dropdown3'],
                operation_data['lora_files'],
                operation_data['lora_files2'],
                operation_data['lora_files3'],
                operation_data['lora_scales_text']
            )
            # Add empty config name update for save operations
            return result + (gr.update(),)
        
        elif operation_data['type'] == 'queue_overwrite':
            # Queue overwrite logic
            config_name = operation_data['config_name']
            
            # Remove existing queued file
            queue_file = os.path.join(config_queue_manager.queue_dir, f"{config_name}.json")
            if os.path.exists(queue_file):
                os.remove(queue_file)
                print(translate("üîÑ Removed existing queued config: {0}").format(config_name))
            
            # Queue the config using existing method
            success, message = config_queue_manager.queue_config(config_name)
            
            if success:
                queue_status = config_queue_manager.get_queue_status()
                status_text = format_queue_status_with_batch_progress(queue_status)
                
                return (
                    f"‚úÖ Config overwritten in queue: {config_name}",  # config_message
                    gr.update(),  # config_dropdown (no change)
                    gr.update(value=status_text),  # queue_status_display
                    gr.update(visible=False),  # confirmation_group
                    None,  # pending_operation
                    gr.update()  # config_name_input (6th output - MISSING IN ORIGINAL)
                )
            else:
                return f"‚ùå {message}", gr.update(), gr.update(), gr.update(visible=False), None, gr.update()
        
        elif operation_data['type'] == 'delete':
            # DELETE OPERATION - CLEAR CONFIG NAME INPUT
            config_name = operation_data['config_name']
            success, message = config_queue_manager.delete_config(config_name)
            
            if success:
                available_configs = config_queue_manager.get_available_configs()
                queue_status = config_queue_manager.get_queue_status()
                status_text = format_queue_status_with_batch_progress(queue_status)
                new_value = available_configs[0] if available_configs else None
                
                return (
                    f"‚úÖ {translate('Config deleted successfully')}: {config_name}.json",
                    gr.update(choices=available_configs, value=new_value),
                    gr.update(value=status_text),
                    gr.update(visible=False),  # Hide confirmation group
                    None,  # Clear operation data
                    gr.update(value="")  # CLEAR the config name input textbox
                )
            else:
                return f"‚ùå {message}", gr.update(), gr.update(), gr.update(visible=False), None, gr.update()
        
        else:
            return "‚ùå Unknown operation type", gr.update(), gr.update(), gr.update(visible=False), None, gr.update()
            
    except Exception as e:
        return f"‚ùå Error confirming operation: {str(e)}", gr.update(), gr.update(), gr.update(visible=False), None, gr.update()

def toggle_lora_full_update(use_lora_val):

    global previous_lora_mode, pending_lora_config_data

    print(translate("üîÑ toggle_lora_full_update called: use_lora={0}").format(use_lora_val))
    
    # Get basic visibility settings
    settings_updates = toggle_lora_settings(use_lora_val)
    
    if not use_lora_val:
        # LoRA disabled - save current mode and clear pending data
        current_mode = getattr(lora_mode, 'value', translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû"))
        if current_mode:
            previous_lora_mode = current_mode
        pending_lora_config_data = None
        print(translate("    LoRA disabled, cleared pending data"))
        return settings_updates + [gr.update(), gr.update(), gr.update()]
    
    # LoRA enabled
    print(translate("    LoRA enabled..."))
    
    # Check for pending configuration
    if pending_lora_config_data is not None:
        print(translate("    Found pending LoRA config for: {0}").format(pending_lora_config_data.get('config_name')))
        
        # Use the already-applied values from the config loading
        if 'applied_values' in pending_lora_config_data:
            lora_dropdown_values = pending_lora_config_data['applied_values']
            choices = scan_lora_directory()  # Fresh scan
            
            print(translate("    Reapplying stored values: {0}").format(lora_dropdown_values))
            
            # Set directory mode with stored values
            settings_updates[0] = gr.update(visible=True, value=translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû"))
            settings_updates[1] = gr.update(visible=False)
            settings_updates[2] = gr.update(visible=True)
            
            dropdown_updates = [
                gr.update(choices=choices, value=lora_dropdown_values[0]),
                gr.update(choices=choices, value=lora_dropdown_values[1]),
                gr.update(choices=choices, value=lora_dropdown_values[2])
            ]
            
            return settings_updates + dropdown_updates
        else:
            # Fallback: reapply from file paths
            lora_files = pending_lora_config_data.get('files', [])
            choices, lora_dropdown_values, applied_files = apply_lora_config_to_dropdowns_safe(lora_files)
            
            settings_updates[0] = gr.update(visible=True, value=translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû"))
            settings_updates[1] = gr.update(visible=False)
            settings_updates[2] = gr.update(visible=True)
            
            dropdown_updates = [
                gr.update(choices=choices, value=lora_dropdown_values[0]),
                gr.update(choices=choices, value=lora_dropdown_values[1]),
                gr.update(choices=choices, value=lora_dropdown_values[2])
            ]
            
            return settings_updates + dropdown_updates
    
    # No pending config - use default behavior
    print(translate("    No pending config, using default behavior"))
    
    if previous_lora_mode == translate("„Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ"):
        settings_updates[0] = gr.update(visible=True, value=translate("„Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ"))
        settings_updates[1] = gr.update(visible=True)
        settings_updates[2] = gr.update(visible=False)
        return settings_updates + [gr.update(), gr.update(), gr.update()]
    else:
        # Default to directory mode
        choices = scan_lora_directory()
        settings_updates[0] = gr.update(visible=True, value=translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû"))
        settings_updates[1] = gr.update(visible=False)
        settings_updates[2] = gr.update(visible=True)
        
        dropdown_updates = [
            gr.update(choices=choices, value=choices[0] if choices else translate("„Å™„Åó")),
            gr.update(choices=choices, value=choices[0] if choices else translate("„Å™„Åó")),
            gr.update(choices=choices, value=choices[0] if choices else translate("„Å™„Åó"))
        ]
        
        return settings_updates + dropdown_updates

def fix_prompt_preset_dropdown_initialization():
    
    # This should be called during UI setup to fix the choices
    try:
        # Get presets data
        from eichi_utils.preset_manager import load_presets
        
        presets_data = load_presets()
        choices = [preset["name"] for preset in presets_data["presets"]]
        
        # Separate default and user presets
        default_presets = [name for name in choices if any(p["name"] == name and p.get("is_default", False) for p in presets_data["presets"])]
        user_presets = [name for name in choices if name not in default_presets]
        
        # Create sorted choices
        sorted_choices = [(name, name) for name in sorted(default_presets) + sorted(user_presets)]
        
        
        # Check if "Ëµ∑ÂãïÊôÇ„Éá„Éï„Ç©„É´„Éà" is in choices
        startup_default = translate("Ëµ∑ÂãïÊôÇ„Éá„Éï„Ç©„É´„Éà")
        choice_names = [choice[1] for choice in sorted_choices]
        
        if startup_default not in choice_names: 
            # Add it if missing
            sorted_choices.insert(0, (startup_default, startup_default))
        # else:
        #     print(f"‚úÖ '{startup_default}' found in preset choices")
        
        return sorted_choices, startup_default
        
    except Exception as e:
        print(translate("Error fixing prompt preset dropdown: {0}").format(e))
        return [], ""

# „Ç§„É°„Éº„Ç∏„Ç≠„É•„Éº„ÅÆ„Åü„ÇÅ„ÅÆÁîªÂÉè„Éï„Ç°„Ç§„É´„É™„Çπ„Éà„ÇíÂèñÂæó„Åô„ÇãÈñ¢Êï∞Ôºà„Ç∞„É≠„Éº„Éê„É´Èñ¢Êï∞Ôºâ
def get_image_queue_files():
    global image_queue_files, input_folder_name_value
    input_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), input_folder_name_value)

    # ÂÖ•Âäõ„Éá„Ç£„É¨„ÇØ„Éà„É™„ÅåÂ≠òÂú®„Åô„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØÔºà„Éú„Çø„É≥Êäº‰∏ãÊôÇ„ÅÆ„Åø‰ΩúÊàê„Åô„Çã„Åü„ÇÅ„ÄÅ„Åì„Åì„Åß„ÅØ‰ΩúÊàê„Åó„Å™„ÅÑÔºâ
    if not os.path.exists(input_dir):
        print(translate("ÂÖ•Âäõ„Éá„Ç£„É¨„ÇØ„Éà„É™„ÅåÂ≠òÂú®„Åó„Åæ„Åõ„Çì: {0}Ôºà‰øùÂ≠òÂèä„Å≥ÂÖ•Âäõ„Éï„Ç©„É´„ÉÄ„ÇíÈñã„Åè„Éú„Çø„É≥„ÇíÊäº„Åô„Å®‰ΩúÊàê„Åï„Çå„Åæ„ÅôÔºâ").format(input_dir))
        return []

    # ÁîªÂÉè„Éï„Ç°„Ç§„É´Ôºàpng, jpg, jpegÔºâ„ÅÆ„Åø„Çí„É™„Çπ„Éà
    image_files = []
    for file in sorted(os.listdir(input_dir)):
        if file.lower().endswith(('.png', '.jpg', '.jpeg')):
            image_path = os.path.join(input_dir, file)
            image_files.append(image_path)

    print(translate("ÂÖ•Âäõ„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÁîªÂÉè„Éï„Ç°„Ç§„É´{0}ÂÄã„ÇíË™≠„ÅøËæº„Åø„Åæ„Åó„Åü").format(len(image_files)))

    image_queue_files = image_files
    return image_files

@torch.no_grad()
def worker(input_image, prompt, n_prompt, seed, total_second_length, latent_window_size, steps, cfg, gs, rs, gpu_memory_preservation, use_teacache, mp4_crf=16, all_padding_value=1.0, image_strength=1.0, keep_section_videos=False, lora_files=None, lora_files2=None, lora_files3=None, lora_scales_text="0.8,0.8,0.8", output_dir=None, save_section_frames=False, use_all_padding=False, use_lora=False, lora_mode=None, lora_dropdown1=None, lora_dropdown2=None, lora_dropdown3=None, save_tensor_data=False, tensor_data_input=None, fp8_optimization=False, resolution=640, batch_index=None, frame_save_mode=None):

    # frame_save_mode„Å´Âü∫„Å•„ÅÑ„Å¶„Éï„É©„Ç∞„ÇíË®≠ÂÆö
    save_latent_frames = False
    save_last_section_frames = False
    
    if frame_save_mode == translate("ÂÖ®„Éï„É¨„Éº„É†ÁîªÂÉè‰øùÂ≠ò"):
        save_latent_frames = True
    elif frame_save_mode == translate("ÊúÄÁµÇ„Çª„ÇØ„Ç∑„Éß„É≥„ÅÆ„ÅøÂÖ®„Éï„É¨„Éº„É†ÁîªÂÉè‰øùÂ≠ò"):
        save_last_section_frames = True

    # ÂÖ•ÂäõÁîªÂÉè„Åæ„Åü„ÅØË°®Á§∫„Åï„Çå„Å¶„ÅÑ„ÇãÊúÄÂæå„ÅÆ„Ç≠„Éº„Éï„É¨„Éº„É†ÁîªÂÉè„ÅÆ„ÅÑ„Åö„Çå„Åã„ÅåÂ≠òÂú®„Åô„Çã„ÅãÁ¢∫Ë™ç
    if isinstance(input_image, str):
        has_any_image = (input_image is not None)
    else:
        has_any_image = (input_image is not None)
    last_visible_section_image = None
    last_visible_section_num = -1

    if not has_any_image and section_settings is not None:
        # ÁèæÂú®„ÅÆÂãïÁîªÈï∑Ë®≠ÂÆö„Åã„ÇâË°®Á§∫„Åï„Çå„Çã„Çª„ÇØ„Ç∑„Éß„É≥Êï∞„ÇíË®àÁÆó
        total_display_sections = None
        try:
            # ÂãïÁîªÈï∑„ÇíÁßíÊï∞„ÅßÂèñÂæó
            seconds = get_video_seconds(total_second_length)

            # „Éï„É¨„Éº„É†„Çµ„Ç§„Ç∫Ë®≠ÂÆö„Åã„Çâlatent_window_size„ÇíË®àÁÆó
            current_latent_window_size = 4.5 if frame_size_setting == "0.5Áßí (17„Éï„É¨„Éº„É†)" else 9
            frame_count = current_latent_window_size * 4 - 3

            # „Çª„ÇØ„Ç∑„Éß„É≥Êï∞„ÇíË®àÁÆó
            total_frames = int(seconds * 30)
            total_display_sections = int(max(round(total_frames / frame_count), 1))
        except Exception as e:
            print(translate("„Çª„ÇØ„Ç∑„Éß„É≥Êï∞Ë®àÁÆó„Ç®„É©„Éº: {0}").format(e))

        # ÊúâÂäπ„Å™„Çª„ÇØ„Ç∑„Éß„É≥Áï™Âè∑„ÇíÂèéÈõÜ
        valid_sections = []
        for section in section_settings:
            if section and len(section) > 1 and section[0] is not None and section[1] is not None:
                try:
                    section_num = int(section[0])
                    # Ë°®Á§∫„Çª„ÇØ„Ç∑„Éß„É≥Êï∞„ÅåË®àÁÆó„Åï„Çå„Å¶„ÅÑ„Çå„Å∞„ÄÅ„Åù„Çå‰ª•‰∏ã„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥„ÅÆ„ÅøËøΩÂä†
                    if total_display_sections is None or section_num < total_display_sections:
                        valid_sections.append((section_num, section[1]))
                except (ValueError, TypeError):
                    continue

        # ÊúâÂäπ„Å™„Çª„ÇØ„Ç∑„Éß„É≥„Åå„ÅÇ„Çå„Å∞„ÄÅÊúÄÂ§ß„ÅÆÁï™Âè∑ÔºàÊúÄÂæå„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥Ôºâ„ÇíÊé¢„Åô
        if valid_sections:
            # Áï™Âè∑„Åß„ÇΩ„Éº„Éà
            valid_sections.sort(key=lambda x: x[0])
            # ÊúÄÂæå„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÂèñÂæó
            last_visible_section_num, last_visible_section_image = valid_sections[-1]

    has_any_image = has_any_image or (last_visible_section_image is not None)
    if not has_any_image:
        raise ValueError("ÂÖ•ÂäõÁîªÂÉè„Åæ„Åü„ÅØË°®Á§∫„Åï„Çå„Å¶„ÅÑ„ÇãÊúÄÂæå„ÅÆ„Ç≠„Éº„Éï„É¨„Éº„É†ÁîªÂÉè„ÅÆ„ÅÑ„Åö„Çå„Åã„ÅåÂøÖË¶Å„Åß„Åô")

    # ÂÖ•ÂäõÁîªÂÉè„Åå„Å™„ÅÑÂ†¥Âêà„ÅØ„Ç≠„Éº„Éï„É¨„Éº„É†ÁîªÂÉè„Çí‰ΩøÁî®
    if input_image is None and last_visible_section_image is not None:
        print(translate("ÂÖ•ÂäõÁîªÂÉè„ÅåÊåáÂÆö„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ„Åü„ÇÅ„ÄÅ„Çª„ÇØ„Ç∑„Éß„É≥{0}„ÅÆ„Ç≠„Éº„Éï„É¨„Éº„É†ÁîªÂÉè„Çí‰ΩøÁî®„Åó„Åæ„Åô").format(last_visible_section_num))
        input_image = last_visible_section_image

    # Âá∫Âäõ„Éï„Ç©„É´„ÉÄ„ÅÆË®≠ÂÆö
    global outputs_folder
    global output_folder_name
    if output_dir and output_dir.strip():
        # Âá∫Âäõ„Éï„Ç©„É´„ÉÄ„Éë„Çπ„ÇíÂèñÂæó
        outputs_folder = get_output_folder_path(output_dir)
        print(translate("Âá∫Âäõ„Éï„Ç©„É´„ÉÄ„ÇíË®≠ÂÆö: {0}").format(outputs_folder))

        # „Éï„Ç©„É´„ÉÄÂêç„ÅåÁèæÂú®„ÅÆË®≠ÂÆö„Å®Áï∞„Å™„ÇãÂ†¥Âêà„ÅØË®≠ÂÆö„Éï„Ç°„Ç§„É´„ÇíÊõ¥Êñ∞
        if output_dir != output_folder_name:
            settings = load_settings()
            settings['output_folder'] = output_dir
            if save_settings(settings):
                output_folder_name = output_dir
                print(translate("Âá∫Âäõ„Éï„Ç©„É´„ÉÄË®≠ÂÆö„Çí‰øùÂ≠ò„Åó„Åæ„Åó„Åü: {0}").format(output_dir))
    else:
        # „Éá„Éï„Ç©„É´„ÉàË®≠ÂÆö„Çí‰ΩøÁî®
        outputs_folder = get_output_folder_path(output_folder_name)
        print(translate("„Éá„Éï„Ç©„É´„ÉàÂá∫Âäõ„Éï„Ç©„É´„ÉÄ„Çí‰ΩøÁî®: {0}").format(outputs_folder))

    # „Éï„Ç©„É´„ÉÄ„ÅåÂ≠òÂú®„Åó„Å™„ÅÑÂ†¥Âêà„ÅØ‰ΩúÊàê
    os.makedirs(outputs_folder, exist_ok=True)

    # Âá¶ÁêÜÊôÇÈñìË®àÊ∏¨„ÅÆÈñãÂßã
    process_start_time = time.time()

    # „Ç∞„É≠„Éº„Éê„É´Â§âÊï∞„ÅßÁä∂ÊÖãÁÆ°ÁêÜ„Åó„Å¶„ÅÑ„Çã„É¢„Éá„É´Â§âÊï∞„ÇíÂÆ£Ë®Ä„Åô„Çã
    global transformer, text_encoder, text_encoder_2

    # text_encoder„Å®text_encoder_2„ÇíÁ¢∫ÂÆü„Å´„É≠„Éº„Éâ
    if not text_encoder_manager.ensure_text_encoder_state():
        raise Exception(translate("text_encoder„Å®text_encoder_2„ÅÆÂàùÊúüÂåñ„Å´Â§±Êïó„Åó„Åæ„Åó„Åü"))
    text_encoder, text_encoder_2 = text_encoder_manager.get_text_encoders()

    # Êó¢Â≠ò„ÅÆË®àÁÆóÊñπÊ≥ï„Çí‰øùÊåÅ„Åó„Å§„Å§„ÄÅË®≠ÂÆö„Åã„Çâ„Çª„ÇØ„Ç∑„Éß„É≥Êï∞„ÇÇÂèñÂæó„Åô„Çã
    total_latent_sections = (total_second_length * 30) / (latent_window_size * 4)
    total_latent_sections = int(max(round(total_latent_sections), 1))

    # ÁèæÂú®„ÅÆ„É¢„Éº„Éâ„ÇíÂèñÂæóÔºàUI„Åã„ÇâÊ∏°„Åï„Çå„ÅüÊÉÖÂ†±„Åã„ÇâÔºâ
    # „Çª„ÇØ„Ç∑„Éß„É≥Êï∞„ÇíÂÖ®„Çª„ÇØ„Ç∑„Éß„É≥Êï∞„Å®„Åó„Å¶‰øùÂ≠ò
    total_sections = total_latent_sections


    #Get config file name
    def get_job_id_with_config_name(batch_index=None):
        """Generate job ID with config name if processing queue, otherwise use timestamp"""
        global current_processing_config_name
        
        batch_suffix = f"_batch{batch_index+1}" if batch_index is not None else ""
        
        if current_processing_config_name:
            # Queue processing - use config name + timestamp
            timestamp = generate_timestamp()
            job_id = f"{current_processing_config_name}_{timestamp}{batch_suffix}"
            print(translate("üìÅ Queue video naming: {0}").format(job_id))
        else:
            # Manual processing - use original naming
            job_id = generate_timestamp() + batch_suffix
            print(translate("üìÅ Manual video naming: {0}").format(job_id))
        
        return job_id

    # Then in the worker function, replace the job_id line with:
    job_id = get_job_id_with_config_name(batch_index)

    # ÁèæÂú®„ÅÆ„Éê„ÉÉ„ÉÅÁï™Âè∑„ÅåÊåáÂÆö„Åï„Çå„Å¶„ÅÑ„Çå„Å∞‰ΩøÁî®„Åô„Çã
    # endframe_ichi„ÅÆ‰ªïÊßò„Å´Âêà„Çè„Åõ„Å¶+1„Åó„ÅüÂÄ§„Çí‰ΩøÁî®
    batch_suffix = f"_batch{batch_index+1}" if batch_index is not None else ""
    #job_id = generate_timestamp() + batch_suffix

    # F1„É¢„Éº„Éâ„Åß„ÅØÈ†ÜÁîüÊàê„ÇíË°å„ÅÜ„Åü„ÇÅ„ÄÅlatent_paddings„ÅÆ„É≠„Ç∏„ÉÉ„ÇØ„ÅØ‰ΩøÁî®„Åó„Å™„ÅÑ
    # ÂÖ®„Çª„ÇØ„Ç∑„Éß„É≥Êï∞„ÇíË®≠ÂÆö
    total_sections = total_latent_sections
    
    # Ê≠£Á¢∫„Å™„Çª„ÇØ„Ç∑„Éß„É≥Êï∞„ÅÆÂÜçË®àÁÆó„Å®Á¢∫Ë™çÔºà„Éà„É©„Éñ„É´„Ç∑„É•„Éº„ÉÜ„Ç£„É≥„Ç∞Áî®Ôºâ
    if total_second_length > 0:
        sections_by_frames = int(max(round((total_second_length * 30) / (latent_window_size * 4 - 3)), 1))
        if sections_by_frames != total_sections:
            print(translate("„Çª„ÇØ„Ç∑„Éß„É≥Êï∞„Å´‰∏ç‰∏ÄËá¥„Åå„ÅÇ„Çä„Åæ„ÅôÔºÅË®àÁÆóÂÄ§„ÇíÂÑ™ÂÖà„Åó„Åæ„Åô"))
            total_sections = sections_by_frames

    print(translate("„Çª„ÇØ„Ç∑„Éß„É≥ÁîüÊàêË©≥Á¥∞ (F1„É¢„Éº„Éâ):"))
    print(translate("  - ÂêàË®à„Çª„ÇØ„Ç∑„Éß„É≥Êï∞: {0} (ÊúÄÁµÇÁ¢∫ÂÆöÂÄ§)").format(total_sections))
    frame_count = latent_window_size * 4 - 3
    print(translate("  - ÂêÑ„Çª„ÇØ„Ç∑„Éß„É≥„ÅÆ„Éï„É¨„Éº„É†Êï∞: Á¥Ñ{0}„Éï„É¨„Éº„É† (latent_window_size: {1})").format(frame_count, latent_window_size))


    # All stream.output_queue.push() calls should now go through the proxy correctly
    stream.output_queue.push(('progress', (None, '', make_progress_bar_html(0, 'Starting ...'))))

    try:
        # F1„É¢„Éº„Éâ„ÅÆ„Éó„É≠„É≥„Éó„ÉàÂá¶ÁêÜ
        section_map = None
        section_numbers_sorted = []

        # Clean GPU
        if not high_vram:
            # „É¢„Éá„É´„ÇíCPU„Å´„Ç¢„É≥„É≠„Éº„Éâ
            unload_complete_models(
                image_encoder, vae
            )

        # Text encoding

        stream.output_queue.push(('progress', (None, '', make_progress_bar_html(0, translate("Text encoding ...")))))

        if not high_vram:
            fake_diffusers_current_device(text_encoder, gpu)  # since we only encode one text - that is one model move and one encode, offload is same time consumption since it is also one load and one encode.
            load_model_as_complete(text_encoder_2, target_device=gpu)
        
        # „Ç§„É°„Éº„Ç∏„Ç≠„É•„Éº„ÅÆ„Ç´„Çπ„Çø„É†„Éó„É≠„É≥„Éó„Éà„ÅÆÂ†¥Âêà„ÄÅË©≥Á¥∞„É≠„Ç∞„ÇíËøΩÂä†
        # „É™„ÇØ„Ç®„Çπ„ÉàÂÜÖ„ÅÆ„Ç≠„É•„ÉºÊÉÖÂ†±„Åã„Çâ„Ç´„Çπ„Çø„É†„Éó„É≠„É≥„Éó„Éà„ÅÆ‰ΩøÁî®„Çí„ÅÑ„Å§„Åß„ÇÇÁ¢∫Ë™ç„Åß„Åç„Çã„Çà„ÅÜÂ§âÊï∞„Åã„Çâ„ÉÅ„Çß„ÉÉ„ÇØ
        # „É≠„Éº„Ç´„É´Â§âÊï∞„Åß„ÅØ„Å™„Åè„ÄÅ„Ç∞„É≠„Éº„Éê„É´Ë®≠ÂÆö„Åã„ÇâÁ¢∫Ë™ç„Åô„Çã
        using_custom_txt = False
        if queue_enabled and queue_type == "image" and batch_index is not None and batch_index > 0:
            if batch_index - 1 < len(image_queue_files):
                img_path = image_queue_files[batch_index - 1]
                txt_path = os.path.splitext(img_path)[0] + ".txt"
                if os.path.exists(txt_path):
                    using_custom_txt = True
        
        # ÂÆüÈöõ„Å´‰ΩøÁî®„Åï„Çå„Çã„Éó„É≠„É≥„Éó„Éà„ÇíÂøÖ„ÅöË°®Á§∫
        actual_prompt = prompt  # ÂÆüÈöõ„Å´‰ΩøÁî®„Åô„Çã„Éó„É≠„É≥„Éó„Éà
        prompt_source = translate("ÂÖ±ÈÄö„Éó„É≠„É≥„Éó„Éà")  # „Éó„É≠„É≥„Éó„Éà„ÅÆÁ®ÆÈ°û

        # „Éó„É≠„É≥„Éó„Éà„ÇΩ„Éº„Çπ„ÅÆÂà§ÂÆö
        if queue_enabled and queue_type == "prompt" and batch_index is not None:
            # „Éó„É≠„É≥„Éó„Éà„Ç≠„É•„Éº„ÅÆÂ†¥Âêà
            prompt_source = translate("„Éó„É≠„É≥„Éó„Éà„Ç≠„É•„Éº")
            print(translate("„Éó„É≠„É≥„Éó„Éà„Ç≠„É•„Éº„Åã„Çâ„ÅÆ„Éó„É≠„É≥„Éó„Éà„Çí„Ç®„É≥„Ç≥„Éº„Éâ„Åó„Å¶„ÅÑ„Åæ„Åô..."))
        elif using_custom_txt:
            # „Ç§„É°„Éº„Ç∏„Ç≠„É•„Éº„ÅÆ„Ç´„Çπ„Çø„É†„Éó„É≠„É≥„Éó„Éà„ÅÆÂ†¥Âêà
            actual_prompt = prompt  # „Ç´„Çπ„Çø„É†„Éó„É≠„É≥„Éó„Éà„Çí‰ΩøÁî®
            prompt_source = translate("„Ç´„Çπ„Çø„É†„Éó„É≠„É≥„Éó„Éà(„Ç§„É°„Éº„Ç∏„Ç≠„É•„Éº)")
            print(translate("„Ç´„Çπ„Çø„É†„Éó„É≠„É≥„Éó„Éà„Çí„Ç®„É≥„Ç≥„Éº„Éâ„Åó„Å¶„ÅÑ„Åæ„Åô..."))
        else:
            # ÈÄöÂ∏∏„ÅÆÂÖ±ÈÄö„Éó„É≠„É≥„Éó„Éà„ÅÆÂ†¥Âêà
            print(translate("ÂÖ±ÈÄö„Éó„É≠„É≥„Éó„Éà„Çí„Ç®„É≥„Ç≥„Éº„Éâ„Åó„Å¶„ÅÑ„Åæ„Åô..."))
        
        # „Éó„É≠„É≥„Éó„Éà„ÅÆÂÜÖÂÆπ„Å®„ÇΩ„Éº„Çπ„ÇíË°®Á§∫
        print(translate("„Éó„É≠„É≥„Éó„ÉàÊÉÖÂ†±: „ÇΩ„Éº„Çπ: {0}").format(prompt_source))
        print(translate("„Éó„É≠„É≥„Éó„ÉàÊÉÖÂ†±: ÂÜÖÂÆπ: {0}").format(actual_prompt))
        
        llama_vec, clip_l_pooler = encode_prompt_conds(prompt, text_encoder, text_encoder_2, tokenizer, tokenizer_2)

        if cfg == 1:
            llama_vec_n, clip_l_pooler_n = torch.zeros_like(llama_vec), torch.zeros_like(clip_l_pooler)
        else:
            llama_vec_n, clip_l_pooler_n = encode_prompt_conds(n_prompt, text_encoder, text_encoder_2, tokenizer, tokenizer_2)

        llama_vec, llama_attention_mask = crop_or_pad_yield_mask(llama_vec, length=512)
        llama_vec_n, llama_attention_mask_n = crop_or_pad_yield_mask(llama_vec_n, length=512)


        # „Åì„Çå‰ª•Èôç„ÅÆÂá¶ÁêÜ„ÅØ text_encoder, text_encoder_2 „ÅØ‰∏çË¶Å„Å™„ÅÆ„Åß„ÄÅ„É°„É¢„É™Ëß£Êîæ„Åó„Å¶„Åó„Åæ„Å£„Å¶Êßã„Çè„Å™„ÅÑ
        if not high_vram:
            text_encoder, text_encoder_2 = None, None
            text_encoder_manager.dispose_text_encoders()

        # „ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„ÅÆ„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åå„ÅÇ„Çå„Å∞Ë™≠„ÅøËæº„Åø
        uploaded_tensor = None
        if tensor_data_input is not None:
            try:
                # „É™„Çπ„ÉàÂûã„ÅÆÂ†¥Âêà„ÄÅÊúÄÂàù„ÅÆË¶ÅÁ¥†„ÇíÂèñÂæó
                if isinstance(tensor_data_input, list):
                    if tensor_data_input and hasattr(tensor_data_input[0], 'name'):
                        tensor_data_input = tensor_data_input[0]
                    else:
                        tensor_data_input = None
                
                if tensor_data_input is not None and hasattr(tensor_data_input, 'name'):
                    tensor_path = tensor_data_input.name
                    print(translate("„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„ÇíË™≠„ÅøËæº„Åø: {0}").format(os.path.basename(tensor_path)))
                    stream.output_queue.push(('progress', (None, '', make_progress_bar_html(0, translate('Loading tensor data ...')))))

                    # safetensors„Åã„Çâ„ÉÜ„É≥„ÇΩ„É´„ÇíË™≠„ÅøËæº„Åø
                    tensor_dict = sf.load_file(tensor_path)

                    # „ÉÜ„É≥„ÇΩ„É´„Å´Âê´„Åæ„Çå„Å¶„ÅÑ„Çã„Ç≠„Éº„Å®„Ç∑„Çß„Ç§„Éó„ÇíÁ¢∫Ë™ç
                    print(translate("„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„ÅÆÂÜÖÂÆπ:"))
                    for key, tensor in tensor_dict.items():
                        print(translate("  - {0}: shape={1}, dtype={2}").format(key, tensor.shape, tensor.dtype))

                    # history_latents„Å®Âëº„Å∞„Çå„Çã„Ç≠„Éº„ÅåÂ≠òÂú®„Åô„Çã„ÅãÁ¢∫Ë™ç
                    if "history_latents" in tensor_dict:
                        uploaded_tensor = tensor_dict["history_latents"]
                        print(translate("„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„ÇøË™≠„ÅøËæº„ÅøÊàêÂäü: shape={0}, dtype={1}").format(uploaded_tensor.shape, uploaded_tensor.dtype))
                        stream.output_queue.push(('progress', (None, translate('Tensor data loaded successfully!'), make_progress_bar_html(10, translate('Tensor data loaded successfully!')))))
                    else:
                        print(translate("Ë≠¶Âëä: „ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„Å´ 'history_latents' „Ç≠„Éº„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì"))
            except Exception as e:
                print(translate("„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„ÇøË™≠„ÅøËæº„Åø„Ç®„É©„Éº: {0}").format(e))
                traceback.print_exc()

        stream.output_queue.push(('progress', (None, '', make_progress_bar_html(0, translate("Image processing ...")))))

        def preprocess_image(img_path_or_array, resolution=640):
            """Path„Åæ„Åü„ÅØÁîªÂÉèÈÖçÂàó„ÇíÂá¶ÁêÜ„Åó„Å¶ÈÅ©Âàá„Å™„Çµ„Ç§„Ç∫„Å´Â§âÊèõ„Åô„Çã"""
            if img_path_or_array is None:
                # ÁîªÂÉè„Åå„Å™„ÅÑÂ†¥Âêà„ÅØÊåáÂÆöËß£ÂÉèÂ∫¶„ÅÆÈªí„ÅÑÁîªÂÉè„ÇíÁîüÊàê
                img = np.zeros((resolution, resolution, 3), dtype=np.uint8)
                height = width = resolution
                return img, img, height, width

            # Tensor„Åã„ÇâNumPy„Å∏Â§âÊèõ„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çå„Å∞Ë°å„ÅÜ
            if isinstance(img_path_or_array, torch.Tensor):
                img_path_or_array = img_path_or_array.cpu().numpy()

            # Path„ÅÆÂ†¥Âêà„ÅØPIL„ÅßÁîªÂÉè„ÇíÈñã„Åè
            if isinstance(img_path_or_array, str) and os.path.exists(img_path_or_array):
                img = np.array(Image.open(img_path_or_array).convert('RGB'))
            else:
                # NumPyÈÖçÂàó„ÅÆÂ†¥Âêà„ÅØ„Åù„ÅÆ„Åæ„Åæ‰Ωø„ÅÜ
                img = img_path_or_array

            H, W, C = img.shape
            # Ëß£ÂÉèÂ∫¶„Éë„É©„É°„Éº„Çø„Çí‰ΩøÁî®„Åó„Å¶„Çµ„Ç§„Ç∫„ÇíÊ±∫ÂÆö
            height, width = find_nearest_bucket(H, W, resolution=resolution)
            img_np = resize_and_center_crop(img, target_width=width, target_height=height)
            img_pt = torch.from_numpy(img_np).float() / 127.5 - 1
            img_pt = img_pt.permute(2, 0, 1)[None, :, None]
            return img_np, img_pt, height, width

        # „Éê„ÉÉ„ÉÅÂá¶ÁêÜ„ÅßÂØæÂøú„Åô„Çã„Åü„ÇÅ„Å´ÂÖ•ÂäõÁîªÂÉè„Çí‰ΩøÁî®
        # workerÈñ¢Êï∞„Å´Ê∏°„Åï„Çå„ÇãÂÖ•ÂäõÁîªÂÉè„ÇíÁõ¥Êé•‰ΩøÁî®Ôºàinput_imageÔºâ
        input_image_np, input_image_pt, height, width = preprocess_image(input_image, resolution=resolution)

        # ÂÖ•ÂäõÁîªÂÉè„Å´„É°„Çø„Éá„Éº„Çø„ÇíÂüã„ÇÅËæº„Çì„Åß‰øùÂ≠ò
        # endframe_ichi„ÅÆ‰ªïÊßò„Å´ÂÆåÂÖ®„Å´Âêà„Çè„Åõ„Çã - „Éê„ÉÉ„ÉÅÁï™Âè∑„ÇíËøΩÂä†„Åó„Å™„ÅÑ
        initial_image_path = os.path.join(outputs_folder, f'{job_id}.png')
        Image.fromarray(input_image_np).save(initial_image_path)

        # „É°„Çø„Éá„Éº„Çø„ÅÆÂüã„ÇÅËæº„Åø
        metadata = {
            PROMPT_KEY: prompt,
            SEED_KEY: seed
        }
        embed_metadata_to_png(initial_image_path, metadata)

        # VAE encoding

        stream.output_queue.push(('progress', (None, '', make_progress_bar_html(0, translate("VAE encoding ...")))))

        if not high_vram:
            load_model_as_complete(vae, target_device=gpu)

        # „Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åï„Çå„Åü„ÉÜ„É≥„ÇΩ„É´„Åå„ÅÇ„Å£„Å¶„ÇÇ„ÄÅÂ∏∏„Å´ÂÖ•ÂäõÁîªÂÉè„Åã„ÇâÈÄöÂ∏∏„ÅÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞„ÇíË°å„ÅÜ
        # „ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„ÅØÂæå„ÅßÂæå‰ªò„Åë„Å®„Åó„Å¶‰ΩøÁî®„Åô„Çã„Åü„ÇÅ„Å´‰øùÊåÅ„Åó„Å¶„Åä„Åè
        if uploaded_tensor is not None:
            print(translate("„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åï„Çå„Åü„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„ÇíÊ§úÂá∫: ÂãïÁîªÁîüÊàêÂæå„Å´ÂæåÊñπ„Å´ÁµêÂêà„Åó„Åæ„Åô"))
            # ÂÖ•ÂäõÁîªÂÉè„ÅåNone„ÅÆÂ†¥Âêà„ÄÅ„ÉÜ„É≥„ÇΩ„É´„Åã„Çâ„Éá„Ç≥„Éº„Éâ„Åó„Å¶Ë°®Á§∫ÁîªÂÉè„ÇíÁîüÊàê
            if input_image is None:
                try:
                    # „ÉÜ„É≥„ÇΩ„É´„ÅÆÊúÄÂàù„ÅÆ„Éï„É¨„Éº„É†„Åã„ÇâÁîªÂÉè„Çí„Éá„Ç≥„Éº„Éâ„Åó„Å¶Ë°®Á§∫Áî®„Å´‰ΩøÁî®
                    preview_latent = uploaded_tensor[:, :, 0:1, :, :].clone()
                    if preview_latent.device != torch.device('cpu'):
                        preview_latent = preview_latent.cpu()
                    if preview_latent.dtype != torch.float16:
                        preview_latent = preview_latent.to(dtype=torch.float16)

                    decoded_image = vae_decode(preview_latent, vae)
                    decoded_image = (decoded_image[0, :, 0] * 127.5 + 127.5).permute(1, 2, 0).cpu().numpy().clip(0, 255).astype(np.uint8)
                    # „Éá„Ç≥„Éº„Éâ„Åó„ÅüÁîªÂÉè„Çí‰øùÂ≠ò
                    Image.fromarray(decoded_image).save(os.path.join(outputs_folder, f'{job_id}_tensor_preview.png'))
                    # „Éá„Ç≥„Éº„Éâ„Åó„ÅüÁîªÂÉè„ÇíÂÖ•ÂäõÁîªÂÉè„Å®„Åó„Å¶Ë®≠ÂÆö
                    input_image = decoded_image
                    # ÂâçÂá¶ÁêÜÁî®„ÅÆ„Éá„Éº„Çø„ÇÇÁîüÊàê
                    input_image_np, input_image_pt, height, width = preprocess_image(input_image)
                    print(translate("„ÉÜ„É≥„ÇΩ„É´„Åã„Çâ„Éá„Ç≥„Éº„Éâ„Åó„ÅüÁîªÂÉè„ÇíÁîüÊàê„Åó„Åæ„Åó„Åü: {0}x{1}").format(height, width))
                except Exception as e:
                    print(translate("„ÉÜ„É≥„ÇΩ„É´„Åã„Çâ„ÅÆ„Éá„Ç≥„Éº„Éâ‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {0}").format(e))
                    # „Éá„Ç≥„Éº„Éâ„Å´Â§±Êïó„Åó„ÅüÂ†¥Âêà„ÅØÈÄöÂ∏∏„ÅÆÂá¶ÁêÜ„ÇíÁ∂öË°å

            # UI‰∏ä„Åß„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„ÅÆÊÉÖÂ†±„ÇíË°®Á§∫
            tensor_info = translate("„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø ({0}„Éï„É¨„Éº„É†) „ÇíÊ§úÂá∫„Åó„Åæ„Åó„Åü„ÄÇÂãïÁîªÁîüÊàêÂæå„Å´ÂæåÊñπ„Å´ÁµêÂêà„Åó„Åæ„Åô„ÄÇ").format(uploaded_tensor.shape[2])
            stream.output_queue.push(('progress', (None, tensor_info, make_progress_bar_html(10, translate('„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„ÇíÂæåÊñπ„Å´ÁµêÂêà')))))

        # Â∏∏„Å´ÂÖ•ÂäõÁîªÂÉè„Åã„ÇâÈÄöÂ∏∏„ÅÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞„ÇíË°å„ÅÜ
        start_latent = vae_encode(input_image_pt, vae)

        # Á∞°Áï•ÂåñË®≠Ë®à: section_latentsÊ©üËÉΩ„ÇíÂâäÈô§

        # CLIP Vision

        stream.output_queue.push(('progress', (None, '', make_progress_bar_html(0, translate("CLIP Vision encoding ...")))))

        if not high_vram:
            load_model_as_complete(image_encoder, target_device=gpu)

        image_encoder_output = hf_clip_vision_encode(input_image_np, feature_extractor, image_encoder)
        image_encoder_last_hidden_state = image_encoder_output.last_hidden_state

        # Dtype

        llama_vec = llama_vec.to(transformer.dtype)
        llama_vec_n = llama_vec_n.to(transformer.dtype)
        clip_l_pooler = clip_l_pooler.to(transformer.dtype)
        clip_l_pooler_n = clip_l_pooler_n.to(transformer.dtype)
        image_encoder_last_hidden_state = image_encoder_last_hidden_state.to(transformer.dtype)

        # Sampling

        stream.output_queue.push(('progress', (None, '', make_progress_bar_html(0, translate("Start sampling ...")))))

        rnd = torch.Generator("cpu").manual_seed(seed)
        # latent_window_size„Åå4.5„ÅÆÂ†¥Âêà„ÅØÁâπÂà•„Å´17„Éï„É¨„Éº„É†„Å®„Åô„Çã
        if latent_window_size == 4.5:
            num_frames = 17  # 5 * 4 - 3 = 17
        else:
            num_frames = int(latent_window_size * 4 - 3)

        # ÂàùÊúü„Éï„É¨„Éº„É†Ê∫ñÂÇô
        history_latents = torch.zeros(size=(1, 16, 16 + 2 + 1, height // 8, width // 8), dtype=torch.float32).cpu()
        history_pixels = None

        # ÈñãÂßã„Éï„É¨„Éº„É†„Çíhistory_latents„Å´ËøΩÂä†
        history_latents = torch.cat([history_latents, start_latent.to(history_latents)], dim=2)
        total_generated_latent_frames = 1  # ÊúÄÂàù„ÅÆ„Éï„É¨„Éº„É†„ÇíÂê´„ÇÄ„ÅÆ„Åß1„Åã„ÇâÈñãÂßã

        # -------- LoRA Ë®≠ÂÆö START ---------

        # UIË®≠ÂÆö„ÅÆuse_lora„Éï„É©„Ç∞ÂÄ§„Çí‰øùÂ≠ò
        original_use_lora = use_lora

        # LoRA„ÅÆÁí∞Â¢ÉÂ§âÊï∞Ë®≠ÂÆöÔºàPYTORCH_CUDA_ALLOC_CONFÔºâ
        if "PYTORCH_CUDA_ALLOC_CONF" not in os.environ:
            old_env = os.environ.get("PYTORCH_CUDA_ALLOC_CONF", "")
            os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
            print(translate("CUDAÁí∞Â¢ÉÂ§âÊï∞Ë®≠ÂÆö: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True (ÂÖÉ„ÅÆÂÄ§: {0})").format(old_env))

        # Ê¨°Âõû„ÅÆtransformerË®≠ÂÆö„ÇíÊõ¥Êñ∞
        current_lora_paths = []
        current_lora_scales = []
        
        if use_lora and has_lora_support:
            # „É¢„Éº„Éâ„Å´Âøú„Åò„Å¶LoRA„Éï„Ç°„Ç§„É´„ÇíÂá¶ÁêÜ
            if lora_mode == translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû"):
                print(translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû„É¢„Éº„Éâ„ÅßLoRA„ÇíÂá¶ÁêÜ„Åó„Åæ„Åô"))
                # „Éâ„É≠„ÉÉ„Éó„ÉÄ„Ç¶„É≥„ÅÆÂÄ§„ÇíÂèñÂæó
                for dropdown in [lora_dropdown1, lora_dropdown2, lora_dropdown3]:
                    if dropdown is not None and dropdown != translate("„Å™„Åó") and dropdown != 0:
                        # „Å™„Åó‰ª•Â§ñ„ÅåÈÅ∏Êäû„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅ„Éë„Çπ„ÇíÁîüÊàê
                        lora_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'lora')
                        lora_path = os.path.join(lora_dir, dropdown)
                        if os.path.exists(lora_path):
                            current_lora_paths.append(lora_path)
                            print(translate("LoRA„Éï„Ç°„Ç§„É´„ÇíËøΩÂä†: {0}").format(lora_path))
                        else:
                            print(translate("LoRA„Éï„Ç°„Ç§„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì: {0}").format(lora_path))
                
                # Êï∞ÂÄ§0„ÅÆÁâπÂà•Âá¶ÁêÜÔºà„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ0„ÅÆË¶ÅÁ¥†„Å®„Åó„Å¶Ëß£ÈáàÔºâ
                if lora_dropdown2 == 0:
                    try:
                        # „Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû„ÅåÂÖ•„Å£„Å¶„Çã„ÅØ„Åö„Å™„ÅÆ„Åß„ÄÅÈÅ∏ÊäûËÇ¢„Åã„Çâ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ0„ÅÆÈ†ÖÁõÆÔºà„Å™„ÅóÔºâ„ÇíÂèñÂæó
                        choices = scan_lora_directory()
                        if choices and len(choices) > 0:
                            if choices[0] != translate("„Å™„Åó"):
                                print(translate("‰∫àÊúü„Åó„Å™„ÅÑÈÅ∏ÊäûËÇ¢„É™„Çπ„Éà: ÊúÄÂàù„ÅÆË¶ÅÁ¥†„Åå„Äå„Å™„Åó„Äç„Åß„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì: {0}").format(choices[0]))
                    except Exception as e:
                        print(translate("„Éâ„É≠„ÉÉ„Éó„ÉÄ„Ç¶„É≥2„ÅÆÁâπÂà•Âá¶ÁêÜ„Åß„Ç®„É©„Éº: {0}").format(e))
            else:
                # „Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„É¢„Éº„Éâ
                print(translate("„Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„É¢„Éº„Éâ„ÅßLoRA„ÇíÂá¶ÁêÜ„Åó„Åæ„Åô"))
                # LoRA„Éï„Ç°„Ç§„É´„ÇíÂèéÈõÜ
                if lora_files is not None:
                    if isinstance(lora_files, list):
                        # Ë§áÊï∞„ÅÆLoRA„Éï„Ç°„Ç§„É´ÔºàÂ∞ÜÊù•„ÅÆGradio„Éê„Éº„Ç∏„Éß„É≥Áî®Ôºâ
                        current_lora_paths.extend([file.name for file in lora_files])
                    else:
                        # Âçò‰∏Ä„ÅÆLoRA„Éï„Ç°„Ç§„É´
                        current_lora_paths.append(lora_files.name)
                
                # 2„Å§ÁõÆ„ÅÆLoRA„Éï„Ç°„Ç§„É´„Åå„ÅÇ„Çå„Å∞ËøΩÂä†
                if lora_files2 is not None:
                    if isinstance(lora_files2, list):
                        # Ë§áÊï∞„ÅÆLoRA„Éï„Ç°„Ç§„É´ÔºàÂ∞ÜÊù•„ÅÆGradio„Éê„Éº„Ç∏„Éß„É≥Áî®Ôºâ
                        current_lora_paths.extend([file.name for file in lora_files2])
                    else:
                        # Âçò‰∏Ä„ÅÆLoRA„Éï„Ç°„Ç§„É´
                        current_lora_paths.append(lora_files2.name)
                
                # 3„Å§ÁõÆ„ÅÆLoRA„Éï„Ç°„Ç§„É´„Åå„ÅÇ„Çå„Å∞ËøΩÂä†ÔºàF1Áâà„Åß„ÇÇÂØæÂøúÔºâ
                if lora_files3 is not None:
                    if isinstance(lora_files3, list):
                        current_lora_paths.extend([file.name for file in lora_files3])
                    else:
                        current_lora_paths.append(lora_files3.name)
            
            # „Çπ„Ç±„Éº„É´ÂÄ§„Çí„ÉÜ„Ç≠„Çπ„Éà„Åã„ÇâËß£Êûê
            if current_lora_paths:  # LoRA„Éë„Çπ„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅÆ„ÅøËß£Êûê
                try:
                    scales_text = lora_scales_text.strip()
                    if scales_text:
                        # „Ç´„É≥„ÉûÂå∫Âàá„Çä„ÅÆ„Çπ„Ç±„Éº„É´ÂÄ§„ÇíËß£Êûê
                        scales = [float(scale.strip()) for scale in scales_text.split(',')]
                        current_lora_scales = scales
                        
                        # Ë∂≥„Çä„Å™„ÅÑÂ†¥Âêà„ÅØ0.8„ÅßÂüã„ÇÅ„Çã
                        if len(scales) < len(current_lora_paths):
                            current_lora_scales.extend([0.8] * (len(current_lora_paths) - len(scales)))
                    else:
                        # „Çπ„Ç±„Éº„É´ÂÄ§„ÅåÊåáÂÆö„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑÂ†¥Âêà„ÅØÂÖ®„Å¶0.8„Çí‰ΩøÁî®
                        current_lora_scales = [0.8] * len(current_lora_paths)
                except Exception as e:
                    print(translate("LoRA„Çπ„Ç±„Éº„É´Ëß£Êûê„Ç®„É©„Éº: {0}").format(e))
                    print(translate("„Éá„Éï„Ç©„É´„Éà„Çπ„Ç±„Éº„É´ 0.8 „Çí‰ΩøÁî®„Åó„Åæ„Åô"))
                    current_lora_scales = [0.8] * len(current_lora_paths)
                
                # „Çπ„Ç±„Éº„É´ÂÄ§„ÅÆÊï∞„ÅåLoRA„Éë„Çπ„ÅÆÊï∞„Å®‰∏ÄËá¥„Åó„Å™„ÅÑÂ†¥Âêà„ÅØË™øÊï¥
                if len(current_lora_scales) < len(current_lora_paths):
                    # Ë∂≥„Çä„Å™„ÅÑÂàÜ„ÅØ0.8„ÅßÂüã„ÇÅ„Çã
                    current_lora_scales.extend([0.8] * (len(current_lora_paths) - len(current_lora_scales)))
                elif len(current_lora_scales) > len(current_lora_paths):
                    # ‰ΩôÂàÜ„ÅØÂàá„ÇäÊç®„Å¶
                    current_lora_scales = current_lora_scales[:len(current_lora_paths)]
        
        # UI„ÅßLoRA‰ΩøÁî®„ÅåÊúâÂäπ„Å´„Å™„Å£„Å¶„ÅÑ„ÅüÂ†¥Âêà„ÄÅ„Éï„Ç°„Ç§„É´ÈÅ∏Êäû„Å´Èñ¢„Çè„Çâ„ÅöÂº∑Âà∂ÁöÑ„Å´ÊúâÂäπÂåñ
        if original_use_lora:
            use_lora = True
            print(translate("UI„ÅßLoRA‰ΩøÁî®„ÅåÊúâÂäπÂåñ„Åï„Çå„Å¶„ÅÑ„Çã„Åü„ÇÅ„ÄÅLoRA‰ΩøÁî®„ÇíÊúâÂäπ„Å´„Åó„Åæ„Åô"))

        # LoRAË®≠ÂÆö„ÇíÊõ¥Êñ∞Ôºà„É™„É≠„Éº„Éâ„ÅØË°å„Çè„Å™„ÅÑÔºâ
        transformer_manager.set_next_settings(
            lora_paths=current_lora_paths,
            lora_scales=current_lora_scales,
            fp8_enabled=fp8_optimization,  # fp8_enabled„Éë„É©„É°„Éº„Çø„ÇíËøΩÂä†
            high_vram_mode=high_vram,
            force_dict_split=True  # Â∏∏„Å´ËæûÊõ∏ÂàÜÂâ≤Âá¶ÁêÜ„ÇíË°å„ÅÜ
        )

        # -------- LoRA Ë®≠ÂÆö END ---------

        # -------- FP8 Ë®≠ÂÆö START ---------
        # FP8Ë®≠ÂÆöÔºàÊó¢„Å´LoRAË®≠ÂÆö„Å´Âê´„ÇÅ„Åü„ÅÆ„Åß‰∏çË¶ÅÔºâ
        # „Åì„ÅÆË°å„ÅØÂâäÈô§„Åó„Å¶„ÇÇÂïèÈ°å„ÅÇ„Çä„Åæ„Åõ„Çì
        # -------- FP8 Ë®≠ÂÆö END ---------

        # „Çª„ÇØ„Ç∑„Éß„É≥Âá¶ÁêÜÈñãÂßãÂâç„Å´transformer„ÅÆÁä∂ÊÖã„ÇíÁ¢∫Ë™ç
        print(translate("„Çª„ÇØ„Ç∑„Éß„É≥Âá¶ÁêÜÈñãÂßãÂâç„ÅÆtransformerÁä∂ÊÖã„ÉÅ„Çß„ÉÉ„ÇØ..."))
        try:
            # transformer„ÅÆÁä∂ÊÖã„ÇíÁ¢∫Ë™ç„Åó„ÄÅÂøÖË¶Å„Å´Âøú„Åò„Å¶„É™„É≠„Éº„Éâ
            if not transformer_manager.ensure_transformer_state():
                raise Exception(translate("transformerÁä∂ÊÖã„ÅÆÁ¢∫Ë™ç„Å´Â§±Êïó„Åó„Åæ„Åó„Åü"))

            # ÊúÄÊñ∞„ÅÆtransformer„Ç§„É≥„Çπ„Çø„É≥„Çπ„ÇíÂèñÂæó
            transformer = transformer_manager.get_transformer()
            print(translate("transformerÁä∂ÊÖã„ÉÅ„Çß„ÉÉ„ÇØÂÆå‰∫Ü"))
        except Exception as e:
            print(translate("transformerÁä∂ÊÖã„ÉÅ„Çß„ÉÉ„ÇØ„Ç®„É©„Éº: {0}").format(e))
            traceback.print_exc()
            raise e

        # „Çª„ÇØ„Ç∑„Éß„É≥È†ÜÊ¨°Âá¶ÁêÜ
        for i_section in range(total_sections):
            # ÂÖà„Å´Â§âÊï∞„ÇíÂÆöÁæ©
            is_first_section = i_section == 0

            # ÂçòÁ¥î„Å™„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Å´„Çà„ÇãÂà§ÂÆö
            is_last_section = i_section == total_sections - 1

            # F1„É¢„Éº„Éâ„Åß„ÅØ„Ç™„Éº„É´„Éë„Éá„Ç£„É≥„Ç∞Ê©üËÉΩ„ÅØÁÑ°ÂäπÂåñ„Åï„Çå„Å¶„ÅÑ„Çã„Åü„ÇÅ„ÄÅÂ∏∏„Å´Âõ∫ÂÆöÂÄ§„Çí‰ΩøÁî®
            # „Åì„ÅÆÂÄ§„ÅØF1„É¢„Éº„Éâ„Åß„ÅØÂÆüÈöõ„Å´„ÅØ‰ΩøÁî®„Åï„Çå„Å™„ÅÑ„Åå„ÄÅ„É≠„Ç∞Âá∫Âäõ„ÅÆ„Åü„ÇÅ„Å´Ë®àÁÆó„Åô„Çã
            latent_padding = 1  # Âõ∫ÂÆöÂÄ§

            latent_padding_size = int(latent_padding * latent_window_size)

            # ÂÆöÁæ©Âæå„Å´„É≠„Ç∞Âá∫ÂäõÔºàF1„É¢„Éº„Éâ„Åß„ÅØ„Ç™„Éº„É´„Éë„Éá„Ç£„É≥„Ç∞„ÅØÂ∏∏„Å´ÁÑ°ÂäπÔºâ
            padding_info = translate("„Éë„Éá„Ç£„É≥„Ç∞ÂÄ§: {0} (F1„É¢„Éº„Éâ„Åß„ÅØÂΩ±Èüø„Å™„Åó)").format(latent_padding)
            print(translate("‚ñ† „Çª„ÇØ„Ç∑„Éß„É≥{0}„ÅÆÂá¶ÁêÜÈñãÂßã ({1})").format(i_section, padding_info))
            print(translate("  - ÁèæÂú®„ÅÆÁîüÊàê„Éï„É¨„Éº„É†Êï∞: {0}„Éï„É¨„Éº„É†").format(total_generated_latent_frames * 4 - 3))
            print(translate("  - ÁîüÊàê‰∫àÂÆö„Éï„É¨„Éº„É†Êï∞: {0}„Éï„É¨„Éº„É†").format(num_frames))
            print(translate("  - ÊúÄÂàù„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥?: {0}").format(is_first_section))
            print(translate("  - ÊúÄÂæå„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥?: {0}").format(is_last_section))
            # set current_latent here
            # Â∏∏„Å´ÈñãÂßã„Éï„É¨„Éº„É†„Çí‰ΩøÁî®
            current_latent = start_latent


            if stream.input_queue.top() == 'end':
                stream.output_queue.push(('end', None))
                return

            # ÂÖ±ÈÄö„Éó„É≠„É≥„Éó„Éà„Çí‰ΩøÁî®
            current_llama_vec, current_clip_l_pooler, current_llama_attention_mask = llama_vec, clip_l_pooler, llama_attention_mask

            print(translate('latent_padding_size = {0}, is_last_section = {1}').format(latent_padding_size, is_last_section))


            # COMMENTED OUT: „Çª„ÇØ„Ç∑„Éß„É≥Âá¶ÁêÜÂâç„ÅÆ„É°„É¢„É™Ëß£ÊîæÔºàÂá¶ÁêÜÈÄüÂ∫¶Âêë‰∏ä„ÅÆ„Åü„ÇÅÔºâ
            # if torch.cuda.is_available():
            #     torch.cuda.synchronize()
            #     torch.cuda.empty_cache()

            # latent_window_size„Åå4.5„ÅÆÂ†¥Âêà„ÅØÁâπÂà•„Å´5„Çí‰ΩøÁî®
            effective_window_size = 5 if latent_window_size == 4.5 else int(latent_window_size)
            # ÂøÖ„ÅöÊï¥Êï∞„ÅÆ„É™„Çπ„Éà„Çí‰ΩøÁî®
            indices = torch.arange(0, sum([1, 16, 2, 1, effective_window_size])).unsqueeze(0)
            clean_latent_indices_start, clean_latent_4x_indices, clean_latent_2x_indices, clean_latent_1x_indices, latent_indices = indices.split([1, 16, 2, 1, effective_window_size], dim=1)
            clean_latent_indices = torch.cat([clean_latent_indices_start, clean_latent_1x_indices], dim=1)

            clean_latents_4x, clean_latents_2x, clean_latents_1x = history_latents[:, :, -sum([16, 2, 1]):, :, :].split([16, 2, 1], dim=2)
            clean_latents = torch.cat([start_latent.to(history_latents), clean_latents_1x], dim=2)

            if not high_vram:
                unload_complete_models()
                # GPU„É°„É¢„É™‰øùÂ≠òÂÄ§„ÇíÊòéÁ§∫ÁöÑ„Å´ÊµÆÂãïÂ∞èÊï∞ÁÇπ„Å´Â§âÊèõ
                preserved_memory = float(gpu_memory_preservation) if gpu_memory_preservation is not None else 6.0
                print(translate('Setting transformer memory preservation to: {0} GB').format(preserved_memory))
                move_model_to_device_with_memory_preservation(transformer, target_device=gpu, preserved_memory_gb=preserved_memory)

            if use_teacache:
                transformer.initialize_teacache(enable_teacache=True, num_steps=steps)
            else:
                transformer.initialize_teacache(enable_teacache=False)

            def callback(d):
                preview = d['denoised']
                preview = vae_decode_fake(preview)

                preview = (preview * 255.0).detach().cpu().numpy().clip(0, 255).astype(np.uint8)
                preview = einops.rearrange(preview, 'b c t h w -> (b h) (t w) c')

                if stream.input_queue.top() == 'end':
                    stream.output_queue.push(('end', None))
                    raise KeyboardInterrupt('User ends the task.')

                current_step = d['i'] + 1
                percentage = int(100.0 * current_step / steps)
                hint = translate('Sampling {0}/{1}').format(current_step, steps)
                # „Çª„ÇØ„Ç∑„Éß„É≥ÊÉÖÂ†±„ÇíËøΩÂä†ÔºàÁèæÂú®„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥/ÂÖ®„Çª„ÇØ„Ç∑„Éß„É≥Ôºâ
                section_info = translate('„Çª„ÇØ„Ç∑„Éß„É≥: {0}/{1}').format(i_section+1, total_sections)
                desc = f"{section_info} " + translate('ÁîüÊàê„Éï„É¨„Éº„É†Êï∞: {total_generated_latent_frames}, ÂãïÁîªÈï∑: {video_length:.2f} Áßí (FPS-30). ÂãïÁîª„ÅåÁîüÊàê‰∏≠„Åß„Åô ...').format(section_info=section_info, total_generated_latent_frames=int(max(0, total_generated_latent_frames * 4 - 3)), video_length=max(0, (total_generated_latent_frames * 4 - 3) / 30))
                stream.output_queue.push(('progress', (preview, desc, make_progress_bar_html(percentage, hint))))
                return

            # ImageÂΩ±ÈüøÂ∫¶„ÇíË®àÁÆóÔºöÂ§ß„Åç„ÅÑÂÄ§„Åª„Å©ÂßãÁÇπ„ÅÆÂΩ±Èüø„ÅåÂº∑„Åè„Å™„Çã„Çà„ÅÜÂ§âÊèõ
            # 1.0/image_strength„Çí‰ΩøÁî®„Åó„ÄÅÊúÄÂ∞èÂÄ§„Çí0.01„Å´Âà∂Èôê
            strength_value = max(0.01, 1.0 / image_strength)
            print(translate('ImageÂΩ±ÈüøÂ∫¶: UIÂÄ§={0:.2f}Ôºà{1:.0f}%Ôºâ‚ÜíË®àÁÆóÂÄ§={2:.4f}ÔºàÂÄ§„ÅåÂ∞è„Åï„ÅÑ„Åª„Å©ÂßãÁÇπ„ÅÆÂΩ±Èüø„ÅåÂº∑„ÅÑÔºâ').format(
                image_strength, image_strength * 100, strength_value))

            generated_latents = sample_hunyuan(
                transformer=transformer,
                sampler='unipc',
                width=width,
                height=height,
                frames=num_frames,
                real_guidance_scale=cfg,
                distilled_guidance_scale=gs,
                guidance_rescale=rs,
                # shift=3.0,
                num_inference_steps=steps,
                generator=rnd,
                prompt_embeds=current_llama_vec,  # „Çª„ÇØ„Ç∑„Éß„É≥„Åî„Å®„ÅÆ„Éó„É≠„É≥„Éó„Éà„Çí‰ΩøÁî®
                prompt_embeds_mask=current_llama_attention_mask,  # „Çª„ÇØ„Ç∑„Éß„É≥„Åî„Å®„ÅÆ„Éû„Çπ„ÇØ„Çí‰ΩøÁî®
                prompt_poolers=current_clip_l_pooler,  # „Çª„ÇØ„Ç∑„Éß„É≥„Åî„Å®„ÅÆ„Éó„É≠„É≥„Éó„Éà„Çí‰ΩøÁî®
                negative_prompt_embeds=llama_vec_n,
                negative_prompt_embeds_mask=llama_attention_mask_n,
                negative_prompt_poolers=clip_l_pooler_n,
                device=gpu,
                dtype=torch.bfloat16,
                image_embeddings=image_encoder_last_hidden_state,
                latent_indices=latent_indices,
                clean_latents=clean_latents,
                clean_latent_indices=clean_latent_indices,
                clean_latents_2x=clean_latents_2x,
                clean_latent_2x_indices=clean_latent_2x_indices,
                clean_latents_4x=clean_latents_4x,
                clean_latent_4x_indices=clean_latent_4x_indices,
                initial_latent=current_latent,  # ÈñãÂßãÊΩúÂú®Á©∫Èñì„ÇíË®≠ÂÆö
                strength=strength_value,        # Ë®àÁÆó„Åó„ÅüÂΩ±ÈüøÂ∫¶„Çí‰ΩøÁî®
                callback=callback,
            )

            # if is_last_section:
            #     generated_latents = torch.cat([start_latent.to(generated_latents), generated_latents], dim=2)

            total_generated_latent_frames += int(generated_latents.shape[2])
            # ÂæåÊñπ„Å´„Éï„É¨„Éº„É†„ÇíËøΩÂä†
            history_latents = torch.cat([history_latents, generated_latents.to(history_latents)], dim=2)

            if not high_vram:
                # Ê∏õÂúßÊôÇ„Å´‰ΩøÁî®„Åô„ÇãGPU„É°„É¢„É™ÂÄ§„ÇÇÊòéÁ§∫ÁöÑ„Å´ÊµÆÂãïÂ∞èÊï∞ÁÇπ„Å´Ë®≠ÂÆö
                preserved_memory_offload = 8.0  # „Åì„Å°„Çâ„ÅØÂõ∫ÂÆöÂÄ§„ÅÆ„Åæ„Åæ
                print(translate('Offloading transformer with memory preservation: {0} GB').format(preserved_memory_offload))
                offload_model_from_device_for_memory_preservation(transformer, target_device=gpu, preserved_memory_gb=preserved_memory_offload)
                load_model_as_complete(vae, target_device=gpu)

            # ÊúÄÊñ∞„Éï„É¨„Éº„É†„ÅØÊú´Â∞æ„Åã„ÇâÂàá„ÇäÂá∫„Åó
            real_history_latents = history_latents[:, :, -total_generated_latent_frames:, :, :]

            # COMMENTED OUT: VAE„Éá„Ç≥„Éº„ÉâÂâç„ÅÆ„É°„É¢„É™„ÇØ„É™„Ç¢ÔºàÂá¶ÁêÜÈÄüÂ∫¶Âêë‰∏ä„ÅÆ„Åü„ÇÅÔºâ
            # if torch.cuda.is_available():
            #     torch.cuda.synchronize()
            #     torch.cuda.empty_cache()
            #     print(translate("VAE„Éá„Ç≥„Éº„ÉâÂâç„É°„É¢„É™: {0:.2f}GB").format(torch.cuda.memory_allocated()/1024**3))

            if history_pixels is None:
                history_pixels = vae_decode(real_history_latents, vae).cpu()
            else:
                # latent_window_size„Åå4.5„ÅÆÂ†¥Âêà„ÅØÁâπÂà•„Å´5„Çí‰ΩøÁî®
                if latent_window_size == 4.5:
                    section_latent_frames = 11 if is_last_section else 10  # 5 * 2 + 1 = 11, 5 * 2 = 10
                    overlapped_frames = 17  # 5 * 4 - 3 = 17
                else:
                    # +1„ÅØÈÄÜÊñπÂêëÁîüÊàêÊôÇ„ÅÆ start_latent ÂàÜ„Å™„ÅÆ„Åß„Ç´„ÉÉ„Éà
                    section_latent_frames = int(latent_window_size * 2) if is_last_section else int(latent_window_size * 2)
                    overlapped_frames = int(latent_window_size * 4 - 3)

                # F1„É¢„Éº„Éâ„Åß„ÅØÊúÄÊñ∞„Éï„É¨„Éº„É†„ÅØÊú´Â∞æ„Å´„ÅÇ„Çã„Åü„ÇÅ„ÄÅÂæåÊñπ„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÂèñÂæó
                current_pixels = vae_decode(real_history_latents[:, :, -section_latent_frames:], vae).cpu()

                # ÂºïÊï∞„ÅÆÈ†ÜÂ∫è„Çí‰øÆÊ≠£ - history_pixels„ÅåÂÖà„ÄÅÊñ∞„Åó„ÅÑcurrent_pixels„ÅåÂæå
                if history_pixels is None:
                    history_pixels = current_pixels
                else:
                    history_pixels = soft_append_bcthw(history_pixels, current_pixels, overlapped_frames)

            # ÂêÑ„Çª„ÇØ„Ç∑„Éß„É≥„ÅÆÊúÄÁµÇ„Éï„É¨„Éº„É†„ÇíÈùôÊ≠¢Áîª„Å®„Åó„Å¶‰øùÂ≠òÔºà„Çª„ÇØ„Ç∑„Éß„É≥Áï™Âè∑‰ªò„ÅçÔºâ
            if save_section_frames and history_pixels is not None:
                try:
                    if i_section == 0 or current_pixels is None:
                        # ÊúÄÂàù„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥„ÅØ history_pixels „ÅÆÊúÄÂæå
                        last_frame = history_pixels[0, :, -1, :, :]
                    else:
                        # 2„Çª„ÇØ„Ç∑„Éß„É≥ÁõÆ‰ª•Èôç„ÅØ current_pixels „ÅÆÊúÄÂæå
                        last_frame = current_pixels[0, :, -1, :, :]
                    last_frame = einops.rearrange(last_frame, 'c h w -> h w c')
                    last_frame = last_frame.cpu().numpy()
                    last_frame = np.clip((last_frame * 127.5 + 127.5), 0, 255).astype(np.uint8)
                    last_frame = resize_and_center_crop(last_frame, target_width=width, target_height=height)

                    # „É°„Çø„Éá„Éº„Çø„ÇíÂüã„ÇÅËæº„ÇÄ„Åü„ÇÅ„ÅÆÊÉÖÂ†±„ÇíÂèéÈõÜ
                    section_metadata = {
                        PROMPT_KEY: prompt,  # „É°„Ç§„É≥„Éó„É≠„É≥„Éó„Éà
                        SEED_KEY: seed,
                        SECTION_NUMBER_KEY: i_section
                    }

                    # „Çª„ÇØ„Ç∑„Éß„É≥Âõ∫Êúâ„ÅÆ„Éó„É≠„É≥„Éó„Éà„Åå„ÅÇ„Çå„Å∞ÂèñÂæó
                    if section_map and i_section in section_map:
                        _, section_prompt = section_map[i_section]
                        if section_prompt and section_prompt.strip():
                            section_metadata[SECTION_PROMPT_KEY] = section_prompt

                    # ÁîªÂÉè„ÅÆ‰øùÂ≠ò„Å®„É°„Çø„Éá„Éº„Çø„ÅÆÂüã„ÇÅËæº„Åø
                    if is_first_section:
                        frame_path = os.path.join(outputs_folder, f'{job_id}_{i_section}_end.png')
                        Image.fromarray(last_frame).save(frame_path)
                        embed_metadata_to_png(frame_path, section_metadata)
                    else:
                        frame_path = os.path.join(outputs_folder, f'{job_id}_{i_section}.png')
                        Image.fromarray(last_frame).save(frame_path)
                        embed_metadata_to_png(frame_path, section_metadata)

                    print(translate("„Çª„ÇØ„Ç∑„Éß„É≥{0}„ÅÆ„Éï„É¨„Éº„É†ÁîªÂÉè„Çí„É°„Çø„Éá„Éº„Çø‰ªò„Åç„Åß‰øùÂ≠ò„Åó„Åæ„Åó„Åü").format(i_section))
                except Exception as e:
                    print(translate("„Çª„ÇØ„Ç∑„Éß„É≥{0}ÊúÄÁµÇ„Éï„É¨„Éº„É†ÁîªÂÉè‰øùÂ≠òÊôÇ„Å´„Ç®„É©„Éº: {1}").format(i_section, e))

            # ÂÖ®„Éï„É¨„Éº„É†ÁîªÂÉè‰øùÂ≠òÊ©üËÉΩ
            # „ÄåÂÖ®„Éï„É¨„Éº„É†ÁîªÂÉè‰øùÂ≠ò„Äç„Åæ„Åü„ÅØ„ÄåÊúÄÁµÇ„Çª„ÇØ„Ç∑„Éß„É≥„ÅÆ„ÅøÂÖ®„Éï„É¨„Éº„É†ÁîªÂÉè‰øùÂ≠ò„Åã„Å§ÊúÄÁµÇ„Çª„ÇØ„Ç∑„Éß„É≥„Äç„ÅåÊúâÂäπ„Å™Â†¥Âêà
            # ÊúÄÁµÇ„Çª„ÇØ„Ç∑„Éß„É≥„Åã„Å©„ÅÜ„Åã„ÅÆÂà§ÂÆö„Çítotal_sections„Åã„ÇâÊ≠£Á¢∫„Å´ÂèñÂæó
            is_last_section = i_section == total_sections - 1
            
            # save_latent_frames „Å® save_last_section_frames „ÅÆÂÄ§„Çícopy
            # „É´„Éº„ÉóÂÜÖ„ÅÆÂ§âÊï∞„ÇíÂ§âÊõ¥„Åó„Å¶„ÇÇ„Ç∞„É≠„Éº„Éê„É´„Å™ÂÄ§„ÅØÂ§â„Çè„Çâ„Å™„ÅÑ„Åü„ÇÅ
            # Ê≥®ÊÑèÔºöÊó¢„Å´„Åì„Åì„Å´Êù•„ÇãÂâç„Å´‰∏á„Åå‰∏Ä„ÅÆÊñáÂ≠óÂàó‚Üí„Éñ„Éº„É´Â§âÊèõÂá¶ÁêÜ„ÅåÊ∏à„Çì„Åß„ÅÑ„Çã„ÅØ„Åö
            
            # ÂÄ§„ÅÆ„Ç≥„Éî„Éº„Åß„ÅØ„Å™„Åè„ÄÅÊòéÁ§∫ÁöÑ„Å´Êñ∞„Åó„ÅÑÂ§âÊï∞„Å´ÈÅ©Âàá„Å™ÂÄ§„ÇíË®≠ÂÆö
            # Boolean„ÅãString„Åã„ÅÆÂûãÂ§âÊèõ„Éü„Çπ„ÇíÈò≤„Åê
            is_save_all_frames = bool(save_latent_frames)
            is_save_last_frame_only = bool(save_last_section_frames)
            
            if is_save_all_frames:
                should_save_frames = True
            elif is_save_last_frame_only and is_last_section:
                should_save_frames = True
            else:
                should_save_frames = False
            
            if should_save_frames:
                try:
                    # source_pixels„ÅØ„ÄÅ„Åì„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥„Åß‰ΩøÁî®„Åô„Çã„Éî„ÇØ„Çª„É´„Éá„Éº„Çø
                    source_pixels = None
                    
                    # i_section=0„ÅÆÂ†¥Âêà„ÄÅcurrent_pixels„ÅåÂÆöÁæ©„Åï„Çå„ÇãÂâç„Å´ÂèÇÁÖß„Åï„Çå„Çã„Åü„ÇÅ„Ç®„É©„Éº„Å®„Å™„Çã
                    # history_pixels„ÇíÂÑ™ÂÖà„Åó„Å¶‰ΩøÁî®„Åô„Çã„Çà„ÅÜÂá¶ÁêÜÈ†ÜÂ∫è„ÇíÂ§âÊõ¥
                    if history_pixels is not None:
                        source_pixels = history_pixels
                        print(translate("„Éï„É¨„Éº„É†ÁîªÂÉè‰øùÂ≠ò: history_pixels„Çí‰ΩøÁî®„Åó„Åæ„Åô"))
                    elif 'current_pixels' in locals() and current_pixels is not None:
                        source_pixels = current_pixels
                        print(translate("„Éï„É¨„Éº„É†ÁîªÂÉè‰øùÂ≠ò: current_pixels„Çí‰ΩøÁî®„Åó„Åæ„Åô"))
                    else:
                        print(translate("„Éï„É¨„Éº„É†ÁîªÂÉè‰øùÂ≠ò: ÊúâÂäπ„Å™„Éî„ÇØ„Çª„É´„Éá„Éº„Çø„Åå„ÅÇ„Çä„Åæ„Åõ„Çì"))
                        return
                        
                    # „Éï„É¨„Éº„É†Êï∞Ôºà1Áßí„É¢„Éº„Éâ„Åß„ÅØ9„Éï„É¨„Éº„É†„ÄÅ0.5Áßí„É¢„Éº„Éâ„Åß„ÅØ5„Éï„É¨„Éº„É†Ôºâ
                    latent_frame_count = source_pixels.shape[2]
                    
                    # ‰øùÂ≠ò„É¢„Éº„Éâ„Å´Âøú„Åò„Åü„É°„ÉÉ„Çª„Éº„Ç∏„ÇíË°®Á§∫
                    # „Ç∞„É≠„Éº„Éê„É´Â§âÊï∞„Åß„ÅØ„Å™„Åè„ÄÅ„É≠„Éº„Ç´„É´„ÅÆcopy„Çí‰ΩøÁî®
                    if is_save_all_frames:
                        print(translate("ÂÖ®„Éï„É¨„Éº„É†ÁîªÂÉè‰øùÂ≠ò: „Çª„ÇØ„Ç∑„Éß„É≥{0}„ÅÆ{1}„Éï„É¨„Éº„É†„Çí‰øùÂ≠ò„Åó„Åæ„Åô").format(i_section, latent_frame_count))
                    elif is_save_last_frame_only and is_last_section:
                        # Âº∑Ë™ø„Åó„Å¶ÊúÄÁµÇ„Çª„ÇØ„Ç∑„Éß„É≥„Åß„ÅÇ„Çã„Åì„Å®„ÇíË°®Á§∫
                        print(translate("ÊúÄÁµÇ„Çª„ÇØ„Ç∑„Éß„É≥„ÅÆ„ÅøÂÖ®„Éï„É¨„Éº„É†ÁîªÂÉè‰øùÂ≠ò: „Çª„ÇØ„Ç∑„Éß„É≥{0}/{1}„ÅÆ{2}„Éï„É¨„Éº„É†„Çí‰øùÂ≠ò„Åó„Åæ„Åô (ÊúÄÁµÇ„Çª„ÇØ„Ç∑„Éß„É≥)").format(
                            i_section, total_sections-1, latent_frame_count))
                    else:
                        print(translate("„Éï„É¨„Éº„É†ÁîªÂÉè‰øùÂ≠ò: „Çª„ÇØ„Ç∑„Éß„É≥{0}„ÅÆ{1}„Éï„É¨„Éº„É†„Çí‰øùÂ≠ò„Åó„Åæ„Åô").format(i_section, latent_frame_count))
                    
                    # „Çª„ÇØ„Ç∑„Éß„É≥„Åî„Å®„ÅÆ„Éï„Ç©„É´„ÉÄ„Çí‰ΩúÊàê
                    frames_folder = os.path.join(outputs_folder, f'{job_id}_frames_section{i_section}')
                    os.makedirs(frames_folder, exist_ok=True)
                    
                    # ÂêÑ„Éï„É¨„Éº„É†„ÅÆ‰øùÂ≠ò
                    for frame_idx in range(latent_frame_count):
                        # „Éï„É¨„Éº„É†„ÇíÂèñÂæó
                        frame = source_pixels[0, :, frame_idx, :, :]
                        frame = einops.rearrange(frame, 'c h w -> h w c')
                        frame = frame.cpu().numpy()
                        frame = np.clip((frame * 127.5 + 127.5), 0, 255).astype(np.uint8)
                        frame = resize_and_center_crop(frame, target_width=width, target_height=height)
                        
                        # „É°„Çø„Éá„Éº„Çø„ÅÆÊ∫ñÂÇô
                        frame_metadata = {
                            PROMPT_KEY: prompt,  # „É°„Ç§„É≥„Éó„É≠„É≥„Éó„Éà
                            SEED_KEY: seed,
                            SECTION_NUMBER_KEY: i_section,
                            "FRAME_NUMBER": frame_idx  # „Éï„É¨„Éº„É†Áï™Âè∑„ÇÇËøΩÂä†
                        }
                        
                        # ÁîªÂÉè„ÅÆ‰øùÂ≠ò„Å®„É°„Çø„Éá„Éº„Çø„ÅÆÂüã„ÇÅËæº„Åø
                        frame_path = os.path.join(frames_folder, f'frame_{frame_idx:03d}.png')
                        Image.fromarray(frame).save(frame_path)
                        embed_metadata_to_png(frame_path, frame_metadata)
                    
                    # ‰øùÂ≠ò„É¢„Éº„Éâ„Å´Âøú„Åò„Åü„É°„ÉÉ„Çª„Éº„Ç∏„ÇíË°®Á§∫
                    # „Ç∞„É≠„Éº„Éê„É´Â§âÊï∞„Åß„ÅØ„Å™„Åè„ÄÅ„É≠„Éº„Ç´„É´„ÅÆcopy„Çí‰ΩøÁî®
                    if is_save_all_frames:
                        print(translate("ÂÖ®„Éï„É¨„Éº„É†ÁîªÂÉè‰øùÂ≠ò: „Çª„ÇØ„Ç∑„Éß„É≥{0}„ÅÆ{1}ÂÄã„ÅÆ„Éï„É¨„Éº„É†ÁîªÂÉè„Çí‰øùÂ≠ò„Åó„Åæ„Åó„Åü: {2}").format(
                            i_section, latent_frame_count, frames_folder))
                    elif is_save_last_frame_only and is_last_section:
                        print(translate("ÊúÄÁµÇ„Çª„ÇØ„Ç∑„Éß„É≥„ÅÆ„ÅøÂÖ®„Éï„É¨„Éº„É†ÁîªÂÉè‰øùÂ≠ò: „Çª„ÇØ„Ç∑„Éß„É≥{0}/{1}„ÅÆ{2}ÂÄã„ÅÆ„Éï„É¨„Éº„É†ÁîªÂÉè„Çí‰øùÂ≠ò„Åó„Åæ„Åó„Åü (ÊúÄÁµÇ„Çª„ÇØ„Ç∑„Éß„É≥): {3}").format(
                            i_section, total_sections-1, latent_frame_count, frames_folder))
                    else:
                        print(translate("„Çª„ÇØ„Ç∑„Éß„É≥{0}„ÅÆ{1}ÂÄã„ÅÆ„Éï„É¨„Éº„É†ÁîªÂÉè„Çí‰øùÂ≠ò„Åó„Åæ„Åó„Åü: {2}").format(
                            i_section, latent_frame_count, frames_folder))
                except Exception as e:
                    print(translate("„Çª„ÇØ„Ç∑„Éß„É≥{0}„ÅÆ„Éï„É¨„Éº„É†ÁîªÂÉè‰øùÂ≠ò‰∏≠„Å´„Ç®„É©„Éº: {1}").format(i_section, e))
                    traceback.print_exc()

            if not high_vram:
                unload_complete_models()

            # MP4„Éï„Ç°„Ç§„É´Âêç„ÅØendframe_ichi„ÅÆÂëΩÂêçË¶èÂâá„Å´Âêà„Çè„Åõ„Çã
            # „Éê„ÉÉ„ÉÅÁï™Âè∑„ÅØ„Éï„Ç°„Ç§„É´Âêç„Å´ÊòéÁ§∫ÁöÑ„Å´Âê´„ÇÅ„Å™„ÅÑ
            output_filename = os.path.join(outputs_folder, f'{job_id}_{total_generated_latent_frames}.mp4')

            # „ÇÇ„Åóhistory_pixels„ÅÆÂÄ§„Åå‰∏çÈÅ©Âàá„Å™ÁØÑÂõ≤„Å´„ÅÇ„ÇãÂ†¥Âêà„ÄÅÁØÑÂõ≤„Çí‰øÆÊ≠£
            if history_pixels.min() < -1.0 or history_pixels.max() > 1.0:
                history_pixels = torch.clamp(history_pixels, -1.0, 1.0)

            # MP4„Çí‰øùÂ≠ò
            save_bcthw_as_mp4(history_pixels, output_filename, fps=30, crf=mp4_crf)

            print(translate('Decoded. Current latent shape {0}; pixel shape {1}').format(real_history_latents.shape, history_pixels.shape))

            print(translate("‚ñ† „Çª„ÇØ„Ç∑„Éß„É≥{0}„ÅÆÂá¶ÁêÜÂÆå‰∫Ü").format(i_section))
            print(translate("  - ÁèæÂú®„ÅÆÁ¥ØË®à„Éï„É¨„Éº„É†Êï∞: {0}„Éï„É¨„Éº„É†").format(int(max(0, total_generated_latent_frames * 4 - 3))))
            print(translate("  - „É¨„É≥„ÉÄ„É™„É≥„Ç∞ÊôÇÈñì: {0}Áßí").format(f"{max(0, (total_generated_latent_frames * 4 - 3) / 30):.2f}"))
            print(translate("  - Âá∫Âäõ„Éï„Ç°„Ç§„É´: {0}").format(output_filename))

            stream.output_queue.push(('file', output_filename))

            if is_last_section:
                combined_output_filename = None
                # ÂÖ®„Çª„ÇØ„Ç∑„Éß„É≥Âá¶ÁêÜÂÆå‰∫ÜÂæå„ÄÅ„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„ÇíÂæåÊñπ„Å´ÁµêÂêà
                if uploaded_tensor is not None:
                    try:
                        original_frames = real_history_latents.shape[2]  # ÂÖÉ„ÅÆ„Éï„É¨„Éº„É†Êï∞„ÇíË®òÈå≤
                        uploaded_frames = uploaded_tensor.shape[2]  # „Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åï„Çå„Åü„Éï„É¨„Éº„É†Êï∞

                        print(translate("„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„ÇíÂæåÊñπ„Å´ÁµêÂêà„Åó„Åæ„Åô: „Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åï„Çå„Åü„Éï„É¨„Éº„É†Êï∞ = {uploaded_frames}").format(uploaded_frames=uploaded_frames))
                        # UI‰∏ä„ÅßÈÄ≤ÊçóÁä∂Ê≥Å„ÇíÊõ¥Êñ∞
                        stream.output_queue.push(('progress', (None, translate("„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø({uploaded_frames}„Éï„É¨„Éº„É†)„ÅÆÁµêÂêà„ÇíÈñãÂßã„Åó„Åæ„Åô...").format(uploaded_frames=uploaded_frames), make_progress_bar_html(80, translate('„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„ÇøÁµêÂêàÊ∫ñÂÇô')))))

                        # „ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„ÇíÂæåÊñπ„Å´ÁµêÂêà„Åô„ÇãÂâç„Å´„ÄÅ‰∫íÊèõÊÄß„ÉÅ„Çß„ÉÉ„ÇØ

                        if uploaded_tensor.shape[3] != real_history_latents.shape[3] or uploaded_tensor.shape[4] != real_history_latents.shape[4]:
                            print(translate("Ë≠¶Âëä: „ÉÜ„É≥„ÇΩ„É´„Çµ„Ç§„Ç∫„ÅåÁï∞„Å™„Çä„Åæ„Åô: „Ç¢„ÉÉ„Éó„É≠„Éº„Éâ={0}, ÁèæÂú®„ÅÆÁîüÊàê={1}").format(uploaded_tensor.shape, real_history_latents.shape))
                            print(translate("„ÉÜ„É≥„ÇΩ„É´„Çµ„Ç§„Ç∫„ÅÆ‰∏ç‰∏ÄËá¥„ÅÆ„Åü„ÇÅ„ÄÅÂâçÊñπÁµêÂêà„Çí„Çπ„Ç≠„ÉÉ„Éó„Åó„Åæ„Åô"))
                            stream.output_queue.push(('progress', (None, translate("„ÉÜ„É≥„ÇΩ„É´„Çµ„Ç§„Ç∫„ÅÆ‰∏ç‰∏ÄËá¥„ÅÆ„Åü„ÇÅ„ÄÅÂâçÊñπÁµêÂêà„Çí„Çπ„Ç≠„ÉÉ„Éó„Åó„Åæ„Åó„Åü"), make_progress_bar_html(85, translate('‰∫íÊèõÊÄß„Ç®„É©„Éº')))))
                        else:
                            # „Éá„Éê„Ç§„Çπ„Å®„Éá„Éº„ÇøÂûã„ÇíÂêà„Çè„Åõ„Çã
                            processed_tensor = uploaded_tensor.clone()
                            if processed_tensor.device != real_history_latents.device:
                                processed_tensor = processed_tensor.to(real_history_latents.device)
                            if processed_tensor.dtype != real_history_latents.dtype:
                                processed_tensor = processed_tensor.to(dtype=real_history_latents.dtype)

                            # ÂÖÉ„ÅÆÂãïÁîª„ÇíÂìÅË≥™„Çí‰øù„Å°„Å§„Å§‰øùÂ≠ò
                            original_output_filename = os.path.join(outputs_folder, f'{job_id}_original.mp4')
                            save_bcthw_as_mp4(history_pixels, original_output_filename, fps=30, crf=mp4_crf)
                            print(translate("ÂÖÉ„ÅÆÂãïÁîª„Çí‰øùÂ≠ò„Åó„Åæ„Åó„Åü: {original_output_filename}").format(original_output_filename=original_output_filename))

                            # ÂÖÉ„Éá„Éº„Çø„ÅÆ„Ç≥„Éî„Éº„ÇíÂèñÂæó
                            combined_history_latents = real_history_latents.clone()
                            combined_history_pixels = history_pixels.clone() if history_pixels is not None else None

                            # ÂêÑ„ÉÅ„É£„É≥„ÇØ„ÅÆÂá¶ÁêÜÂâç„Å´ÊòéÁ§∫ÁöÑ„Å´„É°„É¢„É™Ëß£Êîæ
                            if torch.cuda.is_available():
                                torch.cuda.synchronize()
                                torch.cuda.empty_cache()
                                import gc
                                gc.collect()
                                print(translate("GPU„É°„É¢„É™Á¢∫‰øùÁä∂ÊÖã: {memory:.2f}GB").format(memory=torch.cuda.memory_allocated()/1024**3))

                            # VAE„ÇíGPU„Å´ÁßªÂãï
                            if not high_vram and vae.device != torch.device('cuda'):
                                print(translate("VAE„ÇíGPU„Å´ÁßªÂãï: {0} ‚Üí cuda").format(vae.device))
                                vae.to('cuda')

                            # ÂêÑ„ÉÅ„É£„É≥„ÇØ„ÇíÂá¶ÁêÜ
                            # „ÉÅ„É£„É≥„ÇØ„Çµ„Ç§„Ç∫„ÇíË®≠ÂÆö(ÂêÑ„Çª„ÇØ„Ç∑„Éß„É≥„Å®ÂêåÁ≠â„ÅÆ„Çµ„Ç§„Ç∫„Å´„Åô„Çã)
                            chunk_size = min(5, uploaded_frames)  # ÊúÄÂ§ß„ÉÅ„É£„É≥„ÇØ„Çµ„Ç§„Ç∫„Çí5„Éï„É¨„Éº„É†„Å´Ë®≠ÂÆöÔºà„É°„É¢„É™‰ΩøÁî®Èáè„ÇíÊ∏õ„Çâ„Åô„Åü„ÇÅÔºâ

                            # „ÉÅ„É£„É≥„ÇØÊï∞„ÇíË®àÁÆó
                            num_chunks = (uploaded_frames + chunk_size - 1) // chunk_size

                            # ÂêÑ„ÉÅ„É£„É≥„ÇØ„ÇíÂá¶ÁêÜ
                            for chunk_idx in range(num_chunks):
                                chunk_start = chunk_idx * chunk_size
                                chunk_end = min(chunk_start + chunk_size, uploaded_frames)
                                chunk_frames = chunk_end - chunk_start

                                # ÈÄ≤ÊçóÁä∂Ê≥Å„ÇíÊõ¥Êñ∞
                                chunk_progress = (chunk_idx + 1) / num_chunks * 100
                                progress_message = translate("„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„ÇøÁµêÂêà‰∏≠: „ÉÅ„É£„É≥„ÇØ {0}/{1} („Éï„É¨„Éº„É† {2}-{3}/{4})").format(chunk_idx+1, num_chunks, chunk_start+1, chunk_end, uploaded_frames)
                                stream.output_queue.push(('progress', (None, progress_message, make_progress_bar_html(int(80 + chunk_progress * 0.1), translate('„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„ÇøÂá¶ÁêÜ‰∏≠')))))

                                # ÁèæÂú®„ÅÆ„ÉÅ„É£„É≥„ÇØ„ÇíÂèñÂæó
                                current_chunk = processed_tensor[:, :, chunk_start:chunk_end, :, :]
                                print(translate("„ÉÅ„É£„É≥„ÇØ{0}/{1}Âá¶ÁêÜ‰∏≠: „Éï„É¨„Éº„É† {2}-{3}/{4}").format(chunk_idx+1, num_chunks, chunk_start+1, chunk_end, uploaded_frames))

                                # „É°„É¢„É™Áä∂ÊÖã„ÇíÂá∫Âäõ
                                if torch.cuda.is_available():
                                    print(translate("„ÉÅ„É£„É≥„ÇØ{0}Âá¶ÁêÜÂâç„ÅÆGPU„É°„É¢„É™: {1:.2f}GB/{2:.2f}GB").format(chunk_idx+1, torch.cuda.memory_allocated()/1024**3, torch.cuda.get_device_properties(0).total_memory/1024**3))
                                    # „É°„É¢„É™„Ç≠„É£„ÉÉ„Ç∑„É•„Çí„ÇØ„É™„Ç¢
                                    torch.cuda.empty_cache()

                                try:
                                    # ÂêÑ„ÉÅ„É£„É≥„ÇØÂá¶ÁêÜÂâç„Å´GPU„É°„É¢„É™„ÇíËß£Êîæ
                                    if torch.cuda.is_available():
                                        torch.cuda.synchronize()
                                        torch.cuda.empty_cache()
                                        import gc
                                        gc.collect()
                                    # „ÉÅ„É£„É≥„ÇØ„Çí„Éá„Ç≥„Éº„Éâ
                                    # VAE„Éá„Ç≥„Éº„Éâ„ÅØÊôÇÈñì„Åå„Åã„Åã„Çã„Åü„ÇÅ„ÄÅÈÄ≤Ë°å‰∏≠„Åß„ÅÇ„Çã„Åì„Å®„ÇíË°®Á§∫
                                    print(translate("„ÉÅ„É£„É≥„ÇØ{0}„ÅÆVAE„Éá„Ç≥„Éº„ÉâÈñãÂßã...").format(chunk_idx+1))
                                    stream.output_queue.push(('progress', (None, translate("„ÉÅ„É£„É≥„ÇØ{0}/{1}„ÅÆVAE„Éá„Ç≥„Éº„Éâ‰∏≠...").format(chunk_idx+1, num_chunks), make_progress_bar_html(int(80 + chunk_progress * 0.1), translate('„Éá„Ç≥„Éº„ÉâÂá¶ÁêÜ')))))

                                    # ÊòéÁ§∫ÁöÑ„Å´„Éá„Éê„Ç§„Çπ„ÇíÂêà„Çè„Åõ„Çã
                                    if current_chunk.device != vae.device:
                                        print(translate("  - „Éá„Éê„Ç§„Çπ„ÇíVAE„Å®Âêå„Åò„Å´Â§âÊõ¥: {0} ‚Üí {1}").format(current_chunk.device, vae.device))
                                        current_chunk = current_chunk.to(vae.device)

                                    # Âûã„ÇíÊòéÁ§∫ÁöÑ„Å´Âêà„Çè„Åõ„Çã
                                    if current_chunk.dtype != torch.float16:
                                        print(translate("  - „Éá„Éº„ÇøÂûã„Çífloat16„Å´Â§âÊõ¥: {0} ‚Üí torch.float16").format(current_chunk.dtype))
                                        current_chunk = current_chunk.to(dtype=torch.float16)

                                    # VAE„Éá„Ç≥„Éº„ÉâÂá¶ÁêÜ
                                    chunk_pixels = vae_decode(current_chunk, vae).cpu()
                                    print(translate("„ÉÅ„É£„É≥„ÇØ{0}„ÅÆVAE„Éá„Ç≥„Éº„ÉâÂÆå‰∫Ü („Éï„É¨„Éº„É†Êï∞: {1})").format(chunk_idx+1, chunk_frames))

                                    # „É°„É¢„É™‰ΩøÁî®Èáè„ÇíÂá∫Âäõ
                                    if torch.cuda.is_available():
                                        print(translate("„ÉÅ„É£„É≥„ÇØ{0}„Éá„Ç≥„Éº„ÉâÂæå„ÅÆGPU„É°„É¢„É™: {1:.2f}GB").format(chunk_idx+1, torch.cuda.memory_allocated()/1024**3))

                                    # ÁµêÂêà„Åô„Çã
                                    if combined_history_pixels is None:
                                        # ÂàùÂõû„ÅÆ„ÉÅ„É£„É≥„ÇØ„ÅÆÂ†¥Âêà„ÅØ„Åù„ÅÆ„Åæ„ÅæË®≠ÂÆö
                                        combined_history_pixels = chunk_pixels
                                    else:
                                        # Êó¢Â≠ò„Éá„Éº„Çø„Å®Êñ∞Ë¶è„Éá„Éº„Çø„ÅßÂûã„Å®„Éá„Éê„Ç§„Çπ„ÇíÊèÉ„Åà„Çã
                                        if combined_history_pixels.dtype != chunk_pixels.dtype:
                                            print(translate("  - „Éá„Éº„ÇøÂûã„ÅÆ‰∏ç‰∏ÄËá¥„Çí‰øÆÊ≠£: {0} ‚Üí {1}").format(combined_history_pixels.dtype, chunk_pixels.dtype))
                                            combined_history_pixels = combined_history_pixels.to(dtype=chunk_pixels.dtype)

                                        # ‰∏°Êñπ„Å®„ÇÇÂøÖ„ÅöCPU„Å´ÁßªÂãï„Åó„Å¶„Åã„ÇâÁµêÂêà
                                        if combined_history_pixels.device != torch.device('cpu'):
                                            combined_history_pixels = combined_history_pixels.cpu()
                                        if chunk_pixels.device != torch.device('cpu'):
                                            chunk_pixels = chunk_pixels.cpu()

                                        # ÁµêÂêàÂá¶ÁêÜ
                                        combined_history_pixels = torch.cat([combined_history_pixels, chunk_pixels], dim=2)

                                    # ÁµêÂêàÂæå„ÅÆ„Éï„É¨„Éº„É†Êï∞„ÇíÁ¢∫Ë™ç
                                    current_total_frames = combined_history_pixels.shape[2]
                                    print(translate("„ÉÅ„É£„É≥„ÇØ{0}„ÅÆÁµêÂêàÂÆå‰∫Ü: ÁèæÂú®„ÅÆÁµÑ„ÅøËæº„Åø„Éï„É¨„Éº„É†Êï∞ = {1}").format(chunk_idx+1, current_total_frames))

                                    # ‰∏≠ÈñìÁµêÊûú„ÅÆ‰øùÂ≠òÔºà„ÉÅ„É£„É≥„ÇØ„Åî„Å®„Å´‰øùÂ≠ò„Åô„Çã„Å®ÂäπÁéá„ÅåÊÇ™„ÅÑ„ÅÆ„Åß„ÄÅÊúÄÁµÇ„ÉÅ„É£„É≥„ÇØ„ÅÆ„Åø‰øùÂ≠òÔºâ
                                    if chunk_idx == num_chunks - 1 or (chunk_idx > 0 and (chunk_idx + 1) % 5 == 0):
                                        # 5„ÉÅ„É£„É≥„ÇØ„Åî„Å®„ÄÅ„Åæ„Åü„ÅØÊúÄÂæå„ÅÆ„ÉÅ„É£„É≥„ÇØ„Åß‰øùÂ≠ò
                                        interim_output_filename = os.path.join(outputs_folder, f'{job_id}_combined_interim_{chunk_idx+1}.mp4')
                                        print(translate("‰∏≠ÈñìÁµêÊûú„Çí‰øùÂ≠ò‰∏≠: „ÉÅ„É£„É≥„ÇØ{0}/{1}").format(chunk_idx+1, num_chunks))
                                        stream.output_queue.push(('progress', (None, translate("‰∏≠ÈñìÁµêÊûú„ÅÆMP4Â§âÊèõ‰∏≠... („ÉÅ„É£„É≥„ÇØ{0}/{1})").format(chunk_idx+1, num_chunks), make_progress_bar_html(int(85 + chunk_progress * 0.1), translate('MP4‰øùÂ≠ò‰∏≠')))))

                                        # MP4„Å®„Åó„Å¶‰øùÂ≠ò
                                        save_bcthw_as_mp4(combined_history_pixels, interim_output_filename, fps=30, crf=mp4_crf)
                                        print(translate("‰∏≠ÈñìÁµêÊûú„Çí‰øùÂ≠ò„Åó„Åæ„Åó„Åü: {0}").format(interim_output_filename))

                                        # ÁµêÂêà„Åó„ÅüÂãïÁîª„ÇíUI„Å´ÂèçÊò†„Åô„Çã„Åü„ÇÅ„ÄÅÂá∫Âäõ„Éï„É©„Ç∞„ÇíÁ´ã„Å¶„Çã
                                        stream.output_queue.push(('file', interim_output_filename))
                                except Exception as e:
                                    print(translate("„ÉÅ„É£„É≥„ÇØ{0}„ÅÆÂá¶ÁêÜ‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {1}").format(chunk_idx+1, e))
                                    traceback.print_exc()

                                    # „Ç®„É©„ÉºÊÉÖÂ†±„ÅÆË©≥Á¥∞„Å™Âá∫Âäõ
                                    print(translate("„Ç®„É©„ÉºÊÉÖÂ†±:"))
                                    print(translate("  - „ÉÅ„É£„É≥„ÇØÊÉÖÂ†±: {0}/{1}, „Éï„É¨„Éº„É† {2}-{3}/{4}").format(chunk_idx+1, num_chunks, chunk_start+1, chunk_end, uploaded_frames))
                                    if 'current_chunk' in locals():
                                        print(translate("  - current_chunk: shape={0}, dtype={1}, device={2}").format(current_chunk.shape, current_chunk.dtype, current_chunk.device))
                                    if 'vae' in globals():
                                        print(translate("  - VAEÊÉÖÂ†±: device={0}, dtype={1}").format(vae.device, next(vae.parameters()).dtype))

                                    # GPU„É°„É¢„É™ÊÉÖÂ†±
                                    if torch.cuda.is_available():
                                        print(translate("  - GPU‰ΩøÁî®Èáè: {0:.2f}GB/{1:.2f}GB").format(torch.cuda.memory_allocated()/1024**3, torch.cuda.get_device_properties(0).total_memory/1024**3))

                                    stream.output_queue.push(('progress', (None, translate("„Ç®„É©„Éº: „ÉÅ„É£„É≥„ÇØ{0}„ÅÆÂá¶ÁêÜ„Å´Â§±Êïó„Åó„Åæ„Åó„Åü - {1}").format(chunk_idx+1, str(e)), make_progress_bar_html(90, translate('„Ç®„É©„Éº')))))
                                    break

                            # Âá¶ÁêÜÂÆå‰∫ÜÂæå„Å´ÊòéÁ§∫ÁöÑ„Å´„É°„É¢„É™Ëß£Êîæ
                            if torch.cuda.is_available():
                                torch.cuda.synchronize()
                                torch.cuda.empty_cache()
                                import gc
                                gc.collect()
                                print(translate("„ÉÅ„É£„É≥„ÇØÂá¶ÁêÜÂæå„ÅÆGPU„É°„É¢„É™Á¢∫‰øùÁä∂ÊÖã: {0:.2f}GB").format(torch.cuda.memory_allocated()/1024**3))

                            # ÂÖ®„ÉÅ„É£„É≥„ÇØ„ÅÆÂá¶ÁêÜ„ÅåÂÆå‰∫Ü„Åó„Åü„Çâ„ÄÅÊúÄÁµÇÁöÑ„Å™ÁµêÂêàÂãïÁîª„Çí‰øùÂ≠ò
                            if combined_history_pixels is not None:
                                # ÊúÄÁµÇÁµêÊûú„ÅÆ‰øùÂ≠ò
                                print(translate("ÊúÄÁµÇÁµêÊûú„Çí‰øùÂ≠ò‰∏≠: ÂÖ®{0}„ÉÅ„É£„É≥„ÇØÂÆå‰∫Ü").format(num_chunks))
                                stream.output_queue.push(('progress', (None, translate("ÁµêÂêà„Åó„ÅüÂãïÁîª„ÇíMP4„Å´Â§âÊèõ‰∏≠..."), make_progress_bar_html(95, translate('ÊúÄÁµÇMP4Â§âÊèõÂá¶ÁêÜ')))))

                                # ÊúÄÁµÇÁöÑ„Å™ÁµêÂêà„Éï„Ç°„Ç§„É´Âêç
                                combined_output_filename = os.path.join(outputs_folder, f'{job_id}_combined.mp4')

                                # MP4„Å®„Åó„Å¶‰øùÂ≠ò
                                save_bcthw_as_mp4(combined_history_pixels, combined_output_filename, fps=30, crf=mp4_crf)
                                print(translate("ÊúÄÁµÇÁµêÊûú„Çí‰øùÂ≠ò„Åó„Åæ„Åó„Åü: {0}").format(combined_output_filename))
                                print(translate("ÁµêÂêàÂãïÁîª„ÅÆ‰øùÂ≠òÂ†¥ÊâÄ: {0}").format(os.path.abspath(combined_output_filename)))

                                # ‰∏≠Èñì„Éï„Ç°„Ç§„É´„ÅÆÂâäÈô§Âá¶ÁêÜ
                                print(translate("‰∏≠Èñì„Éï„Ç°„Ç§„É´„ÅÆÂâäÈô§„ÇíÈñãÂßã„Åó„Åæ„Åô..."))
                                deleted_files = []
                                try:
                                    # ÁèæÂú®„ÅÆ„Ç∏„Éß„ÉñID„Å´Èñ¢ÈÄ£„Åô„Çã‰∏≠Èñì„Éï„Ç°„Ç§„É´„ÇíÊ≠£Ë¶èË°®Áèæ„Åß„Éï„Ç£„É´„Çø„É™„É≥„Ç∞
                                    import re
                                    interim_pattern = re.compile(f'{job_id}_combined_interim_\d+\.mp4')

                                    for filename in os.listdir(outputs_folder):
                                        if interim_pattern.match(filename):
                                            interim_path = os.path.join(outputs_folder, filename)
                                            try:
                                                os.remove(interim_path)
                                                deleted_files.append(filename)
                                                print(translate("  - ‰∏≠Èñì„Éï„Ç°„Ç§„É´„ÇíÂâäÈô§„Åó„Åæ„Åó„Åü: {0}").format(filename))
                                            except Exception as e:
                                                print(translate("  - „Éï„Ç°„Ç§„É´ÂâäÈô§„Ç®„É©„Éº ({0}): {1}").format(filename, e))

                                    if deleted_files:
                                        print(translate("ÂêàË®à {0} ÂÄã„ÅÆ‰∏≠Èñì„Éï„Ç°„Ç§„É´„ÇíÂâäÈô§„Åó„Åæ„Åó„Åü").format(len(deleted_files)))
                                        # ÂâäÈô§„Éï„Ç°„Ç§„É´Âêç„Çí„É¶„Éº„Ç∂„Éº„Å´Ë°®Á§∫
                                        files_str = ', '.join(deleted_files)
                                        stream.output_queue.push(('progress', (None, translate("‰∏≠Èñì„Éï„Ç°„Ç§„É´„ÇíÂâäÈô§„Åó„Åæ„Åó„Åü: {0}").format(files_str), make_progress_bar_html(97, translate('„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„ÉóÂÆå‰∫Ü')))))
                                    else:
                                        print(translate("ÂâäÈô§ÂØæË±°„ÅÆ‰∏≠Èñì„Éï„Ç°„Ç§„É´„ÅØË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü"))
                                except Exception as e:
                                    print(translate("‰∏≠Èñì„Éï„Ç°„Ç§„É´ÂâäÈô§‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {0}").format(e))
                                    traceback.print_exc()

                                # ÁµêÂêà„Åó„ÅüÂãïÁîª„ÇíUI„Å´ÂèçÊò†„Åô„Çã„Åü„ÇÅ„ÄÅÂá∫Âäõ„Éï„É©„Ç∞„ÇíÁ´ã„Å¶„Çã
                                stream.output_queue.push(('file', combined_output_filename))

                                # ÁµêÂêàÂæå„ÅÆÂÖ®„Éï„É¨„Éº„É†Êï∞„ÇíË®àÁÆó„Åó„Å¶Ë°®Á§∫
                                combined_frames = combined_history_pixels.shape[2]
                                combined_size_mb = (combined_history_pixels.element_size() * combined_history_pixels.nelement()) / (1024 * 1024)
                                print(translate("ÁµêÂêàÂÆå‰∫ÜÊÉÖÂ†±: „ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø({0}„Éï„É¨„Éº„É†) + Êñ∞Ë¶èÂãïÁîª({1}„Éï„É¨„Éº„É†) = ÂêàË®à{2}„Éï„É¨„Éº„É†").format(uploaded_frames, original_frames, combined_frames))
                                print(translate("ÁµêÂêàÂãïÁîª„ÅÆÂÜçÁîüÊôÇÈñì: {0:.2f}Áßí").format(combined_frames / 30))
                                print(translate("„Éá„Éº„Çø„Çµ„Ç§„Ç∫: {0:.2f} MBÔºàÂà∂ÈôêÁÑ°„ÅóÔºâ").format(combined_size_mb))

                                # UI‰∏ä„ÅßÂÆå‰∫Ü„É°„ÉÉ„Çª„Éº„Ç∏„ÇíË°®Á§∫
                                stream.output_queue.push(('progress', (None, translate("„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø({0}„Éï„É¨„Éº„É†)„Å®ÂãïÁîª({1}„Éï„É¨„Éº„É†)„ÅÆÁµêÂêà„ÅåÂÆå‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ\nÂêàË®à„Éï„É¨„Éº„É†Êï∞: {2}„Éï„É¨„Éº„É† ({3:.2f}Áßí) - „Çµ„Ç§„Ç∫Âà∂Èôê„Å™„Åó").format(uploaded_frames, original_frames, combined_frames, combined_frames / 30), make_progress_bar_html(100, translate('ÁµêÂêàÂÆå‰∫Ü')))))
                            else:
                                print(translate("„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„ÅÆÁµêÂêà„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇ"))
                                stream.output_queue.push(('progress', (None, translate("„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„ÅÆÁµêÂêà„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇ"), make_progress_bar_html(100, translate('„Ç®„É©„Éº')))))


                            # real_history_latents„Å®history_pixels„ÇíÁµêÂêàÊ∏à„Åø„ÅÆ„ÇÇ„ÅÆ„Å´Êõ¥Êñ∞
                            real_history_latents = combined_history_latents
                            history_pixels = combined_history_pixels

                            # ÁµêÂêà„Åó„ÅüÂãïÁîª„ÇíUI„Å´ÂèçÊò†„Åô„Çã„Åü„ÇÅ„ÄÅÂá∫Âäõ„Éï„É©„Ç∞„ÇíÁ´ã„Å¶„Çã
                            stream.output_queue.push(('file', combined_output_filename))

                            # Âá∫Âäõ„Éï„Ç°„Ç§„É´Âêç„ÇíÊõ¥Êñ∞
                            output_filename = combined_output_filename

                            # ÁµêÂêàÂæå„ÅÆÂÖ®„Éï„É¨„Éº„É†Êï∞„ÇíË®àÁÆó„Åó„Å¶Ë°®Á§∫
                            combined_frames = combined_history_pixels.shape[2]
                            combined_size_mb = (combined_history_pixels.element_size() * combined_history_pixels.nelement()) / (1024 * 1024)
                            print(translate("ÁµêÂêàÂÆå‰∫ÜÊÉÖÂ†±: „ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø({0}„Éï„É¨„Éº„É†) + Êñ∞Ë¶èÂãïÁîª({1}„Éï„É¨„Éº„É†) = ÂêàË®à{2}„Éï„É¨„Éº„É†").format(uploaded_frames, original_frames, combined_frames))
                            print(translate("ÁµêÂêàÂãïÁîª„ÅÆÂÜçÁîüÊôÇÈñì: {0:.2f}Áßí").format(combined_frames / 30))
                            print(translate("„Éá„Éº„Çø„Çµ„Ç§„Ç∫: {0:.2f} MBÔºàÂà∂ÈôêÁÑ°„ÅóÔºâ").format(combined_size_mb))

                            # UI‰∏ä„ÅßÂÆå‰∫Ü„É°„ÉÉ„Çª„Éº„Ç∏„ÇíË°®Á§∫
                            stream.output_queue.push(('progress', (None, translate("„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø({0}„Éï„É¨„Éº„É†)„Å®ÂãïÁîª({1}„Éï„É¨„Éº„É†)„ÅÆÁµêÂêà„ÅåÂÆå‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ\nÂêàË®à„Éï„É¨„Éº„É†Êï∞: {2}„Éï„É¨„Éº„É† ({3:.2f}Áßí)").format(uploaded_frames, original_frames, combined_frames, combined_frames / 30), make_progress_bar_html(100, translate('ÁµêÂêàÂÆå‰∫Ü')))))
                    except Exception as e:
                        print(translate("„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„ÇøÁµêÂêà‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {0}").format(e))
                        traceback.print_exc()
                        stream.output_queue.push(('progress', (None, translate("„Ç®„É©„Éº: „ÉÜ„É≥„ÇΩ„É´„Éá„Éº„ÇøÁµêÂêà„Å´Â§±Êïó„Åó„Åæ„Åó„Åü - {0}").format(str(e)), make_progress_bar_html(100, translate('„Ç®„É©„Éº')))))

                # Âá¶ÁêÜÁµÇ‰∫ÜÊôÇ„Å´ÈÄöÁü•Ôºà„Ç¢„É©„Éº„É†Ë®≠ÂÆö„ÅåÊúâÂäπ„Å™Â†¥Âêà„ÅÆ„ÅøÔºâ
                # „Ç¢„É©„Éº„É†Âà§ÂÆö„ÇíË°å„ÅÜÔºàGradio„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„Åã„ÇâÊ≠£„Åó„ÅèÂÄ§„ÇíÂèñÂæóÔºâ
                should_play_alarm = False  # „Éá„Éï„Ç©„É´„Éà„ÅØ„Ç™„Éï
                
                # Gradio„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Åã„Çâ„ÅÆÂÄ§ÂèñÂæó
                if isinstance(alarm_on_completion, bool):
                    should_play_alarm = alarm_on_completion
                elif hasattr(alarm_on_completion, 'value') and isinstance(alarm_on_completion.value, bool):
                    should_play_alarm = alarm_on_completion.value
                else:
                    # UI„Åã„Çâ„ÅÆÂÄ§ÂèñÂæó„Å´Â§±Êïó„Åó„ÅüÂ†¥Âêà„ÅØË®≠ÂÆö„Éï„Ç°„Ç§„É´„Åã„ÇâÂèñÂæó
                    try:
                        from eichi_utils.settings_manager import load_app_settings_f1
                        app_settings = load_app_settings_f1()
                        if app_settings and "alarm_on_completion" in app_settings:
                            should_play_alarm = app_settings["alarm_on_completion"]
                    except:
                        # Ë®≠ÂÆö„Éï„Ç°„Ç§„É´„Åã„Çâ„ÇÇÂèñÂæó„Åß„Åç„Å™„ÅÑÂ†¥Âêà„ÅØ„Éá„Éï„Ç©„É´„Éà„Åß„Ç™„Éï
                        should_play_alarm = False
                
                if should_play_alarm:
                    if HAS_WINSOUND:
                        winsound.PlaySound("SystemExclamation", winsound.SND_ALIAS)
                    else:
                        print(translate("Âá¶ÁêÜ„ÅåÂÆå‰∫Ü„Åó„Åæ„Åó„Åü"))  # Linux„Åß„ÅÆ‰ª£ÊõøÈÄöÁü•

                # „É°„É¢„É™Ëß£Êîæ„ÇíÊòéÁ§∫ÁöÑ„Å´ÂÆüË°å
                if torch.cuda.is_available():
                    torch.cuda.synchronize()
                    torch.cuda.empty_cache()
                    import gc
                    gc.collect()
                    print(translate("Âá¶ÁêÜÂÆå‰∫ÜÂæå„ÅÆ„É°„É¢„É™„ÇØ„É™„Ç¢: {memory:.2f}GB/{total_memory:.2f}GB").format(memory=torch.cuda.memory_allocated()/1024**3, total_memory=torch.cuda.get_device_properties(0).total_memory/1024**3))

                # „ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„ÅÆ‰øùÂ≠òÂá¶ÁêÜ
                if save_tensor_data:
                    try:
                        # ÁµêÊûú„ÅÆ„ÉÜ„É≥„ÇΩ„É´„Çí‰øùÂ≠ò„Åô„Çã„Éï„Ç°„Ç§„É´„Éë„Çπ
                        tensor_file_path = os.path.join(outputs_folder, f'{job_id}.safetensors')

                        # ‰øùÂ≠ò„Åô„Çã„Éá„Éº„Çø„ÇíÊ∫ñÂÇô
                        print(translate("=== „ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø‰øùÂ≠òÂá¶ÁêÜÈñãÂßã ==="))
                        print(translate("‰øùÂ≠òÂØæË±°„Éï„É¨„Éº„É†Êï∞: {frames}").format(frames=real_history_latents.shape[2]))

                        # „Çµ„Ç§„Ç∫Âà∂Èôê„ÇíÂÆåÂÖ®„Å´Êí§ÂªÉ„Åó„ÄÅÂÖ®„Éï„É¨„Éº„É†„Çí‰øùÂ≠ò
                        tensor_to_save = real_history_latents.clone().cpu()

                        # „ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„ÅÆ‰øùÂ≠ò„Çµ„Ç§„Ç∫„ÅÆÊ¶ÇÁÆó
                        tensor_size_mb = (tensor_to_save.element_size() * tensor_to_save.nelement()) / (1024 * 1024)

                        print(translate("„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„Çí‰øùÂ≠ò‰∏≠... shape: {shape}, „Éï„É¨„Éº„É†Êï∞: {frames}, „Çµ„Ç§„Ç∫: {size:.2f} MB").format(shape=tensor_to_save.shape, frames=tensor_to_save.shape[2], size=tensor_size_mb))
                        stream.output_queue.push(('progress', (None, translate('„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„Çí‰øùÂ≠ò‰∏≠... ({frames}„Éï„É¨„Éº„É†)').format(frames=tensor_to_save.shape[2]), make_progress_bar_html(95, translate('„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„ÅÆ‰øùÂ≠ò')))))

                        # „É°„Çø„Éá„Éº„Çø„ÅÆÊ∫ñÂÇôÔºà„Éï„É¨„Éº„É†Êï∞„ÇÇÂê´„ÇÅ„ÇãÔºâ
                        metadata = torch.tensor([height, width, tensor_to_save.shape[2]], dtype=torch.int32)

                        # safetensorsÂΩ¢Âºè„Åß‰øùÂ≠ò
                        tensor_dict = {
                            "history_latents": tensor_to_save,
                            "metadata": metadata
                        }
                        sf.save_file(tensor_dict, tensor_file_path)

                        print(translate("„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„Çí‰øùÂ≠ò„Åó„Åæ„Åó„Åü: {path}").format(path=tensor_file_path))
                        print(translate("‰øùÂ≠òÊ∏à„Åø„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„ÇøÊÉÖÂ†±: {frames}„Éï„É¨„Éº„É†, {size:.2f} MB").format(frames=tensor_to_save.shape[2], size=tensor_size_mb))
                        print(translate("=== „ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø‰øùÂ≠òÂá¶ÁêÜÂÆå‰∫Ü ==="))
                        stream.output_queue.push(('progress', (None, translate("„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„Åå‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü: {path} ({frames}„Éï„É¨„Éº„É†, {size:.2f} MB)").format(path=os.path.basename(tensor_file_path), frames=tensor_to_save.shape[2], size=tensor_size_mb), make_progress_bar_html(100, translate('Âá¶ÁêÜÂÆå‰∫Ü')))))

                        # „Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åï„Çå„Åü„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„Åå„ÅÇ„Çå„Å∞„ÄÅ„Åù„Çå„ÇÇÁµêÂêà„Åó„Åü„ÇÇ„ÅÆ„Çí‰øùÂ≠ò„Åô„Çã
                        if tensor_data_input is not None and uploaded_tensor is not None:
                            try:
                                # „Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åï„Çå„Åü„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„ÅÆ„Éï„Ç°„Ç§„É´Âêç„ÇíÂèñÂæó
                                uploaded_tensor_filename = os.path.basename(tensor_data_input.name)
                                tensor_combined_path = os.path.join(outputs_folder, f'{job_id}_combined_tensors.safetensors')

                                print(translate("=== „ÉÜ„É≥„ÇΩ„É´„Éá„Éº„ÇøÁµêÂêàÂá¶ÁêÜÈñãÂßã ==="))
                                print(translate("ÁîüÊàê„ÉÜ„É≥„ÇΩ„É´„Å®ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´„ÇíÁµêÂêà„Åó„Å¶‰øùÂ≠ò„Åó„Åæ„Åô"))
                                print(translate("ÁîüÊàê„ÉÜ„É≥„ÇΩ„É´: {frames}„Éï„É¨„Éº„É†").format(frames=tensor_to_save.shape[2]))
                                print(translate("ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´: {frames}„Éï„É¨„Éº„É†").format(frames=uploaded_tensor.shape[2]))

                                # „Éá„Éº„ÇøÂûã„Å®„Éá„Éê„Ç§„Çπ„ÇíÁµ±‰∏Ä
                                if uploaded_tensor.dtype != tensor_to_save.dtype:
                                    uploaded_tensor = uploaded_tensor.to(dtype=tensor_to_save.dtype)
                                if uploaded_tensor.device != tensor_to_save.device:
                                    uploaded_tensor = uploaded_tensor.to(device=tensor_to_save.device)

                                # „Çµ„Ç§„Ç∫„ÉÅ„Çß„ÉÉ„ÇØ
                                if uploaded_tensor.shape[3] != tensor_to_save.shape[3] or uploaded_tensor.shape[4] != tensor_to_save.shape[4]:
                                    print(translate("Ë≠¶Âëä: „ÉÜ„É≥„ÇΩ„É´„Çµ„Ç§„Ç∫„Åå‰∏ÄËá¥„Åó„Å™„ÅÑ„Åü„ÇÅÁµêÂêà„Åß„Åç„Åæ„Åõ„Çì: {uploaded_shape} vs {tensor_shape}").format(uploaded_shape=uploaded_tensor.shape, tensor_shape=tensor_to_save.shape))
                                else:
                                    # ÁµêÂêàÔºàÁîüÊàê„ÉÜ„É≥„ÇΩ„É´„ÅÆÂæå„Å´„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åï„Çå„Åü„ÉÜ„É≥„ÇΩ„É´„ÇíËøΩÂä†Ôºâ
                                    combined_tensor = torch.cat([tensor_to_save, uploaded_tensor], dim=2)
                                    combined_frames = combined_tensor.shape[2]
                                    combined_size_mb = (combined_tensor.element_size() * combined_tensor.nelement()) / (1024 * 1024)

                                    # „É°„Çø„Éá„Éº„ÇøÊõ¥Êñ∞
                                    combined_metadata = torch.tensor([height, width, combined_frames], dtype=torch.int32)

                                    # ÁµêÂêà„Åó„Åü„ÉÜ„É≥„ÇΩ„É´„Çí‰øùÂ≠ò
                                    combined_tensor_dict = {
                                        "history_latents": combined_tensor,
                                        "metadata": combined_metadata
                                    }
                                    sf.save_file(combined_tensor_dict, tensor_combined_path)

                                    print(translate("ÁµêÂêà„ÉÜ„É≥„ÇΩ„É´„Çí‰øùÂ≠ò„Åó„Åæ„Åó„Åü: {path}").format(path=tensor_combined_path))
                                    print(translate("ÁµêÂêà„ÉÜ„É≥„ÇΩ„É´ÊÉÖÂ†±: ÂêàË®à{0}„Éï„É¨„Éº„É† ({1}+{2}), {3:.2f} MB").format(frames, tensor_to_save.shape[2], uploaded_tensor.shape[2], size))
                                    print(translate("=== „ÉÜ„É≥„ÇΩ„É´„Éá„Éº„ÇøÁµêÂêàÂá¶ÁêÜÂÆå‰∫Ü ==="))
                                    stream.output_queue.push(('progress', (None, translate("„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„ÇøÁµêÂêà„Åå‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü: ÂêàË®à{frames}„Éï„É¨„Éº„É†").format(frames=combined_frames), make_progress_bar_html(100, translate('ÁµêÂêà„ÉÜ„É≥„ÇΩ„É´‰øùÂ≠òÂÆå‰∫Ü')))))
                            except Exception as e:
                                print(translate("„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„ÇøÁµêÂêà‰øùÂ≠ò„Ç®„É©„Éº: {0}").format(e))
                                traceback.print_exc()
                    except Exception as e:
                        print(translate("„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø‰øùÂ≠ò„Ç®„É©„Éº: {0}").format(e))
                        traceback.print_exc()
                        stream.output_queue.push(('progress', (None, translate("„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„ÅÆ‰øùÂ≠ò‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü„ÄÇ"), make_progress_bar_html(100, translate('Âá¶ÁêÜÂÆå‰∫Ü')))))

                # ÂÖ®‰Ωì„ÅÆÂá¶ÁêÜÊôÇÈñì„ÇíË®àÁÆó
                process_end_time = time.time()
                total_process_time = process_end_time - process_start_time
                hours, remainder = divmod(total_process_time, 3600)
                minutes, seconds = divmod(remainder, 60)
                time_str = ""
                if hours > 0:
                    time_str = translate("{0}ÊôÇÈñì {1}ÂàÜ {2}Áßí").format(int(hours), int(minutes), f"{seconds:.1f}")
                elif minutes > 0:
                    time_str = translate("{0}ÂàÜ {1}Áßí").format(int(minutes), f"{seconds:.1f}")
                else:
                    time_str = translate("{0:.1f}Áßí").format(seconds)
                print(translate("ÂÖ®‰Ωì„ÅÆÂá¶ÁêÜÊôÇÈñì: {0}").format(time_str))

                # ÂÆå‰∫Ü„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆË®≠ÂÆöÔºàÁµêÂêàÊúâÁÑ°„Å´„Çà„Å£„Å¶Â§âÊõ¥Ôºâ
                if combined_output_filename is not None:
                    # „ÉÜ„É≥„ÇΩ„É´ÁµêÂêà„ÅåÊàêÂäü„Åó„ÅüÂ†¥Âêà„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏
                    combined_filename_only = os.path.basename(combined_output_filename)
                    completion_message = translate("„Åô„Åπ„Å¶„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥({sections}/{total_sections})„ÅåÂÆå‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„Å®„ÅÆÂæåÊñπÁµêÂêà„ÇÇÂÆå‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇÁµêÂêà„Éï„Ç°„Ç§„É´Âêç: {filename}\nÂÖ®‰Ωì„ÅÆÂá¶ÁêÜÊôÇÈñì: {time}").format(sections=sections, total_sections=total_sections, filename=combined_filename_only, time=time_str)
                    # ÊúÄÁµÇÁöÑ„Å™Âá∫Âäõ„Éï„Ç°„Ç§„É´„ÇíÁµêÂêà„Åó„Åü„ÇÇ„ÅÆ„Å´Â§âÊõ¥
                    output_filename = combined_output_filename
                else:
                    # ÈÄöÂ∏∏„ÅÆÂÆå‰∫Ü„É°„ÉÉ„Çª„Éº„Ç∏
                    completion_message = translate("„Åô„Åπ„Å¶„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥({sections}/{total_sections})„ÅåÂÆå‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇÂÖ®‰Ωì„ÅÆÂá¶ÁêÜÊôÇÈñì: {time}").format(sections=total_sections, total_sections=total_sections, time=time_str)

                stream.output_queue.push(('progress', (None, completion_message, make_progress_bar_html(100, translate('Âá¶ÁêÜÂÆå‰∫Ü')))))

                # ‰∏≠Èñì„Éï„Ç°„Ç§„É´„ÅÆÂâäÈô§Âá¶ÁêÜ
                if not keep_section_videos:
                    # ÊúÄÁµÇÂãïÁîª„ÅÆ„Éï„É´„Éë„Çπ
                    final_video_path = output_filename
                    final_video_name = os.path.basename(final_video_path)
                    # job_idÈÉ®ÂàÜ„ÇíÂèñÂæóÔºà„Çø„Ç§„É†„Çπ„Çø„É≥„ÉóÈÉ®ÂàÜÔºâ
                    job_id_part = job_id

                    # „Éá„Ç£„É¨„ÇØ„Éà„É™ÂÜÖ„ÅÆ„Åô„Åπ„Å¶„ÅÆ„Éï„Ç°„Ç§„É´„ÇíÂèñÂæó
                    files = os.listdir(outputs_folder)
                    deleted_count = 0

                    for file in files:
                        # Âêå„Åòjob_id„ÇíÊåÅ„Å§MP4„Éï„Ç°„Ç§„É´„Åã„ÉÅ„Çß„ÉÉ„ÇØ
                        # ÁµêÂêà„Éï„Ç°„Ç§„É´('combined'„ÇíÂê´„ÇÄ)„ÅØÊ∂à„Åï„Å™„ÅÑ„Çà„ÅÜ„Å´‰øùË≠∑
                        if file.startswith(job_id_part) and file.endswith('.mp4') \
                           and file != final_video_name \
                           and 'combined' not in file:  # combined„Éï„Ç°„Ç§„É´„ÅØ‰øùË≠∑
                            file_path = os.path.join(outputs_folder, file)
                            try:
                                os.remove(file_path)
                                deleted_count += 1
                                print(translate("‰∏≠Èñì„Éï„Ç°„Ç§„É´: {0}").format(file))
                            except Exception as e:
                                print(translate("„Éï„Ç°„Ç§„É´ÂâäÈô§ÊôÇ„ÅÆ„Ç®„É©„Éº {0}: {1}").format(file, e))

                    if deleted_count > 0:
                        print(translate("{0}ÂÄã„ÅÆ‰∏≠Èñì„Éï„Ç°„Ç§„É´„ÇíÂâäÈô§„Åó„Åæ„Åó„Åü„ÄÇÊúÄÁµÇ„Éï„Ç°„Ç§„É´„ÅØ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Åæ„Åô: {1}").format(deleted_count, final_video_name))
                        final_message = translate("‰∏≠Èñì„Éï„Ç°„Ç§„É´„ÇíÂâäÈô§„Åó„Åæ„Åó„Åü„ÄÇÊúÄÁµÇÂãïÁîª„Å®ÁµêÂêàÂãïÁîª„ÅØ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ")
                        stream.output_queue.push(('progress', (None, final_message, make_progress_bar_html(100, translate('Âá¶ÁêÜÂÆå‰∫Ü')))))

                break
    except:
        traceback.print_exc()

        if not high_vram:
            unload_complete_models(
                text_encoder, text_encoder_2, image_encoder, vae, transformer
            )

    stream.output_queue.push(('end', None))
    return

# ÁîªÂÉè„ÅÆ„Éê„É™„Éá„Éº„Ç∑„Éß„É≥Èñ¢Êï∞
def validate_images(input_image, section_settings, length_radio=None, frame_size_radio=None):
    """ÂÖ•ÂäõÁîªÂÉè„Åæ„Åü„ÅØÁîªÈù¢„Å´Ë°®Á§∫„Åï„Çå„Å¶„ÅÑ„ÇãÊúÄÂæå„ÅÆ„Ç≠„Éº„Éï„É¨„Éº„É†ÁîªÂÉè„ÅÆ„ÅÑ„Åö„Çå„Åã„ÅåÊúâÂäπ„Åã„ÇíÁ¢∫Ë™ç„Åô„Çã - SLIDER PRIORITIZED"""
    # ÂÖ•ÂäõÁîªÂÉè„Çí„ÉÅ„Çß„ÉÉ„ÇØ
    if input_image is not None:
        return True, ""

    # ÁèæÂú®„ÅÆË®≠ÂÆö„Åã„ÇâË°®Á§∫„Åô„Åπ„Åç„Çª„ÇØ„Ç∑„Éß„É≥Êï∞„ÇíË®àÁÆó
    total_display_sections = None
    if frame_size_radio is not None:
        try:
            # Try to get slider value from global components first
            seconds = None
            
            # Access the slider value directly from global components
            global current_ui_components
            if 'total_second_length' in current_ui_components:
                slider_component = current_ui_components['total_second_length']
                if hasattr(slider_component, 'value'):
                    seconds = slider_component.value
                    print(translate("üéØ validate_images using SLIDER value: {0}s").format(seconds))
            
            # Fallback to radio if slider not available
            if seconds is None and length_radio is not None:
                seconds = get_video_seconds(length_radio.value)
                print(translate("üîÑ validate_images fallback to RADIO value: {0}s").format(seconds))
            
            # Default fallback
            if seconds is None:
                seconds = 1
                print(translate("‚ö†Ô∏è validate_images using DEFAULT value: {0}s").format(seconds))

            # „Éï„É¨„Éº„É†„Çµ„Ç§„Ç∫Ë®≠ÂÆö„Åã„Çâlatent_window_size„ÇíË®àÁÆó
            latent_window_size = 4.5 if frame_size_radio.value == translate("0.5Áßí (17„Éï„É¨„Éº„É†)") else 9
            frame_count = latent_window_size * 4 - 3

            # „Çª„ÇØ„Ç∑„Éß„É≥Êï∞„ÇíË®àÁÆó
            total_frames = int(seconds * 30)
            total_display_sections = int(max(round(total_frames / frame_count), 1))
            
        except Exception as e:
            print(translate("„Çª„ÇØ„Ç∑„Éß„É≥Êï∞Ë®àÁÆó„Ç®„É©„Éº: {0}").format(e))

    # ÂÖ•ÂäõÁîªÂÉè„Åå„Å™„ÅÑÂ†¥Âêà„ÄÅË°®Á§∫„Åï„Çå„Å¶„ÅÑ„Çã„Çª„ÇØ„Ç∑„Éß„É≥„ÅÆ‰∏≠„ÅßÊúÄÂæå„ÅÆ„Ç≠„Éº„Éï„É¨„Éº„É†ÁîªÂÉè„Çí„ÉÅ„Çß„ÉÉ„ÇØ
    last_visible_section_image = None
    last_visible_section_num = -1

    if section_settings is not None and not isinstance(section_settings, bool):
        # ÊúâÂäπ„Å™„Çª„ÇØ„Ç∑„Éß„É≥Áï™Âè∑„ÇíÂèéÈõÜ
        valid_sections = []
        try:
            for section in section_settings:
                if section and len(section) > 1 and section[0] is not None:
                    try:
                        section_num = int(section[0])
                        # Ë°®Á§∫„Çª„ÇØ„Ç∑„Éß„É≥Êï∞„ÅåË®àÁÆó„Åï„Çå„Å¶„ÅÑ„Çå„Å∞„ÄÅ„Åù„Çå‰ª•‰∏ã„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥„ÅÆ„ÅøËøΩÂä†
                        if total_display_sections is None or section_num < total_display_sections:
                            valid_sections.append((section_num, section[1]))
                    except (ValueError, TypeError):
                        continue
        except (TypeError, ValueError):
            # section_settings„Åå„Ç§„ÉÜ„É©„Éñ„É´„Åß„Å™„ÅÑÂ†¥ÂêàÔºà„Éñ„Éº„É´ÂÄ§„Å™„Å©Ôºâ„ÄÅÁ©∫„ÅÆ„É™„Çπ„Éà„Å®„Åó„Å¶Êâ±„ÅÜ
            valid_sections = []

        # ÊúâÂäπ„Å™„Çª„ÇØ„Ç∑„Éß„É≥„Åå„ÅÇ„Çå„Å∞„ÄÅÊúÄÂ§ß„ÅÆÁï™Âè∑ÔºàÊúÄÂæå„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥Ôºâ„ÇíÊé¢„Åô
        if valid_sections:
            # Áï™Âè∑„Åß„ÇΩ„Éº„Éà
            valid_sections.sort(key=lambda x: x[0])
            # ÊúÄÂæå„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÂèñÂæó
            last_visible_section_num, last_visible_section_image = valid_sections[-1]

    # ÊúÄÂæå„ÅÆ„Ç≠„Éº„Éï„É¨„Éº„É†ÁîªÂÉè„Åå„ÅÇ„Çå„Å∞OK
    if last_visible_section_image is not None:
        return True, ""

    # „Å©„Å°„Çâ„ÅÆÁîªÂÉè„ÇÇ„Å™„ÅÑÂ†¥Âêà„ÅØ„Ç®„É©„Éº
    error_html = f"""
    <div style="padding: 15px; border-radius: 10px; background-color: #ffebee; border: 1px solid #f44336; margin: 10px 0;">
        <h3 style="color: #d32f2f; margin: 0 0 10px 0;">{translate('ÁîªÂÉè„ÅåÈÅ∏Êäû„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì')}</h3>
        <p>{translate('ÁîüÊàê„ÇíÈñãÂßã„Åô„ÇãÂâç„Å´„ÄåImage„ÄçÊ¨Ñ„Åæ„Åü„ÅØË°®Á§∫„Åï„Çå„Å¶„ÅÑ„ÇãÊúÄÂæå„ÅÆ„Ç≠„Éº„Éï„É¨„Éº„É†ÁîªÂÉè„Å´ÁîªÂÉè„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Åì„Çå„ÅØÂè°Êô∫„ÅÆÂßãÁô∫ÁÇπ„Å®„Å™„ÇãÈáçË¶Å„Å™ÁîªÂÉè„Åß„Åô„ÄÇ')}</p>
    </div>
    """
    error_bar = make_progress_bar_html(100, translate('ÁîªÂÉè„Åå„ÅÇ„Çä„Åæ„Åõ„Çì'))
    return False, error_html + error_bar

def process(input_image, prompt, n_prompt, seed, total_second_length, latent_window_size, steps, cfg, gs, rs, gpu_memory_preservation, use_teacache, use_random_seed, mp4_crf=16, all_padding_value=1.0, image_strength=1.0, frame_size_setting="1Áßí (33„Éï„É¨„Éº„É†)", keep_section_videos=False, lora_files=None, lora_files2=None, lora_files3=None, lora_scales_text="0.8,0.8,0.8", output_dir=None, save_section_frames=False, use_all_padding=False, use_lora=False, lora_mode=None, lora_dropdown1=None, lora_dropdown2=None, lora_dropdown3=None, save_tensor_data=False, section_settings=None, tensor_data_input=None, fp8_optimization=False, resolution=640, batch_count=1, frame_save_mode=translate("‰øùÂ≠ò„Åó„Å™„ÅÑ"), use_queue=False, prompt_queue_file=None, save_settings_on_start=False, alarm_on_completion=False):
    # ÂºïÊï∞„ÅÆÂûãÁ¢∫Ë™ç
    # Áï∞Â∏∏„Å™Âûã„ÅÆ‰øÆÊ≠£ (bool„Å™„Å©)
    if section_settings is not None and not isinstance(section_settings, list):
        print(translate("section_settings„Åå„É™„Çπ„ÉàÂûã„Åß„ÅØ„ÅÇ„Çä„Åæ„Åõ„ÇìÔºö{0}. ÂàùÊúüÂåñ„Åó„Åæ„Åô„ÄÇ").format(type(section_settings).__name__))
        section_settings = [[None, None, ""] for _ in range(50)]
    # „É°„Ç§„É≥ÁîüÊàêÂá¶ÁêÜ
    global stream
    global batch_stopped
    global queue_enabled, queue_type, prompt_queue_file_path, image_queue_files

    # „Éê„ÉÉ„ÉÅÂá¶ÁêÜÈñãÂßãÊôÇ„Å´ÂÅúÊ≠¢„Éï„É©„Ç∞„Çí„É™„Çª„ÉÉ„Éà
    batch_stopped = False


    # „Éï„É¨„Éº„É†„Çµ„Ç§„Ç∫Ë®≠ÂÆö„Å´Âøú„Åò„Å¶latent_window_size„ÇíÂÖà„Å´Ë™øÊï¥
    if frame_size_setting == "0.5Áßí (17„Éï„É¨„Éº„É†)":
        # 0.5Áßí„ÅÆÂ†¥Âêà„ÅØlatent_window_size=4.5„Å´Ë®≠ÂÆöÔºàÂÆüÈöõ„Å´„ÅØ4.5*4-3=17„Éï„É¨„Éº„É†‚âí0.5Áßí@30fpsÔºâ
        latent_window_size = 4.5
        print(translate('„Éï„É¨„Éº„É†„Çµ„Ç§„Ç∫„Çí0.5Áßí„É¢„Éº„Éâ„Å´Ë®≠ÂÆö: latent_window_size = {0}').format(latent_window_size))
    else:
        # „Éá„Éï„Ç©„É´„Éà„ÅÆ1Áßí„É¢„Éº„Éâ„Åß„ÅØlatent_window_size=9„Çí‰ΩøÁî®Ôºà9*4-3=33„Éï„É¨„Éº„É†‚âí1Áßí@30fpsÔºâ
        latent_window_size = 9
        print(translate('„Éï„É¨„Éº„É†„Çµ„Ç§„Ç∫„Çí1Áßí„É¢„Éº„Éâ„Å´Ë®≠ÂÆö: latent_window_size = {0}').format(latent_window_size))

    # „Éê„ÉÉ„ÉÅÂá¶ÁêÜÂõûÊï∞„ÇíÁ¢∫Ë™ç„Åó„ÄÅË©≥Á¥∞„ÇíÂá∫Âäõ
    batch_count = max(1, min(int(batch_count), 100))  # 1„Äú100„ÅÆÈñì„Å´Âà∂Èôê


    # Check if we're in queue processing mode and should track batch progress
    is_queue_processing = (current_processing_config_name is not None)
    if is_queue_processing:
        print(translate("üìä Queue processing detected - initializing batch progress tracking for {0} batches").format(batch_count))
        update_batch_progress(0, batch_count)  # Initialize progress


    print(translate("„Éê„ÉÉ„ÉÅÂá¶ÁêÜÂõûÊï∞: {0}Âõû").format(batch_count))

    # Ëß£ÂÉèÂ∫¶„ÇíÂÆâÂÖ®„Å™ÂÄ§„Å´‰∏∏„ÇÅ„Å¶„É≠„Ç∞Ë°®Á§∫
    from diffusers_helper.bucket_tools import SAFE_RESOLUTIONS

    # Ëß£ÂÉèÂ∫¶ÂÄ§„ÇíË°®Á§∫
    print(translate("UI„Åã„ÇâÂèó„ÅëÂèñ„Å£„ÅüËß£ÂÉèÂ∫¶ÂÄ§: {0}ÔºàÂûã: {1}Ôºâ").format(resolution, type(resolution).__name__))

    # ÂÆâÂÖ®„Å™ÂÄ§„Å´‰∏∏„ÇÅ„Çã
    if resolution not in SAFE_RESOLUTIONS:
        closest_resolution = min(SAFE_RESOLUTIONS, key=lambda x: abs(x - resolution))
        print(translate('ÂÆâÂÖ®„Å™Ëß£ÂÉèÂ∫¶ÂÄ§„Åß„ÅØ„Å™„ÅÑ„Åü„ÇÅ„ÄÅ{0}„Åã„Çâ{1}„Å´Ëá™ÂãïË™øÊï¥„Åó„Åæ„Åó„Åü').format(resolution, closest_resolution))
        resolution = closest_resolution

    # Ëß£ÂÉèÂ∫¶Ë®≠ÂÆö„ÇíÂá∫Âäõ
    print(translate('Ëß£ÂÉèÂ∫¶„ÇíË®≠ÂÆö: {0}').format(resolution))

    # ÂãïÁîªÁîüÊàê„ÅÆË®≠ÂÆöÊÉÖÂ†±„Çí„É≠„Ç∞„Å´Âá∫Âäõ
    # 4.5„ÅÆÂ†¥Âêà„ÅØ5„Å®„Åó„Å¶Ë®àÁÆó„Åô„Çã„Åü„ÇÅ„ÅÆÁâπÂà•Âá¶ÁêÜ
    if latent_window_size == 4.5:
        frame_count = 17  # 5 * 4 - 3 = 17
    else:
        frame_count = int(latent_window_size * 4 - 3)
    total_latent_sections = int(max(round((total_second_length * 30) / frame_count), 1))

    # F1„É¢„Éº„Éâ„Åß„ÅØÂ∏∏„Å´ÈÄöÂ∏∏„ÅÆ„Åø
    mode_name = translate("ÈÄöÂ∏∏„É¢„Éº„Éâ")

    print(translate("==== ÂãïÁîªÁîüÊàêÈñãÂßã ====="))
    print(translate("ÁîüÊàê„É¢„Éº„Éâ: {0}").format(mode_name))
    print(translate("ÂãïÁîªÈï∑: {0}Áßí").format(total_second_length))
    
    # Ëá™Âãï‰øùÂ≠òÊ©üËÉΩ
    if save_settings_on_start:
        try:
            from eichi_utils.settings_manager import save_app_settings_f1
            current_settings = {
                "resolution": resolution,
                "mp4_crf": mp4_crf,
                "steps": steps,
                "cfg": cfg,
                "use_teacache": use_teacache,
                "gpu_memory_preservation": gpu_memory_preservation,
                "gs": gs,
                "image_strength": image_strength,
                "keep_section_videos": keep_section_videos,
                "save_section_frames": save_section_frames,
                "save_tensor_data": save_tensor_data,
                "frame_save_mode": frame_save_mode,
                "save_settings_on_start": save_settings_on_start,
                "alarm_on_completion": alarm_on_completion
            }
            save_app_settings_f1(current_settings)
            print(translate("Ëá™Âãï‰øùÂ≠ò„ÅåÂÆå‰∫Ü„Åó„Åæ„Åó„Åü"))
        except Exception as e:
            print(translate("Ëá™Âãï‰øùÂ≠ò‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {0}").format(str(e)))
    print(translate("„Éï„É¨„Éº„É†„Çµ„Ç§„Ç∫: {0}").format(frame_size_setting))
    print(translate("ÁîüÊàê„Çª„ÇØ„Ç∑„Éß„É≥Êï∞: {0}Âõû").format(total_latent_sections))
    print(translate("„Çµ„É≥„Éó„É™„É≥„Ç∞„Çπ„ÉÜ„ÉÉ„ÉóÊï∞: {0}").format(steps))
    print(translate("TeaCache‰ΩøÁî®: {0}").format(use_teacache))
    # TeaCache‰ΩøÁî®„ÅÆÁõ¥Âæå„Å´SEEDÂÄ§„ÅÆÊÉÖÂ†±„ÇíË°®Á§∫
    print(translate("‰ΩøÁî®SEEDÂÄ§: {0}").format(seed))
    print(translate("LoRA‰ΩøÁî®: {0}").format(use_lora))

    # FP8ÊúÄÈÅ©ÂåñË®≠ÂÆö„ÅÆ„É≠„Ç∞Âá∫Âäõ
    print(translate("FP8ÊúÄÈÅ©Âåñ: {0}").format(fp8_optimization))

    # „Ç™„Éº„É´„Éë„Éá„Ç£„É≥„Ç∞Ë®≠ÂÆö„ÅÆ„É≠„Ç∞Âá∫ÂäõÔºàF1„É¢„Éº„Éâ„Åß„ÅØÂ∏∏„Å´ÁÑ°ÂäπÔºâ
    print(translate("„Ç™„Éº„É´„Éë„Éá„Ç£„É≥„Ç∞: F1„É¢„Éº„Éâ„Åß„ÅØÁÑ°ÂäπÂåñ„Åï„Çå„Å¶„ÅÑ„Åæ„Åô"))

    # LoRAÊÉÖÂ†±„ÅÆ„É≠„Ç∞Âá∫Âäõ
    if use_lora and has_lora_support:
        all_lora_files = []
        lora_paths = []
        
        # LoRA„ÅÆË™≠„ÅøËæº„ÅøÊñπÂºè„Å´Âøú„Åò„Å¶Âá¶ÁêÜ„ÇíÂàÜÂ≤ê
        if lora_mode == translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû"):
            # „Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû„É¢„Éº„Éâ„ÅÆÂ†¥Âêà
            print(translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû„É¢„Éº„Éâ„ÅßLoRA„ÇíÂá¶ÁêÜ"))
            lora_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'lora')
            
            # Áâπ„Å´lora_dropdown2„ÅÆÂÄ§„ÅåÂïèÈ°å„Å´„Å™„Çã„Åì„Å®„ÅåÂ§ö„ÅÑ„ÅÆ„ÅßË©≥Á¥∞„É≠„Ç∞
            if isinstance(lora_dropdown2, int) and lora_dropdown2 == 0:
                
                # ÁâπÂà•Âá¶ÁêÜ: Êï∞ÂÄ§„ÅÆ0„ÅØ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Å®„Åó„Å¶Ëß£Èáà„Åï„Çå„Å¶„ÅÑ„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çã
                # ÈÅ∏ÊäûËÇ¢„É™„Çπ„Éà„ÅÆ0Áï™ÁõÆÔºà„Å™„ÅóÔºâ„Å®„Åó„Å¶Êâ±„ÅÜ
                dropdown_direct_value = translate("„Å™„Åó")
                        
                # „ÇÇ„ÅóÊó¢„Å´Âá¶ÁêÜÊ∏à„Åø„ÅÆÊñáÂ≠óÂàóÂÄ§„ÅåÂà•„Å´„ÅÇ„Çå„Å∞„Åù„Å°„Çâ„ÇíÂÑ™ÂÖà
                if isinstance(lora_dropdown2, str) and lora_dropdown2 != "0" and lora_dropdown2 != translate("„Å™„Åó"):
                    dropdown_direct_value = lora_dropdown2
            
            # ÂêÑ„Éâ„É≠„ÉÉ„Éó„ÉÄ„Ç¶„É≥„ÅÆÂÄ§„ÇíÂá¶ÁêÜ
            for dropdown, label in zip([lora_dropdown1, lora_dropdown2, lora_dropdown3], ["LoRA1", "LoRA2", "LoRA3"]):
                if dropdown is not None and dropdown != translate("„Å™„Åó") and dropdown != 0:
                    # ÈÅ∏Êäû„ÅÇ„Çä
                    file_path = os.path.join(lora_dir, dropdown)
                    if os.path.exists(file_path):
                        lora_paths.append(file_path)
                        print(translate("{0}ÈÅ∏Êäû: {1}").format(label, dropdown))
                    else:
                        print(translate("ÈÅ∏Êäû„Åï„Çå„Åü{0}„Éï„Ç°„Ç§„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì: {1}").format(label, file_path))
        else:
            # „Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„É¢„Éº„Éâ„ÅÆÂ†¥Âêà
            print(translate("„Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„É¢„Éº„Éâ„ÅßLoRA„ÇíÂá¶ÁêÜ"))
            
            # 1„Å§ÁõÆ„ÅÆLoRA„Éï„Ç°„Ç§„É´„ÇíÂá¶ÁêÜ
            if lora_files is not None:
                if isinstance(lora_files, list):
                    all_lora_files.extend(lora_files)
                else:
                    all_lora_files.append(lora_files)
                    
            # 2„Å§ÁõÆ„ÅÆLoRA„Éï„Ç°„Ç§„É´„ÇíÂá¶ÁêÜ
            if lora_files2 is not None:
                if isinstance(lora_files2, list):
                    all_lora_files.extend(lora_files2)
                else:
                    all_lora_files.append(lora_files2)
            
            # 3„Å§ÁõÆ„ÅÆLoRA„Éï„Ç°„Ç§„É´„ÇíÂá¶ÁêÜÔºàF1Áâà„Åß„ÇÇÂØæÂøúÔºâ
            if lora_files3 is not None:
                if isinstance(lora_files3, list):
                    all_lora_files.extend(lora_files3)
                else:
                    all_lora_files.append(lora_files3)
            
            # „Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Éï„Ç°„Ç§„É´„Åã„Çâ„Éë„Çπ„É™„Çπ„Éà„ÇíÁîüÊàê
            for lora_file in all_lora_files:
                if hasattr(lora_file, 'name'):
                    lora_paths.append(lora_file.name)
        
        # „Çπ„Ç±„Éº„É´ÂÄ§„ÇíËß£Êûê
        try:
            scales = [float(s.strip()) for s in lora_scales_text.split(',')]
        except:
            # Ëß£Êûê„Ç®„É©„Éº„ÅÆÂ†¥Âêà„ÅØ„Éá„Éï„Ç©„É´„ÉàÂÄ§„Çí‰ΩøÁî®
            scales = [0.8] * len(lora_paths)
            
        # „Çπ„Ç±„Éº„É´ÂÄ§„ÅÆÊï∞„ÇíË™øÊï¥
        if len(scales) < len(lora_paths):
            scales.extend([0.8] * (len(lora_paths) - len(scales)))
        elif len(scales) > len(lora_paths):
            scales = scales[:len(lora_paths)]
            
        # LoRA„Éï„Ç°„Ç§„É´ÊÉÖÂ†±„ÇíÂá∫Âäõ
        if len(lora_paths) == 1:
            # Âçò‰∏Ä„Éï„Ç°„Ç§„É´
            print(translate("LoRA„Éï„Ç°„Ç§„É´: {0}").format(os.path.basename(lora_paths[0])))
            print(translate("LoRAÈÅ©Áî®Âº∑Â∫¶: {0}").format(scales[0]))
        elif len(lora_paths) > 1:
            # Ë§áÊï∞„Éï„Ç°„Ç§„É´
            print(translate("LoRA„Éï„Ç°„Ç§„É´ (Ë§áÊï∞):"))
            for i, path in enumerate(lora_paths):
                print(translate("   - {0} („Çπ„Ç±„Éº„É´: {1})").format(os.path.basename(path), scales[i] if i < len(scales) else 0.8))
        else:
            # LoRA„Éï„Ç°„Ç§„É´„Å™„Åó
            print(translate("LoRA: ‰ΩøÁî®„Åó„Å™„ÅÑ"))

    # „Çª„ÇØ„Ç∑„Éß„É≥„Åî„Å®„ÅÆ„Ç≠„Éº„Éï„É¨„Éº„É†ÁîªÂÉè„ÅÆ‰ΩøÁî®Áä∂Ê≥Å„Çí„É≠„Ç∞„Å´Âá∫Âäõ
    valid_sections = []
    if section_settings is not None:
        # „É™„Çπ„Éà„Åß„Å™„ÅÑÂ†¥Âêà„ÅØÁ©∫„ÅÆ„É™„Çπ„Éà„Å®„Åó„Å¶Êâ±„ÅÜ
        if not isinstance(section_settings, list):
            print(translate("section_settings„Åå„É™„Çπ„ÉàÂûã„Åß„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇÁ©∫„ÅÆ„É™„Çπ„Éà„Å®„Åó„Å¶Êâ±„ÅÑ„Åæ„Åô„ÄÇ"))
            section_settings = []

        for i, sec_data in enumerate(section_settings):
            if sec_data and isinstance(sec_data, list) and len(sec_data) > 1 and sec_data[1] is not None:  # ÁîªÂÉè„ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà
                valid_sections.append(sec_data[0])

    if valid_sections:
        print(translate("‰ΩøÁî®„Åô„Çã„Ç≠„Éº„Éï„É¨„Éº„É†ÁîªÂÉè: „Çª„ÇØ„Ç∑„Éß„É≥{0}").format(', '.join(map(str, valid_sections))))
    else:
        print(translate("„Ç≠„Éº„Éï„É¨„Éº„É†ÁîªÂÉè: „Éá„Éï„Ç©„É´„ÉàË®≠ÂÆö„ÅÆ„Åø‰ΩøÁî®"))

    print("=============================")

    # „Éê„ÉÉ„ÉÅÂá¶ÁêÜ„ÅÆÂÖ®‰ΩìÂÅúÊ≠¢Áî®„Éï„É©„Ç∞
    batch_stopped = False

    # ÂÖÉ„ÅÆ„Ç∑„Éº„ÉâÂÄ§„Çí‰øùÂ≠òÔºà„Éê„ÉÉ„ÉÅÂá¶ÁêÜÁî®Ôºâ
    original_seed = seed
    
    # „É©„É≥„ÉÄ„É†„Ç∑„Éº„ÉâÁîüÊàê„ÇíÊñáÂ≠óÂàóÂûã„ÇÇÂê´„ÇÅ„Å¶Âà§ÂÆö
    use_random = False
    if isinstance(use_random_seed, bool):
        use_random = use_random_seed
    elif isinstance(use_random_seed, str):
        use_random = use_random_seed.lower() in ["true", "yes", "1", "on"]
    
    if use_random:
        # „É©„É≥„ÉÄ„É†„Ç∑„Éº„ÉâË®≠ÂÆöÂâç„ÅÆÂÄ§„Çí‰øùÂ≠ò
        previous_seed = seed
        # ÁâπÂÆö„ÅÆÁØÑÂõ≤ÂÜÖ„ÅßÊñ∞„Åó„ÅÑ„Ç∑„Éº„ÉâÂÄ§„ÇíÁîüÊàê
        seed = random.randint(0, 2**32 - 1)
        # „É¶„Éº„Ç∂„Éº„Å´„Çè„Åã„Çä„ÇÑ„Åô„ÅÑ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíË°®Á§∫
        print(translate("„É©„É≥„ÉÄ„É†„Ç∑„Éº„ÉâÊ©üËÉΩ„ÅåÊúâÂäπ„Å™„Åü„ÇÅ„ÄÅÊåáÂÆö„Åï„Çå„ÅüSEEDÂÄ§ {0} „ÅÆ‰ª£„Çè„Çä„Å´Êñ∞„Åó„ÅÑSEEDÂÄ§ {1} „Çí‰ΩøÁî®„Åó„Åæ„Åô„ÄÇ").format(previous_seed, seed))
        # UI„ÅÆseedÊ¨Ñ„ÇÇ„É©„É≥„ÉÄ„É†ÂÄ§„ÅßÊõ¥Êñ∞
        yield gr.skip(), None, '', '', gr.update(interactive=False), gr.update(interactive=True), gr.update(value=seed)
        # „É©„É≥„ÉÄ„É†„Ç∑„Éº„Éâ„ÅÆÂ†¥Âêà„ÅØÊúÄÂàù„ÅÆÂÄ§„ÇíÊõ¥Êñ∞
        original_seed = seed
    else:
        print(translate("ÊåáÂÆö„Åï„Çå„ÅüSEEDÂÄ§ {0} „Çí‰ΩøÁî®„Åó„Åæ„Åô„ÄÇ").format(seed))
        yield gr.skip(), None, '', '', gr.update(interactive=False), gr.update(interactive=True), gr.update()

    stream = AsyncStream()

    # stream‰ΩúÊàêÂæå„ÄÅ„Éê„ÉÉ„ÉÅÂá¶ÁêÜÂâç„ÇÇ„ÅÜ‰∏ÄÂ∫¶„Éï„É©„Ç∞„ÇíÁ¢∫Ë™ç
    if batch_stopped:
        print(translate("„Éê„ÉÉ„ÉÅÂá¶ÁêÜ„Åå‰∏≠Êñ≠„Åï„Çå„Åæ„Åó„ÅüÔºà„Éê„ÉÉ„ÉÅÈñãÂßãÂâçÔºâ"))
        yield (
            gr.skip(),
            gr.update(visible=False),
            translate("„Éê„ÉÉ„ÉÅÂá¶ÁêÜ„Åå‰∏≠Êñ≠„Åï„Çå„Åæ„Åó„Åü"),
            '',
            gr.update(interactive=True),
            gr.update(interactive=False, value=translate("End Generation")),
            gr.update()
        )
        return

    # „Éê„ÉÉ„ÉÅÂá¶ÁêÜ„É´„Éº„Éó„ÅÆÈñãÂßã
    if queue_enabled:
        if queue_type == "image":
            print(translate("„Éê„ÉÉ„ÉÅÂá¶ÁêÜÊÉÖÂ†±: ÂêàË®à{0}Âõû").format(batch_count))
            print(translate("„Ç§„É°„Éº„Ç∏„Ç≠„É•„Éº: ÊúâÂäπ, ÂÖ•ÂäõÁîªÂÉè1Êûö + ÁîªÂÉè„Éï„Ç°„Ç§„É´{0}Êûö").format(len(image_queue_files)))
            print(translate("Âá¶ÁêÜÈ†ÜÂ∫è: 1ÂõûÁõÆ=ÂÖ•ÂäõÁîªÂÉè, 2ÂõûÁõÆ‰ª•Èôç=ÂÖ•Âäõ„Éï„Ç©„É´„ÉÄ„ÅÆÁîªÂÉè„Éï„Ç°„Ç§„É´"))
            # „Éê„ÉÉ„ÉÅÂá¶ÁêÜ„ÇíÂº∑Ë™øË°®Á§∫
            for i in range(batch_count):
                if i == 0:
                    img_src = "ÂÖ•ÂäõÁîªÂÉè"
                else:
                    img_idx = i - 1
                    if img_idx < len(image_queue_files):
                        img_src = os.path.basename(image_queue_files[img_idx])
                    else:
                        img_src = "ÂÖ•ÂäõÁîªÂÉèÔºà„Ç≠„É•„ÉºÁîªÂÉè‰∏çË∂≥Ôºâ"
                print(translate("   ‚îî „Éê„ÉÉ„ÉÅ{0}: {1}").format(i+1, img_src))
        else:
            queue_lines_count = 0
            if prompt_queue_file_path and os.path.exists(prompt_queue_file_path):
                try:
                    with open(prompt_queue_file_path, 'r', encoding='utf-8') as f:
                        queue_lines = [line.strip() for line in f.readlines() if line.strip()]
                        queue_lines_count = len(queue_lines)
                        # ÂêÑ„Éó„É≠„É≥„Éó„Éà„ÅÆ„Éó„É¨„Éì„É•„Éº„ÇíË°®Á§∫
                        for i in range(min(batch_count, queue_lines_count)):
                            prompt_preview = queue_lines[i][:50] + "..." if len(queue_lines[i]) > 50 else queue_lines[i]
                            print(translate("   ‚îî „Éê„ÉÉ„ÉÅ{0}: {1}").format(i+1, prompt_preview))
                except:
                    pass
            print(translate("„Éê„ÉÉ„ÉÅÂá¶ÁêÜÊÉÖÂ†±: ÂêàË®à{0}Âõû").format(batch_count))
            print(translate("„Éó„É≠„É≥„Éó„Éà„Ç≠„É•„Éº: ÊúâÂäπ, „Éó„É≠„É≥„Éó„ÉàË°åÊï∞={0}Ë°å").format(queue_lines_count))
    else:
        print(translate("„Éê„ÉÉ„ÉÅÂá¶ÁêÜÊÉÖÂ†±: ÂêàË®à{0}Âõû").format(batch_count))
        print(translate("„Ç≠„É•„ÉºÊ©üËÉΩ: ÁÑ°Âäπ"))
    for batch_index in range(batch_count):
        # ÂÅúÊ≠¢„Éï„É©„Ç∞„ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅØÂÖ®„Éê„ÉÉ„ÉÅÂá¶ÁêÜ„Çí‰∏≠Ê≠¢
        if batch_stopped:
            print(translate("„Éê„ÉÉ„ÉÅÂá¶ÁêÜ„Åå„É¶„Éº„Ç∂„Éº„Å´„Çà„Å£„Å¶‰∏≠Ê≠¢„Åï„Çå„Åæ„Åó„Åü"))
            yield (
                gr.skip(),
                gr.update(visible=False),
                translate("„Éê„ÉÉ„ÉÅÂá¶ÁêÜ„Åå‰∏≠Ê≠¢„Åï„Çå„Åæ„Åó„Åü„ÄÇ"),
                '',
                gr.update(interactive=True),
                gr.update(interactive=False, value=translate("End Generation")),
                gr.update()
            )
            break

        # ADDED: Update batch progress for queue processing
        if is_queue_processing:
            current_batch_num = batch_index + 1
            update_batch_progress(current_batch_num, batch_count)


        # ÁèæÂú®„ÅÆ„Éê„ÉÉ„ÉÅÁï™Âè∑„ÇíË°®Á§∫
        if batch_count > 1:
            batch_info = translate("„Éê„ÉÉ„ÉÅÂá¶ÁêÜ: {0}/{1}").format(batch_index + 1, batch_count)
            print(f"{batch_info}")
            # UI„Å´„ÇÇ„Éê„ÉÉ„ÉÅÊÉÖÂ†±„ÇíË°®Á§∫
            yield gr.skip(), gr.update(visible=False), batch_info, "", gr.update(interactive=False), gr.update(interactive=True), gr.update()


        # ‰ªäÂõûÂá¶ÁêÜÁî®„ÅÆ„Éó„É≠„É≥„Éó„Éà„Å®„Ç§„É°„Éº„Ç∏„ÇíÂèñÂæóÔºà„Ç≠„É•„ÉºÊ©üËÉΩÂØæÂøúÔºâ
        current_prompt = prompt
        current_image = input_image
        
        # „Ç§„É°„Éº„Ç∏„Ç≠„É•„Éº„Åß„Ç´„Çπ„Çø„É†„Éó„É≠„É≥„Éó„Éà„Çí‰ΩøÁî®„Åó„Å¶„ÅÑ„Çã„Åã„Å©„ÅÜ„Åã„ÇíÁ¢∫Ë™çÔºà„É≠„Ç∞Âá∫ÂäõÁî®Ôºâ
        using_custom_prompt = False
        if queue_enabled and queue_type == "image" and batch_index > 0:
            if batch_index - 1 < len(image_queue_files):
                queue_img_path = image_queue_files[batch_index - 1]
                img_basename = os.path.splitext(queue_img_path)[0]
                txt_path = f"{img_basename}.txt"
                if os.path.exists(txt_path):
                    img_name = os.path.basename(queue_img_path)
                    using_custom_prompt = True
                    print(translate("„Çª„ÇØ„Ç∑„Éß„É≥{0}„ÅØ„Ç§„É°„Éº„Ç∏„Ç≠„É•„ÉºÁîªÂÉè„Äå{1}„Äç„ÅÆÂ∞ÇÁî®„Éó„É≠„É≥„Éó„Éà„Çí‰ΩøÁî®„Åó„Åæ„Åô").format("ÂÖ®„Å¶", img_name))

        # „Ç≠„É•„ÉºÊ©üËÉΩ„ÅÆÂá¶ÁêÜ
        if queue_enabled:
            if queue_type == "prompt" and prompt_queue_file_path is not None:
                # „Éó„É≠„É≥„Éó„Éà„Ç≠„É•„Éº„ÅÆÂá¶ÁêÜ
                if os.path.exists(prompt_queue_file_path):
                    try:
                        with open(prompt_queue_file_path, 'r', encoding='utf-8') as f:
                            lines = [line.strip() for line in f.readlines() if line.strip()]
                            if batch_index < len(lines):
                                # „Éó„É≠„É≥„Éó„Éà„Ç≠„É•„Éº„Åã„Çâ„Éó„É≠„É≥„Éó„Éà„ÇíÂèñÂæó
                                current_prompt = lines[batch_index]
                                print(translate("„Éó„É≠„É≥„Éó„Éà„Ç≠„É•„ÉºÂÆüË°å‰∏≠: „Éê„ÉÉ„ÉÅ {0}/{1}").format(batch_index+1, batch_count))
                                print(translate("  ‚îî „Éó„É≠„É≥„Éó„Éà: „Äå{0}...„Äç").format(current_prompt[:50]))
                            else:
                                print(translate("„Éó„É≠„É≥„Éó„Éà„Ç≠„É•„ÉºÂÆüË°å‰∏≠: „Éê„ÉÉ„ÉÅ {0}/{1} „ÅØ„Éó„É≠„É≥„Éó„ÉàË°åÊï∞„ÇíË∂Ö„Åà„Å¶„ÅÑ„Çã„Åü„ÇÅÂÖÉ„ÅÆ„Éó„É≠„É≥„Éó„Éà„Çí‰ΩøÁî®").format(batch_index+1, batch_count))
                    except Exception as e:
                        print(translate("„Éó„É≠„É≥„Éó„Éà„Ç≠„É•„Éº„Éï„Ç°„Ç§„É´Ë™≠„ÅøËæº„Åø„Ç®„É©„Éº: {0}").format(str(e)))

            elif queue_type == "image" and len(image_queue_files) > 0:
                # „Ç§„É°„Éº„Ç∏„Ç≠„É•„Éº„ÅÆÂá¶ÁêÜ
                # ÊúÄÂàù„ÅÆ„Éê„ÉÉ„ÉÅ„ÅØÂÖ•ÂäõÁîªÂÉè„Çí‰ΩøÁî®
                if batch_index == 0:
                    print(translate("„Ç§„É°„Éº„Ç∏„Ç≠„É•„ÉºÂÆüË°å‰∏≠: „Éê„ÉÉ„ÉÅ {0}/{1} „ÅØÂÖ•ÂäõÁîªÂÉè„Çí‰ΩøÁî®").format(batch_index+1, batch_count))
                elif batch_index > 0:
                    # 2ÂõûÁõÆ‰ª•Èôç„ÅØ„Ç§„É°„Éº„Ç∏„Ç≠„É•„Éº„ÅÆÁîªÂÉè„ÇíÈ†ÜÁï™„Å´‰ΩøÁî®
                    image_index = batch_index - 1  # 0ÂõûÁõÆÔºàÂÖ•ÂäõÁîªÂÉèÔºâ„ÅÆÂàÜ„ÇíÂºï„Åè

                    if image_index < len(image_queue_files):
                        current_image = image_queue_files[image_index]
                        image_filename = os.path.basename(current_image)
                        print(translate("„Ç§„É°„Éº„Ç∏„Ç≠„É•„ÉºÂÆüË°å‰∏≠: „Éê„ÉÉ„ÉÅ {0}/{1} „ÅÆÁîªÂÉè„Äå{2}„Äç").format(batch_index+1, batch_count, image_filename))
                        print(translate("  ‚îî ÁîªÂÉè„Éï„Ç°„Ç§„É´„Éë„Çπ: {0}").format(current_image))
                        
                        # ÂêåÂêç„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„Éï„Ç°„Ç§„É´„Åå„ÅÇ„Çã„ÅãÁ¢∫Ë™ç„Åó„ÄÅ„ÅÇ„Çå„Å∞ÂÜÖÂÆπ„Çí„Éó„É≠„É≥„Éó„Éà„Å®„Åó„Å¶‰ΩøÁî®
                        img_basename = os.path.splitext(current_image)[0]
                        txt_path = f"{img_basename}.txt"
                        if os.path.exists(txt_path):
                            try:
                                with open(txt_path, 'r', encoding='utf-8') as f:
                                    custom_prompt = f.read().strip()
                                if custom_prompt:
                                    print(translate("„Ç§„É°„Éº„Ç∏„Ç≠„É•„Éº: ÁîªÂÉè„Äå{0}„ÄçÁî®„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„Éï„Ç°„Ç§„É´„ÇíË™≠„ÅøËæº„Åø„Åæ„Åó„Åü").format(image_filename))
                                    print(translate("„Ç´„Çπ„Çø„É†„Éó„É≠„É≥„Éó„Éà: {0}").format(custom_prompt[:50] + "..." if len(custom_prompt) > 50 else custom_prompt))
                                    # „Ç´„Çπ„Çø„É†„Éó„É≠„É≥„Éó„Éà„ÇíË®≠ÂÆöÔºàcurrent_prompt„Çí‰∏äÊõ∏„ÅçÔºâ
                                    current_prompt = custom_prompt
                            except Exception as e:
                                print(translate("„Ç§„É°„Éº„Ç∏„Ç≠„É•„Éº: „ÉÜ„Ç≠„Çπ„Éà„Éï„Ç°„Ç§„É´Ë™≠„ÅøËæº„Åø„Ç®„É©„Éº: {0}").format(e))
                    else:
                        # ÁîªÂÉèÊï∞„ÅåË∂≥„Çä„Å™„ÅÑÂ†¥Âêà„ÅØÂÖ•ÂäõÁîªÂÉè„Å´Êàª„Çã
                        print(translate("„Ç§„É°„Éº„Ç∏„Ç≠„É•„ÉºÂÆüË°å‰∏≠: „Éê„ÉÉ„ÉÅ {0}/{1} „ÅØÁîªÂÉèÊï∞„ÇíË∂Ö„Åà„Å¶„ÅÑ„Çã„Åü„ÇÅÂÖ•ÂäõÁîªÂÉè„Çí‰ΩøÁî®").format(batch_index+1, batch_count))

        # „Éê„ÉÉ„ÉÅ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Å´Âøú„Åò„Å¶SEEDÂÄ§„ÇíË®≠ÂÆö
        current_seed = original_seed + batch_index
        if batch_count > 1:
            print(translate("ÂàùÊúüSEEDÂÄ§: {0}").format(current_seed))
        # ÁèæÂú®„ÅÆ„Éê„ÉÉ„ÉÅÁî®„ÅÆ„Ç∑„Éº„Éâ„ÇíË®≠ÂÆö
        seed = current_seed

        # „ÇÇ„ÅÜ‰∏ÄÂ∫¶ÂÅúÊ≠¢„Éï„É©„Ç∞„ÇíÁ¢∫Ë™ç - workerÂá¶ÁêÜÂÆüË°åÂâç
        if batch_stopped:
            print(translate("„Éê„ÉÉ„ÉÅÂá¶ÁêÜ„Åå‰∏≠Êñ≠„Åï„Çå„Åæ„Åó„Åü„ÄÇworkerÈñ¢Êï∞„ÅÆÂÆüË°å„Çí„Ç≠„É£„É≥„Çª„É´„Åó„Åæ„Åô„ÄÇ"))
            # ‰∏≠Êñ≠„É°„ÉÉ„Çª„Éº„Ç∏„ÇíUI„Å´Ë°®Á§∫
            yield (gr.skip(),
                   gr.update(visible=False),
                   translate("„Éê„ÉÉ„ÉÅÂá¶ÁêÜ„Åå‰∏≠Êñ≠„Åï„Çå„Åæ„Åó„ÅüÔºà{0}/{1}Ôºâ").format(batch_index, batch_count),
                   '',
                   gr.update(interactive=True),
                   gr.update(interactive=False, value=translate("End Generation")),
                   gr.update())
            break

        # GPU„É°„É¢„É™„ÅÆË®≠ÂÆöÂÄ§„ÇíÂá∫Âäõ„Åó„ÄÅÊ≠£„Åó„ÅÑÂûã„Å´Â§âÊèõ
        gpu_memory_value = float(gpu_memory_preservation) if gpu_memory_preservation is not None else 6.0
        print(translate('Using GPU memory preservation setting: {0} GB').format(gpu_memory_value))

        # Âá∫Âäõ„Éï„Ç©„É´„ÉÄ„ÅåÁ©∫„ÅÆÂ†¥Âêà„ÅØ„Éá„Éï„Ç©„É´„ÉàÂÄ§„Çí‰ΩøÁî®
        if not output_dir or not output_dir.strip():
            output_dir = "outputs"
        print(translate('Output directory: {0}').format(output_dir))

        # Gradio„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Åã„ÇâÂÆüÈöõ„ÅÆÂÄ§„ÇíÂèñÂæó
        if hasattr(frame_save_mode, 'value'):
            frame_save_mode_actual = frame_save_mode.value
        else:
            frame_save_mode_actual = frame_save_mode
            
        print(translate("ÁèæÂú®„ÅÆ„Éê„ÉÉ„ÉÅ: {0}/{1}, ÁîªÂÉè: {2}").format(
            batch_index + 1,
            batch_count,
            os.path.basename(current_image) if isinstance(current_image, str) else "ÂÖ•ÂäõÁîªÂÉè"
        ))

        # „Ç≠„É•„ÉºÊ©üËÉΩ‰ΩøÁî®ÊôÇ„ÅÆÁèæÂú®„ÅÆ„Éó„É≠„É≥„Éó„Éà„Å®„Ç§„É°„Éº„Ç∏„Åß„ÉØ„Éº„Ç´„Éº„ÇíÂÆüË°å
        async_run(
            worker,
            current_image,  # „Ç≠„É•„ÉºÊ©üËÉΩ„ÅßÈÅ∏Êäû„Åï„Çå„ÅüÁîªÂÉè
            current_prompt,  # „Ç≠„É•„ÉºÊ©üËÉΩ„ÅßÈÅ∏Êäû„Åï„Çå„Åü„Éó„É≠„É≥„Éó„Éà
            n_prompt,
            seed,
            total_second_length,
            latent_window_size,
            steps,
            cfg,
            gs,
            rs,
            gpu_memory_value,
            use_teacache,
            mp4_crf,
            all_padding_value,
            image_strength,
            keep_section_videos,
            lora_files,
            lora_files2,
            lora_files3,  # ËøΩÂä†: lora_files3
            lora_scales_text,
            output_dir,
            save_section_frames,
            use_all_padding,
            use_lora,
            lora_mode,  # ËøΩÂä†: lora_mode
            lora_dropdown1,  # ËøΩÂä†: lora_dropdown1
            lora_dropdown2,  # ËøΩÂä†: lora_dropdown2
            lora_dropdown3,  # ËøΩÂä†: lora_dropdown3
            save_tensor_data,
            tensor_data_input,
            fp8_optimization,
            resolution,
            batch_index,
            frame_save_mode_actual
        )

        # ÁèæÂú®„ÅÆ„Éê„ÉÉ„ÉÅ„ÅÆÂá∫Âäõ„Éï„Ç°„Ç§„É´Âêç
        batch_output_filename = None

        # ÁèæÂú®„ÅÆ„Éê„ÉÉ„ÉÅ„ÅÆÂá¶ÁêÜÁµêÊûú„ÇíÂèñÂæó
        while True:
            flag, data = stream.output_queue.next()

            if flag == 'file':
                batch_output_filename = data
                # „Çà„ÇäÊòéÁ¢∫„Å™Êõ¥Êñ∞ÊñπÊ≥ï„Çí‰ΩøÁî®„Åó„ÄÅpreview_image„ÇíÊòéÁ§∫ÁöÑ„Å´„ÇØ„É™„Ç¢
                yield (
                    batch_output_filename if batch_output_filename is not None else gr.skip(), 
                    gr.update(value=None, visible=False), 
                    gr.update(), 
                    gr.update(), 
                    gr.update(interactive=False), 
                    gr.update(interactive=True), 
                    gr.update(),
                )

            if flag == 'progress':
                preview, desc, html = data
                # „Éê„ÉÉ„ÉÅÂá¶ÁêÜ‰∏≠„ÅØÁèæÂú®„ÅÆ„Éê„ÉÉ„ÉÅÊÉÖÂ†±„ÇíËøΩÂä†
                if batch_count > 1:
                    batch_info = translate("„Éê„ÉÉ„ÉÅÂá¶ÁêÜ: {0}/{1} - ").format(batch_index + 1, batch_count)
                    desc = batch_info + desc
                # preview_image„ÇíÊòéÁ§∫ÁöÑ„Å´Ë®≠ÂÆö
                yield gr.skip(), gr.update(visible=True, value=preview), desc, html, gr.update(interactive=False), gr.update(interactive=True), gr.update()

            if flag == 'end':


                # ADDED: Log batch completion for queue processing
                if is_queue_processing:
                    print(translate("üìä Batch {0}/{1} completed for queue processing").format(batch_index + 1, batch_count))

                # „Åì„ÅÆ„Éê„ÉÉ„ÉÅ„ÅÆÂá¶ÁêÜ„ÅåÁµÇ‰∫Ü
                if batch_index == batch_count - 1 or batch_stopped:
                    # ÊúÄÁµÇ„Éê„ÉÉ„ÉÅ„ÅÆÂ†¥Âêà„ÅØÂá¶ÁêÜÂÆå‰∫Ü„ÇíÈÄöÁü•
                    completion_message = ""
                    if batch_stopped:
                        completion_message = translate("„Éê„ÉÉ„ÉÅÂá¶ÁêÜ„Åå‰∏≠Ê≠¢„Åï„Çå„Åæ„Åó„ÅüÔºà{0}/{1}Ôºâ").format(batch_index + 1, batch_count)
                    else:
                        completion_message = translate("„Éê„ÉÉ„ÉÅÂá¶ÁêÜ„ÅåÂÆå‰∫Ü„Åó„Åæ„Åó„ÅüÔºà{0}/{1}Ôºâ").format(batch_count, batch_count)


                    # ADDED: Reset batch progress when all batches complete (for queue processing)
                    if is_queue_processing:
                        print(translate("üìä All batches completed - resetting batch progress"))
                        # Don't reset here, let the queue manager handle it when config is fully done

                    yield (
                        batch_output_filename if batch_output_filename is not None else gr.skip(),
                        gr.update(value=None, visible=False),
                        completion_message,
                        '',
                        gr.update(interactive=True),
                        gr.update(interactive=False, value=translate("End Generation")),
                        gr.update()
                    )
                    # ÊúÄÂæå„ÅÆ„Éê„ÉÉ„ÉÅ„ÅåÁµÇ„Çè„Å£„Åü„ÅÆ„ÅßÁµÇ‰∫Ü
                    print(translate("„Éê„ÉÉ„ÉÅ„Ç∑„Éº„Ç±„É≥„ÇπÂÆå‰∫Ü: ÂÖ® {0} „Éê„ÉÉ„ÉÅ„ÅÆÂá¶ÁêÜ„ÇíÁµÇ‰∫Ü").format(batch_count))
                else:
                    # Ê¨°„ÅÆ„Éê„ÉÉ„ÉÅ„Å´ÈÄ≤„ÇÄ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíË°®Á§∫
                    next_batch_message = translate("„Éê„ÉÉ„ÉÅÂá¶ÁêÜ: {0}/{1} ÂÆå‰∫Ü„ÄÅÊ¨°„ÅÆ„Éê„ÉÉ„ÉÅ„Å´ÈÄ≤„Åø„Åæ„Åô...").format(batch_index + 1, batch_count)
                    print(translate("„Éê„ÉÉ„ÉÅ {0}/{1} ÂÆå‰∫Ü - Ê¨°„ÅÆ„Éê„ÉÉ„ÉÅ„Å´ÈÄ≤„Åø„Åæ„Åô").format(batch_index + 1, batch_count))
                    yield (
                        batch_output_filename if batch_output_filename is not None else gr.skip(),
                        gr.update(value=None, visible=False),
                        next_batch_message,
                        '',
                        gr.update(interactive=False),
                        gr.update(interactive=True),
                        gr.update()
                    )
                    # „Éê„ÉÉ„ÉÅ„É´„Éº„Éó„ÅÆÂÜÖÂÅ¥„Åß‰ΩøÁî®„Åï„Çå„ÇãÂ§âÊï∞„ÇíÊ¨°„ÅÆ„Éê„ÉÉ„ÉÅÁî®„Å´Êõ¥Êñ∞„Åô„Çã
                    continue_next_batch = True
                break

        # ÊúÄÁµÇÁöÑ„Å™Âá∫Âäõ„Éï„Ç°„Ç§„É´Âêç„ÇíÊõ¥Êñ∞
        output_filename = batch_output_filename

        # „Éê„ÉÉ„ÉÅÂá¶ÁêÜ„ÅåÂÅúÊ≠¢„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅØ„É´„Éº„Éó„ÇíÊäú„Åë„Çã
        if batch_stopped:
            print(translate("„Éê„ÉÉ„ÉÅÂá¶ÁêÜ„É´„Éº„Éó„Çí‰∏≠Êñ≠„Åó„Åæ„Åô"))
            break
  

# Êó¢Â≠ò„ÅÆQuick PromptsÔºàÂàùÊúüÂåñÊôÇ„Å´„Éó„É™„Çª„ÉÉ„Éà„Å´Â§âÊèõ„Åï„Çå„Çã„ÅÆ„Åß„ÄÅ‰∫íÊèõÊÄß„ÅÆ„Åü„ÇÅ„Å´ÊÆã„ÅôÔºâ
quick_prompts = [
    'A character doing some simple body movements.',
    'A character uses expressive hand gestures and body language.',
    'A character walks leisurely with relaxed movements.',
    'A character performs dynamic movements with energy and flowing motion.',
    'A character moves in unexpected ways, with surprising transitions poses.',
]
quick_prompts = [[x] for x in quick_prompts]

css = get_app_css()
block = gr.Blocks(css=css).queue()
with block:
    gr.HTML('<h1>FramePack<span class="title-suffix">-<s>eichi</s> F1</span></h1>')

    # ‰∏ÄÁï™‰∏ä„ÅÆË°å„Å´„ÄåÁîüÊàê„É¢„Éº„Éâ„ÄÅ„Çª„ÇØ„Ç∑„Éß„É≥„Éï„É¨„Éº„É†„Çµ„Ç§„Ç∫„ÄÅ„Ç™„Éº„É´„Éë„Éá„Ç£„É≥„Ç∞„ÄÅÂãïÁîªÈï∑„Äç„ÇíÈÖçÁΩÆ
    with gr.Row():
        with gr.Column(scale=1):
            # ÁîüÊàê„É¢„Éº„Éâ„ÅÆ„É©„Ç∏„Ç™„Éú„Çø„É≥ÔºàF1„É¢„Éº„Éâ„Åß„ÅØÈÄöÂ∏∏„ÅÆ„ÅøÔºâ
            mode_radio = gr.Radio(choices=[MODE_TYPE_NORMAL], value=MODE_TYPE_NORMAL, label=translate("ÁîüÊàê„É¢„Éº„Éâ"), info=translate("F1„É¢„Éº„Éâ„Åß„ÅØÈÄöÂ∏∏„ÅÆ„ÅøÂà©Áî®ÂèØËÉΩ"))
        with gr.Column(scale=1):
            # „Éï„É¨„Éº„É†„Çµ„Ç§„Ç∫ÂàáÊõøÁî®„ÅÆUI„Ç≥„É≥„Éà„É≠„Éº„É´ÔºàÂêçÂâç„Çí„Äå„Çª„ÇØ„Ç∑„Éß„É≥„Éï„É¨„Éº„É†„Çµ„Ç§„Ç∫„Äç„Å´Â§âÊõ¥Ôºâ
            frame_size_radio = gr.Radio(
                choices=[translate("1Áßí (33„Éï„É¨„Éº„É†)"), translate("0.5Áßí (17„Éï„É¨„Éº„É†)")],
                value=translate("1Áßí (33„Éï„É¨„Éº„É†)"),
                label=translate("„Çª„ÇØ„Ç∑„Éß„É≥„Éï„É¨„Éº„É†„Çµ„Ç§„Ç∫"),
                info=translate("1Áßí = È´òÂìÅË≥™„ÉªÈÄöÂ∏∏ÈÄüÂ∫¶ / 0.5Áßí = „Çà„Çä„Å™„ÇÅ„Çâ„Åã„Å™Âãï„ÅçÔºàÂÆüÈ®ìÁöÑÊ©üËÉΩÔºâ")
            )
        with gr.Column(scale=1):
            # „Ç™„Éº„É´„Éë„Éá„Ç£„É≥„Ç∞Ë®≠ÂÆö (F1„É¢„Éº„Éâ„Åß„ÅØÁÑ°ÂäπÂåñ)
            use_all_padding = gr.Checkbox(
                label=translate("„Ç™„Éº„É´„Éë„Éá„Ç£„É≥„Ç∞"),
                value=False,
                info=translate("F1„É¢„Éº„Éâ„Åß„ÅØ‰ΩøÁî®„Åß„Åç„Åæ„Åõ„Çì„ÄÇÁÑ°Âç∞„É¢„Éº„Éâ„Åß„ÅÆ„ÅøÊúâÂäπ„Åß„Åô„ÄÇ"),
                elem_id="all_padding_checkbox",
                interactive=False  # F1„É¢„Éº„Éâ„Åß„ÅØÈùûÊ¥ªÊÄßÂåñ
            )
            all_padding_value = gr.Slider(
                label=translate("„Éë„Éá„Ç£„É≥„Ç∞ÂÄ§"),
                minimum=0.2,
                maximum=3,
                value=1,
                step=0.1,
                info=translate("F1„É¢„Éº„Éâ„Åß„ÅØ‰ΩøÁî®„Åß„Åç„Åæ„Åõ„Çì"),
                visible=False,
                interactive=False  # F1„É¢„Éº„Éâ„Åß„ÅØÈùûÊ¥ªÊÄßÂåñ
            )

            # „Ç™„Éº„É´„Éë„Éá„Ç£„É≥„Ç∞„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„ÇπÁä∂ÊÖã„Å´Âøú„Åò„Å¶„Çπ„É©„Ç§„ÉÄ„Éº„ÅÆË°®Á§∫/ÈùûË°®Á§∫„ÇíÂàá„ÇäÊõø„Åà„Çã
            def toggle_all_padding_visibility(use_all_padding):
                return gr.update(visible=use_all_padding)

            use_all_padding.change(
                fn=toggle_all_padding_visibility,
                inputs=[use_all_padding],
                outputs=[all_padding_value]
            )
        with gr.Column(scale=1):
            # Ë®≠ÂÆö„Åã„ÇâÂãïÁöÑ„Å´ÈÅ∏ÊäûËÇ¢„ÇíÁîüÊàê
            length_radio = gr.Radio(choices=get_video_modes(), value=translate("1Áßí"), label=translate("ÂãïÁîªÈï∑"), info=translate("ÂãïÁîª„ÅÆÈï∑„Åï„ÇíË®≠ÂÆö„ÄÇF1„É¢„Éº„Éâ„Åß„ÅØÂè≥‰∏ã„ÅÆ„ÄåÂãïÁîª„ÅÆÁ∑èÈï∑ÔºàÁßíÔºâ„Äç„Åß20Áßí„Çà„ÇäÈï∑„ÅÑÂãïÁîªÈï∑„ÇíË®≠ÂÆöÂèØËÉΩ„Åß„Åô"))

    with gr.Row():
        with gr.Column():
            input_image = gr.Image(sources=['upload', 'clipboard'], type="filepath", label="Image", height=320)

            # „ÉÜ„É≥„ÇΩ„É´„Éá„Éº„ÇøË®≠ÂÆö„Çí„Ç∞„É´„Éº„ÉóÂåñ„Åó„Å¶ÁÅ∞Ëâ≤„ÅÆ„Çø„Ç§„Éà„É´„Éê„Éº„Å´Â§âÊõ¥
            with gr.Group():
                gr.Markdown(f"### " + translate("„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„ÇøË®≠ÂÆö"))

                # „ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø‰ΩøÁî®ÊúâÁÑ°„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ
                use_tensor_data = gr.Checkbox(label=translate("„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„Çí‰ΩøÁî®„Åô„Çã"), value=False, info=translate("„ÉÅ„Çß„ÉÉ„ÇØ„Çí„Ç™„É≥„Å´„Åô„Çã„Å®„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åß„Åç„Åæ„Åô"))

                # „ÉÜ„É≥„ÇΩ„É´„Éá„Éº„ÇøË®≠ÂÆö„Ç≥„É≥„Éù„Éº„Éç„É≥„ÉàÔºàÂàùÊúüÁä∂ÊÖã„Åß„ÅØÈùûË°®Á§∫Ôºâ
                with gr.Group(visible=False) as tensor_data_group:
                    tensor_data_input = gr.File(
                        label=translate("„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ (.safetensors) - ÁîüÊàêÂãïÁîª„ÅÆÂæåÊñπ(Êú´Â∞æ)„Å´ÁµêÂêà„Åï„Çå„Åæ„Åô"),
                        file_types=[".safetensors"]
                    )

                    gr.Markdown(translate("‚Äª „ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åô„Çã„Å®ÈÄöÂ∏∏„ÅÆÂãïÁîªÁîüÊàêÂæå„Å´„ÄÅ„Åù„ÅÆÂãïÁîª„ÅÆÂæåÊñπÔºàÊú´Â∞æÔºâ„Å´ÁµêÂêà„Åï„Çå„Åæ„Åô„ÄÇ\nÁµêÂêà„Åó„ÅüÂãïÁîª„ÅØ„ÄåÂÖÉ„ÅÆ„Éï„Ç°„Ç§„É´Âêç_combined.mp4„Äç„Å®„Åó„Å¶‰øùÂ≠ò„Åï„Çå„Åæ„Åô„ÄÇ\n‚Äª „ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„ÅÆ‰øùÂ≠òÊ©üËÉΩ„ÇíÊúâÂäπ„Å´„Åô„Çã„Å®„ÄÅÁîüÊàê„Å®„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„ÅÆ„ÉÜ„É≥„ÇΩ„É´„ÇíÁµêÂêà„Åó„Åü„Éá„Éº„Çø„ÇÇ‰øùÂ≠ò„Åï„Çå„Åæ„Åô„ÄÇ\n‚Äª „ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„ÅÆÁµêÂêà„ÅØÂà•„ÉÑ„Éº„É´ `python eichi_utils/tensor_combiner.py --ui` „Åß„ÇÇ„Åß„Åç„Åæ„Åô„ÄÇ"))

                # „ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ„ÅÆÁä∂ÊÖã„Å´„Çà„Å£„Å¶„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„ÇøË®≠ÂÆö„ÅÆË°®Á§∫/ÈùûË°®Á§∫„ÇíÂàá„ÇäÊõø„Åà„ÇãÈñ¢Êï∞
                def toggle_tensor_data_settings(use_tensor):
                    return gr.update(visible=use_tensor)

                # „ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ„ÅÆÂ§âÊõ¥„Ç§„Éô„É≥„Éà„Å´Èñ¢Êï∞„ÇíÁ¥ê„Å•„Åë
                use_tensor_data.change(
                    fn=toggle_tensor_data_settings,
                    inputs=[use_tensor_data],
                    outputs=[tensor_data_group]
                )

                # „Ç≠„É•„ÉºÊ©üËÉΩ„ÅÆ„Éà„Ç∞„É´Èñ¢Êï∞
                def toggle_queue_settings(use_queue_val):
                    # „Ç∞„É≠„Éº„Éê„É´Â§âÊï∞„Çí‰ΩøÁî®
                    global queue_enabled, queue_type

                    # „ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ„ÅÆÂÄ§„Çí„Éñ„Éº„É´ÂÄ§„Å´Á¢∫ÂÆü„Å´Â§âÊèõ
                    is_enabled = False

                    # Gradio„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„ÅÆÂ†¥Âêà
                    if hasattr(use_queue_val, 'value'):
                        is_enabled = bool(use_queue_val.value)
                    # „Éñ„Éº„É´ÂÄ§„ÅÆÂ†¥Âêà
                    elif isinstance(use_queue_val, bool):
                        is_enabled = use_queue_val
                    # ÊñáÂ≠óÂàó„ÅÆÂ†¥Âêà (True/False„ÇíË°®„ÅôÊñáÂ≠óÂàó„Åã„ÉÅ„Çß„ÉÉ„ÇØ)
                    elif isinstance(use_queue_val, str) and use_queue_val.lower() in ('true', 'false', 't', 'f', 'yes', 'no', 'y', 'n', '1', '0'):
                        is_enabled = use_queue_val.lower() in ('true', 't', 'yes', 'y', '1')

                    # „Ç∞„É≠„Éº„Éê„É´Â§âÊï∞„Å´‰øùÂ≠ò
                    queue_enabled = is_enabled

                    print(translate("„Éà„Ç∞„É´Èñ¢Êï∞: „ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ„ÅÆÂûã={0}, ÂÄ§={1}").format(type(use_queue_val).__name__, use_queue_val))
                    print(translate("„Ç≠„É•„ÉºË®≠ÂÆö„ÅÆË°®Á§∫Áä∂ÊÖã„ÇíÂ§âÊõ¥: {0} („Ç∞„É≠„Éº„Éê„É´Â§âÊï∞„Å´‰øùÂ≠ò: queue_enabled={1})").format(is_enabled, queue_enabled))

                    # „Ç≠„É•„Éº„Çø„Ç§„Éó„Å´Âøú„Åò„Å¶ÈÅ©Âàá„Å™„Ç∞„É´„Éº„Éó„ÇíË°®Á§∫/ÈùûË°®Á§∫
                    if is_enabled:
                        if queue_type == "prompt":
                            return [gr.update(visible=True), gr.update(visible=True), gr.update(visible=False)]
                        else:  # image
                            return [gr.update(visible=True), gr.update(visible=False), gr.update(visible=True)]
                    else:
                        # „ÉÅ„Çß„ÉÉ„ÇØ„Åå„Ç™„Éï„Å™„ÇâÂÖ®„Å¶ÈùûË°®Á§∫
                        return [gr.update(visible=False), gr.update(visible=False), gr.update(visible=False)]

                # „Ç≠„É•„Éº„Çø„Ç§„Éó„ÅÆÂàá„ÇäÊõø„ÅàÈñ¢Êï∞
                def toggle_queue_type(queue_type_val):
                    global queue_type

                    # „Ç≠„É•„Éº„Çø„Ç§„Éó„Çí„Ç∞„É≠„Éº„Éê„É´Â§âÊï∞„Å´‰øùÂ≠ò
                    if queue_type_val == translate("„Éó„É≠„É≥„Éó„Éà„Ç≠„É•„Éº"):
                        queue_type = "prompt"
                        return [gr.update(visible=True), gr.update(visible=False)]
                    else:
                        queue_type = "image"
                        # „Ç§„É°„Éº„Ç∏„Ç≠„É•„Éº„ÇíÈÅ∏Êäû„Åó„ÅüÂ†¥Âêà„ÄÅÁîªÂÉè„Éï„Ç°„Ç§„É´„É™„Çπ„Éà„ÇíÊõ¥Êñ∞
                        get_image_queue_files()
                        return [gr.update(visible=False), gr.update(visible=True)]

                # Config Queue System„ÅÆË°®Á§∫/ÈùûË°®Á§∫„ÇíÂà∂Âæ°„Åô„ÇãÈñ¢Êï∞
                def toggle_config_queue(use_config_queue_val):
                    """Config Queue System„ÅÆË°®Á§∫/ÈùûË°®Á§∫„ÇíÂàá„ÇäÊõø„Åà„Çã"""
                    is_enabled = bool(use_config_queue_val)
                    print(translate("Config Queue System Ë°®Á§∫Ë®≠ÂÆö: {0}").format(is_enabled))
                    return gr.update(visible=is_enabled)

                # „Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„ÉâÂá¶ÁêÜÈñ¢Êï∞
                def handle_file_upload(file_obj):
                    global prompt_queue_file_path

                    if file_obj is not None:
                        print(translate("„Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„ÉâÊ§úÂá∫: Âûã={0}").format(type(file_obj).__name__))

                        if hasattr(file_obj, 'name'):
                            prompt_queue_file_path = file_obj.name
                            print(translate("„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Éï„Ç°„Ç§„É´„Éë„Çπ‰øùÂ≠ò: {0}").format(prompt_queue_file_path))
                        else:
                            prompt_queue_file_path = file_obj
                            print(translate("„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Éï„Ç°„Ç§„É´„Éá„Éº„Çø‰øùÂ≠ò: {0}").format(file_obj))
                    else:
                        prompt_queue_file_path = None
                        print(translate("„Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„ÉâËß£Èô§"))

                    return file_obj

                # ÂÖ•Âäõ„Éï„Ç©„É´„ÉÄÂêçÂ§âÊõ¥„Éè„É≥„Éâ„É©Ôºà„Éï„Ç©„É´„ÉÄ‰ΩúÊàê„ÇíË°å„Çè„Å™„ÅÑË®≠Ë®àÔºâ
                def handle_input_folder_change(folder_name):
                    """ÂÖ•Âäõ„Éï„Ç©„É´„ÉÄÂêç„ÅåÂ§âÊõ¥„Åï„Çå„Åü„Å®„Åç„ÅÆÂá¶ÁêÜÔºà„Ç∞„É≠„Éº„Éê„É´Â§âÊï∞„Å´‰øùÂ≠ò„Åô„Çã„Å†„ÅëÔºâ"""
                    global input_folder_name_value

                    # ÂÖ•ÂäõÂÄ§„Çí„Éà„É™„Éü„É≥„Ç∞
                    folder_name = folder_name.strip()

                    # Á©∫„ÅÆÂ†¥Âêà„ÅØ„Éá„Éï„Ç©„É´„ÉàÂÄ§„Å´Êàª„Åô
                    if not folder_name:
                        folder_name = "inputs"

                    # ÁÑ°Âäπ„Å™ÊñáÂ≠ó„ÇíÂâäÈô§Ôºà„Éë„ÇπÂå∫Âàá„ÇäÊñáÂ≠ó„ÇÑ„Éï„Ç°„Ç§„É´Âêç„Å´‰Ωø„Åà„Å™„ÅÑÊñáÂ≠óÔºâ
                    folder_name = ''.join(c for c in folder_name if c.isalnum() or c in ('_', '-'))

                    # „Ç∞„É≠„Éº„Éê„É´Â§âÊï∞„ÇíÊõ¥Êñ∞ÔºàË®≠ÂÆö„ÅÆ‰øùÂ≠ò„ÅØË°å„Çè„Å™„ÅÑÔºâ
                    input_folder_name_value = folder_name
                    print(translate("ÂÖ•Âäõ„Éï„Ç©„É´„ÉÄÂêç„Çí„É°„É¢„É™„Å´‰øùÂ≠ò: {0}Ôºà‰øùÂ≠òÂèä„Å≥ÂÖ•Âäõ„Éï„Ç©„É´„ÉÄ„ÇíÈñã„Åè„Éú„Çø„É≥„ÇíÊäº„Åô„Å®‰øùÂ≠ò„Åï„Çå„Åæ„ÅôÔºâ").format(folder_name))

                    # UI„ÅÆË°®Á§∫„ÇíÊõ¥Êñ∞
                    return gr.update(value=folder_name)

                # ÂÖ•Âäõ„Éï„Ç©„É´„ÉÄ„ÇíÈñã„Åè„Éú„Çø„É≥„Éè„É≥„Éâ„É©ÔºàË®≠ÂÆö‰øùÂ≠ò„Å®„Éï„Ç©„É´„ÉÄ‰ΩúÊàê„ÇíË°å„ÅÜÔºâ
                def open_input_folder():
                    """ÂÖ•Âäõ„Éï„Ç©„É´„ÉÄ„ÇíÈñã„ÅèÂá¶ÁêÜÔºà‰øùÂ≠ò„ÇÇÂÆüË°åÔºâ"""
                    global input_folder_name_value

                    # Ë®≠ÂÆö„Çí‰øùÂ≠ò
                    settings = load_settings()
                    settings['input_folder'] = input_folder_name_value
                    save_settings(settings)
                    print(translate("ÂÖ•Âäõ„Éï„Ç©„É´„ÉÄË®≠ÂÆö„Çí‰øùÂ≠ò„Åó„Åæ„Åó„Åü: {0}").format(input_folder_name_value))

                    # ÂÖ•Âäõ„Éï„Ç©„É´„ÉÄ„ÅÆ„Éë„Çπ„ÇíÂèñÂæó
                    input_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), input_folder_name_value)

                    # „Éï„Ç©„É´„ÉÄ„ÅåÂ≠òÂú®„Åó„Å™„Åë„Çå„Å∞‰ΩúÊàê
                    if not os.path.exists(input_dir):
                        os.makedirs(input_dir, exist_ok=True)
                        print(translate("ÂÖ•Âäõ„Éá„Ç£„É¨„ÇØ„Éà„É™„Çí‰ΩúÊàê„Åó„Åæ„Åó„Åü: {0}").format(input_dir))

                    # ÁîªÂÉè„Éï„Ç°„Ç§„É´„É™„Çπ„Éà„ÇíÊõ¥Êñ∞
                    get_image_queue_files()

                    # „Éó„É©„ÉÉ„Éà„Éï„Ç©„Éº„É†„Å´Âøú„Åò„Å¶„Éï„Ç©„É´„ÉÄ„ÇíÈñã„Åè
                    try:
                        if os.name == 'nt':  # Windows
                            os.startfile(input_dir)
                        elif os.name == 'posix':  # macOS, Linux
                            if sys.platform == 'darwin':  # macOS
                                subprocess.Popen(['open', input_dir])
                            else:  # Linux
                                subprocess.Popen(['xdg-open', input_dir])
                        print(translate("ÂÖ•Âäõ„Éï„Ç©„É´„ÉÄ„ÇíÈñã„Åç„Åæ„Åó„Åü: {0}").format(input_dir))
                        return translate("Ë®≠ÂÆö„Çí‰øùÂ≠ò„Åó„ÄÅÂÖ•Âäõ„Éï„Ç©„É´„ÉÄ„ÇíÈñã„Åç„Åæ„Åó„Åü")
                    except Exception as e:
                        error_msg = translate("„Éï„Ç©„É´„ÉÄ„ÇíÈñã„Åë„Åæ„Åõ„Çì„Åß„Åó„Åü: {0}").format(str(e))
                        print(error_msg)
                        return error_msg

            # „ÉÜ„É≥„ÇΩ„É´„Éá„Éº„ÇøË®≠ÂÆö„ÅÆ‰∏ã„Å´Ëß£ÂÉèÂ∫¶„Çπ„É©„Ç§„ÉÄ„Éº„Å®„Éê„ÉÉ„ÉÅÂá¶ÁêÜÂõûÊï∞„ÇíËøΩÂä†
            with gr.Group():
                with gr.Row():
                    with gr.Column(scale=2):
                        resolution = gr.Dropdown(
                            label=translate("Ëß£ÂÉèÂ∫¶"),
                            choices=[512, 640, 768, 960, 1080],
                            value=saved_app_settings.get("resolution", 640) if saved_app_settings else 640,
                            info=translate("Âá∫ÂäõÂãïÁîª„ÅÆÂü∫Ê∫ñËß£ÂÉèÂ∫¶„ÄÇ640Êé®Â•®„ÄÇ960/1080„ÅØÈ´òË≤†Ëç∑„ÉªÈ´ò„É°„É¢„É™Ê∂àË≤ª"),
                            elem_classes="saveable-setting"
                        )
                    with gr.Column(scale=1):
                        batch_count = gr.Slider(
                            label=translate("„Éê„ÉÉ„ÉÅÂá¶ÁêÜÂõûÊï∞"),
                            minimum=1,
                            maximum=100,
                            value=1,
                            step=1,
                            info=translate("Âêå„ÅòË®≠ÂÆö„ÅßÈÄ£Á∂öÁîüÊàê„Åô„ÇãÂõûÊï∞„ÄÇSEED„ÅØÂêÑÂõû„Åß+1„Åï„Çå„Åæ„Åô")
                        )

                # „Ç≠„É•„ÉºÊ©üËÉΩ„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ
                use_queue = gr.Checkbox(
                    label=translate("„Ç≠„É•„Éº„Çí‰ΩøÁî®"),
                    value=False,
                    info=translate("„ÉÅ„Çß„ÉÉ„ÇØ„Çí„Ç™„É≥„Å´„Åô„Çã„Å®„Éó„É≠„É≥„Éó„Éà„Åæ„Åü„ÅØÁîªÂÉè„ÅÆÈÄ£Á∂öÂá¶ÁêÜ„Åå„Åß„Åç„Åæ„Åô„ÄÇ")
                )

                # Config Queue SystemÂà∂Âæ°Áî®„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ
                use_config_queue = gr.Checkbox(
                    label=translate("Config Queue System „Çí‰ΩøÁî®ÔºàÂÆüÈ®ìÁöÑÊ©üËÉΩÔºâ"),
                    value=False,
                    info=translate("„ÉÅ„Çß„ÉÉ„ÇØ„Çí„Ç™„É≥„Å´„Åô„Çã„Å®Êñ∞„Åó„ÅÑConfig Queue System„ÅåÂà©Áî®„Åß„Åç„Åæ„Åô„ÄÇ")
                )

                # Config Queue System„Çí„Ç∞„É´„Éº„Éó„ÅßÂõ≤„Çì„ÅßË°®Á§∫Âà∂Âæ°
                with gr.Group(visible=False) as config_queue_group:
                    config_queue_components = create_enhanced_config_queue_ui()
                    queue_start_button = config_queue_components['enhanced_start_queue_btn']  

                # „Ç≠„É•„Éº„Çø„Ç§„Éó„ÅÆÈÅ∏Êäû
                queue_type_selector = gr.Radio(
                    choices=[translate("„Éó„É≠„É≥„Éó„Éà„Ç≠„É•„Éº"), translate("„Ç§„É°„Éº„Ç∏„Ç≠„É•„Éº")],
                    value=translate("„Éó„É≠„É≥„Éó„Éà„Ç≠„É•„Éº"),
                    label=translate("„Ç≠„É•„Éº„Çø„Ç§„Éó"),
                    visible=False,
                    interactive=True
                )

                # „Éó„É≠„É≥„Éó„Éà„Ç≠„É•„ÉºË®≠ÂÆö„Ç≥„É≥„Éù„Éº„Éç„É≥„ÉàÔºàÂàùÊúüÁä∂ÊÖã„Åß„ÅØÈùûË°®Á§∫Ôºâ
                with gr.Group(visible=False) as prompt_queue_group:
                    prompt_queue_file = gr.File(
                        label=translate("„Éó„É≠„É≥„Éó„Éà„Ç≠„É•„Éº„Éï„Ç°„Ç§„É´ (.txt) - 1Ë°å„Å´1„Å§„ÅÆ„Éó„É≠„É≥„Éó„Éà„ÅåË®òËºâ„Åï„Çå„Åü„ÉÜ„Ç≠„Çπ„Éà„Éï„Ç°„Ç§„É´"),
                        file_types=[".txt"]
                    )
                    gr.Markdown(translate("‚Äª „Éï„Ç°„Ç§„É´ÂÜÖ„ÅÆÂêÑË°å„ÅåÂà•„ÄÖ„ÅÆ„Éó„É≠„É≥„Éó„Éà„Å®„Åó„Å¶Âá¶ÁêÜ„Åï„Çå„Åæ„Åô„ÄÇ\n‚Äª „ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ„Åå„Ç™„Éï„ÅÆÂ†¥Âêà„ÅØÁÑ°Âäπ„ÄÇ\n‚Äª „Éê„ÉÉ„ÉÅÂá¶ÁêÜÂõûÊï∞„Çà„ÇäË°åÊï∞„ÅåÂ§ö„ÅÑÂ†¥Âêà„ÅØË°åÊï∞ÂàÜÂá¶ÁêÜ„Åï„Çå„Åæ„Åô„ÄÇ\n‚Äª „Éê„ÉÉ„ÉÅÂá¶ÁêÜÂõûÊï∞„Åå1„Åß„ÇÇ„Ç≠„É•„ÉºÂõûÊï∞„ÅåÂÑ™ÂÖà„Åï„Çå„Åæ„Åô„ÄÇ"))

                # „Ç§„É°„Éº„Ç∏„Ç≠„É•„ÉºÁî®„Ç∞„É´„Éº„Éó
                with gr.Group(visible=False) as image_queue_group:
                    gr.Markdown(translate("‚Äª 1ÂõûÁõÆ„ÅØImageÁîªÂÉè„Çí‰ΩøÁî®„Åó„ÄÅ2ÂõûÁõÆ‰ª•Èôç„ÅØÂÖ•Âäõ„Éï„Ç©„É´„ÉÄ„ÅÆÁîªÂÉè„Éï„Ç°„Ç§„É´„ÇíÂêçÂâçÈ†Ü„Å´‰ΩøÁî®„Åó„Åæ„Åô„ÄÇ\n‚Äª ÁîªÂÉè„Å®ÂêåÂêç„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„Éï„Ç°„Ç§„É´Ôºà‰æãÔºöimage1.jpg ‚Üí image1.txtÔºâ„Åå„ÅÇ„Çå„Å∞„ÄÅ„Åù„ÅÆÂÜÖÂÆπ„ÇíËá™ÂãïÁöÑ„Å´„Éó„É≠„É≥„Éó„Éà„Å®„Åó„Å¶‰ΩøÁî®„Åó„Åæ„Åô„ÄÇ\n‚Äª „Éê„ÉÉ„ÉÅÂõûÊï∞„ÅåÂÖ®ÁîªÂÉèÊï∞„ÇíË∂Ö„Åà„ÇãÂ†¥Âêà„ÄÅÊÆã„Çä„ÅØImageÁîªÂÉè„ÅßÂá¶ÁêÜ„Åï„Çå„Åæ„Åô„ÄÇ\n‚Äª „Éê„ÉÉ„ÉÅÂá¶ÁêÜÂõûÊï∞„Åå1„Åß„ÇÇ„Ç≠„É•„ÉºÂõûÊï∞„ÅåÂÑ™ÂÖà„Åï„Çå„Åæ„Åô„ÄÇ"))

                    # ÂÖ•Âäõ„Éï„Ç©„É´„ÉÄË®≠ÂÆö
                    with gr.Row():
                        input_folder_name = gr.Textbox(
                            label=translate("ÂÖ•Âäõ„Éï„Ç©„É´„ÉÄÂêç"),
                            value=input_folder_name_value,  # „Ç∞„É≠„Éº„Éê„É´Â§âÊï∞„Åã„ÇâÂÄ§„ÇíÂèñÂæó
                            info=translate("ÁîªÂÉè„Éï„Ç°„Ç§„É´„ÇíÊ†ºÁ¥ç„Åô„Çã„Éï„Ç©„É´„ÉÄÂêç")
                        )
                        open_input_folder_btn = gr.Button(value="üìÇ " + translate("‰øùÂ≠òÂèä„Å≥ÂÖ•Âäõ„Éï„Ç©„É´„ÉÄ„ÇíÈñã„Åè"), size="md")

                # „ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ„ÅÆÂ§âÊõ¥„Ç§„Éô„É≥„Éà„Å´Èñ¢Êï∞„ÇíÁ¥ê„Å•„Åë
                use_queue.change(
                    fn=toggle_queue_settings,
                    inputs=[use_queue],
                    outputs=[queue_type_selector, prompt_queue_group, image_queue_group]
                )

                # Config Queue System„ÅÆË°®Á§∫Âàá„ÇäÊõø„Åà„Ç§„Éô„É≥„Éà
                use_config_queue.change(
                    fn=toggle_config_queue,
                    inputs=[use_config_queue],
                    outputs=[config_queue_group]
                )

                # „Ç≠„É•„Éº„Çø„Ç§„Éó„ÅÆÈÅ∏Êäû„Ç§„Éô„É≥„Éà„Å´Èñ¢Êï∞„ÇíÁ¥ê„Å•„Åë
                queue_type_selector.change(
                    fn=toggle_queue_type,
                    inputs=[queue_type_selector],
                    outputs=[prompt_queue_group, image_queue_group]
                )

                # „Ç§„É°„Éº„Ç∏„Ç≠„É•„Éº„ÅÆ„Åü„ÇÅ„ÅÆÁîªÂÉè„Éï„Ç°„Ç§„É´„É™„Çπ„ÉàÂèñÂæóÈñ¢Êï∞„ÅØ„Ç∞„É≠„Éº„Éê„É´Èñ¢Êï∞„Çí‰ΩøÁî®

                # „Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Ç§„Éô„É≥„Éà„Çí„Éè„É≥„Éâ„É©„Å´Êé•Á∂ö
                prompt_queue_file.change(
                    fn=handle_file_upload,
                    inputs=[prompt_queue_file],
                    outputs=[prompt_queue_file]
                )

                # ÂÖ•Âäõ„Éï„Ç©„É´„ÉÄÂêçÂ§âÊõ¥„Ç§„Éô„É≥„Éà„Çí„Éè„É≥„Éâ„É©„Å´Êé•Á∂ö
                input_folder_name.change(
                    fn=handle_input_folder_change,
                    inputs=[input_folder_name],
                    outputs=[input_folder_name]
                )

                # ÂÖ•Âäõ„Éï„Ç©„É´„ÉÄ„ÇíÈñã„Åè„Éú„Çø„É≥„Å´„Ç§„Éô„É≥„Éà„ÇíÊé•Á∂ö
                open_input_folder_btn.click(
                    fn=open_input_folder,
                    inputs=[],
                    outputs=[gr.Textbox(visible=False)]  # ‰∏ÄÊôÇÁöÑ„Å™„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØË°®Á§∫Áî®ÔºàÈùûË°®Á§∫Ôºâ
                )

            # ÈñãÂßã„ÉªÁµÇ‰∫Ü„Éú„Çø„É≥
            with gr.Row():
                start_button = gr.Button(value=translate("Start Generation"))
                end_button = gr.Button(value=translate("End Generation"), interactive=False)

            # FP8ÊúÄÈÅ©ÂåñË®≠ÂÆö
            with gr.Row():
                fp8_optimization = gr.Checkbox(
                    label=translate("FP8 ÊúÄÈÅ©Âåñ"),
                    value=True,
                    info=translate("„É°„É¢„É™‰ΩøÁî®Èáè„ÇíÂâäÊ∏õ„ÅóÈÄüÂ∫¶„ÇíÊîπÂñÑÔºàPyTorch 2.1‰ª•‰∏ä„ÅåÂøÖË¶ÅÔºâ")
                )

            # „Çª„ÇØ„Ç∑„Éß„É≥ÂÖ•ÂäõÁî®„ÅÆ„É™„Çπ„Éà„ÇíÂàùÊúüÂåñ
            section_number_inputs = []
            section_image_inputs = []
            section_prompt_inputs = []  # „Éó„É≠„É≥„Éó„ÉàÂÖ•ÂäõÊ¨ÑÁî®„ÅÆ„É™„Çπ„Éà
            section_row_groups = []  # ÂêÑ„Çª„ÇØ„Ç∑„Éß„É≥„ÅÆUIË°å„ÇíÁÆ°ÁêÜ„Åô„Çã„É™„Çπ„Éà

            # Ë®≠ÂÆö„Åã„ÇâÊúÄÂ§ß„Ç≠„Éº„Éï„É¨„Éº„É†Êï∞„ÇíÂèñÂæó
            max_keyframes = get_max_keyframes_count()

            # ÁèæÂú®„ÅÆÂãïÁîª„É¢„Éº„Éâ„ÅßÂøÖË¶Å„Å™„Çª„ÇØ„Ç∑„Éß„É≥Êï∞„ÇíÂèñÂæó„Åô„ÇãÈñ¢Êï∞
            def get_current_sections_count():
                mode_value = length_radio.value
                if mode_value in VIDEO_MODE_SETTINGS:
                    # sectionsÂÄ§„Çí„Åù„ÅÆ„Åæ„Åæ‰ΩøÁî® - Ê≥®Ôºö„Åì„Çå„ÅØ0„Åã„ÇâÂßã„ÇÅ„ÅüÂ†¥Âêà„ÅÆÊúÄÂ§ßÂÄ§„Å®„Å™„Çã
                    return VIDEO_MODE_SETTINGS[mode_value]["sections"]
                return max_keyframes  # „Éá„Éï„Ç©„É´„ÉàÂÄ§

            # ÁèæÂú®„ÅÆÂøÖË¶Å„Çª„ÇØ„Ç∑„Éß„É≥Êï∞„ÇíÂèñÂæó
            initial_sections_count = get_current_sections_count()
            # Á∞°Áï•Âåñ„Çª„ÇØ„Ç∑„Éß„É≥Ë°®Á§∫
            # „Çª„ÇØ„Ç∑„Éß„É≥„Çø„Ç§„Éà„É´„ÅÆÈñ¢Êï∞„ÅØÂâäÈô§„Åó„ÄÅÂõ∫ÂÆö„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆ„ÅøË°®Á§∫

            # Âüã„ÇÅËæº„Åø„Éó„É≠„É≥„Éó„Éà„Åä„Çà„Å≥„Ç∑„Éº„Éâ„ÇíË§áÂÜô„Åô„Çã„ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ - ÂèÇÁÖßÁî®„Å´ÂÆöÁæ©ÔºàË°®Á§∫„ÅØLoRAË®≠ÂÆö„ÅÆ‰∏ã„ÅßË°å„ÅÜÔºâ
            # „Ç∞„É≠„Éº„Éê„É´Â§âÊï∞„Å®„Åó„Å¶ÂÆöÁæ©„Åó„ÄÅÂæå„Åß‰ªñ„ÅÆÂ†¥ÊâÄ„Åã„ÇâÂèÇÁÖß„Åß„Åç„Çã„Çà„ÅÜ„Å´„Åô„Çã
            global copy_metadata
            copy_metadata = gr.Checkbox(
                label=translate("Âüã„ÇÅËæº„Åø„Éó„É≠„É≥„Éó„Éà„Åä„Çà„Å≥„Ç∑„Éº„Éâ„ÇíË§áÂÜô„Åô„Çã"),
                value=False,
                info=translate("„ÉÅ„Çß„ÉÉ„ÇØ„Çí„Ç™„É≥„Å´„Åô„Çã„Å®„ÄÅÁîªÂÉè„ÅÆ„É°„Çø„Éá„Éº„Çø„Åã„Çâ„Éó„É≠„É≥„Éó„Éà„Å®„Ç∑„Éº„Éâ„ÇíËá™ÂãïÁöÑ„Å´ÂèñÂæó„Åó„Åæ„Åô"),
                visible=False  # ÂÖÉ„ÅÆ‰ΩçÁΩÆ„Åß„ÅØÈùûË°®Á§∫
            )

            # F1„É¢„Éº„Éâ„Åß„ÅØ„Çª„ÇØ„Ç∑„Éß„É≥Ë®≠ÂÆö„ÅØÂÆåÂÖ®„Å´ÂâäÈô§
            # Èö†„Åó„Ç≥„É≥„Éù„Éº„Éç„É≥„ÉàÔºà‰∫íÊèõÊÄß„ÅÆ„Åü„ÇÅÔºâ
            section_image_inputs = []
            section_number_inputs = []
            section_prompt_inputs = []
            section_row_groups = []


            # „É°„Çø„Éá„Éº„ÇøÊäΩÂá∫Èñ¢Êï∞„ÇíÂÆöÁæ©ÔºàÂæå„ÅßÁôªÈå≤„Åô„ÇãÔºâ
            def update_from_image_metadata(image_path, copy_enabled=False):
                """Image„Ç¢„ÉÉ„Éó„É≠„Éº„ÉâÊôÇ„Å´„É°„Çø„Éá„Éº„Çø„ÇíÊäΩÂá∫„Åó„Å¶UI„Å´ÂèçÊò†„Åô„Çã
                F1„É¢„Éº„Éâ„Åß„ÅØ„Ç≠„Éº„Éï„É¨„Éº„É†„Ç≥„Éî„ÉºÊ©üËÉΩ„ÇíÂâäÈô§Ê∏à„Åø„ÅÆ„Åü„ÇÅ„ÄÅÂçòÁ¥îÂåñ
                """
                # Ë§áÂÜôÊ©üËÉΩ„ÅåÁÑ°Âäπ„ÅÆÂ†¥Âêà„ÅØ‰Ωï„ÇÇ„Åó„Å™„ÅÑ
                if not copy_enabled:
                    return [gr.update()] * 2

                if image_path is None:
                    return [gr.update()] * 2

                try:
                    # „Éï„Ç°„Ç§„É´„Éë„Çπ„Åã„ÇâÁõ¥Êé•„É°„Çø„Éá„Éº„Çø„ÇíÊäΩÂá∫
                    metadata = extract_metadata_from_png(image_path)

                    if not metadata:
                        print(translate("„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åï„Çå„ÅüÁîªÂÉè„Å´„É°„Çø„Éá„Éº„Çø„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì"))
                        return [gr.update()] * 2

                    print(translate("ÁîªÂÉè„Åã„Çâ„É°„Çø„Éá„Éº„Çø„ÇíÊäΩÂá∫„Åó„Åæ„Åó„Åü: {0}").format(metadata))

                    # „Éó„É≠„É≥„Éó„Éà„Å®SEED„ÇíUI„Å´ÂèçÊò†
                    prompt_update = gr.update()
                    seed_update = gr.update()

                    if PROMPT_KEY in metadata and metadata[PROMPT_KEY]:
                        prompt_update = gr.update(value=metadata[PROMPT_KEY])

                    if SEED_KEY in metadata and metadata[SEED_KEY]:
                        # SEEDÂÄ§„ÇíÊï¥Êï∞„Å´Â§âÊèõ
                        try:
                            seed_value = int(metadata[SEED_KEY])
                            seed_update = gr.update(value=seed_value)
                        except (ValueError, TypeError):
                            print(translate("SEEDÂÄ§„ÅÆÂ§âÊèõ„Ç®„É©„Éº: {0}").format(metadata[SEED_KEY]))

                    return [prompt_update, seed_update]
                except Exception as e:
                    print(translate("„É°„Çø„Éá„Éº„ÇøÊäΩÂá∫Âá¶ÁêÜ‰∏≠„ÅÆ„Ç®„É©„Éº: {0}").format(e))
                    traceback.print_exc()
                    print(translate("„É°„Çø„Éá„Éº„ÇøÊäΩÂá∫„Ç®„É©„Éº: {0}").format(e))
                    return [gr.update()] * 2

            # Ê≥®ÊÑè: „Ç§„Éô„É≥„ÉàÁôªÈå≤„ÅØÂ§âÊï∞ÂÆöÁæ©Âæå„Å´Ë°å„ÅÜ„Åü„ÇÅ„ÄÅÂæå„ÅßÂÆüË°å„Åô„Çã
            # „É°„Çø„Éá„Éº„ÇøÊäΩÂá∫Âá¶ÁêÜ„ÅÆÁôªÈå≤„ÅØ„ÄÅprompt„Å®seedÂ§âÊï∞„ÅÆÂÆöÁæ©Âæå„Å´ÁßªÂãï„Åó„Åæ„Åô

            # LoRAË®≠ÂÆö„Ç∞„É´„Éº„Éó„ÇíËøΩÂä†
            with gr.Group(visible=has_lora_support) as lora_settings_group:
                gr.Markdown(f"### " + translate("LoRAË®≠ÂÆö"))

                # LoRA‰ΩøÁî®ÊúâÁÑ°„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ

                use_lora = gr.Checkbox(label=translate("LoRA„Çí‰ΩøÁî®„Åô„Çã"), value=False, info=translate("„ÉÅ„Çß„ÉÉ„ÇØ„Çí„Ç™„É≥„Å´„Åô„Çã„Å®LoRA„Çí‰ΩøÁî®„Åó„Åæ„ÅôÔºàË¶Å16GB VRAM‰ª•‰∏äÔºâ"))

                def scan_lora_directory():
                    """./lora„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâLoRA„É¢„Éá„É´„Éï„Ç°„Ç§„É´„ÇíÊ§úÁ¥¢„Åô„ÇãÈñ¢Êï∞ - ENHANCED VERSION"""
                    lora_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'lora')
                    choices = []
                    
                    # „Éá„Ç£„É¨„ÇØ„Éà„É™„ÅåÂ≠òÂú®„Åó„Å™„ÅÑÂ†¥Âêà„ÅØ‰ΩúÊàê
                    if not os.path.exists(lora_dir):
                        os.makedirs(lora_dir, exist_ok=True)
                        print(translate("LoRA„Éá„Ç£„É¨„ÇØ„Éà„É™„ÅåÂ≠òÂú®„Åó„Å™„Åã„Å£„Åü„Åü„ÇÅ‰ΩúÊàê„Åó„Åæ„Åó„Åü: {0}").format(lora_dir))
                    
                    # „Éá„Ç£„É¨„ÇØ„Éà„É™ÂÜÖ„ÅÆ„Éï„Ç°„Ç§„É´„Çí„É™„Çπ„Éà„Ç¢„ÉÉ„Éó
                    try:
                        for filename in os.listdir(lora_dir):
                            if filename.endswith(('.safetensors', '.pt', '.bin')):
                                choices.append(filename)
                    except Exception as e:
                        print(translate("Error scanning LoRA directory: {0}").format(e))
                    
                    # Á©∫„ÅÆÈÅ∏ÊäûËÇ¢„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØ"„Å™„Åó"„ÇíËøΩÂä†
                    choices = sorted(choices)
                    
                    # „Å™„Åó„ÅÆÈÅ∏ÊäûËÇ¢„ÇíÊúÄÂàù„Å´ËøΩÂä†
                    none_choice = translate("„Å™„Åó")
                    choices.insert(0, none_choice)
                    
                    # ÈáçË¶Å: „Åô„Åπ„Å¶„ÅÆÈÅ∏ÊäûËÇ¢„ÅåÁ¢∫ÂÆü„Å´ÊñáÂ≠óÂàóÂûã„Åß„ÅÇ„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç
                    for i, choice in enumerate(choices):
                        if not isinstance(choice, str):
                            choices[i] = str(choice)
                    
                    print(translate("üîç Scanned LoRA directory: found {0} files").format(len(choices)-1))
                    return choices
                
                # LoRA„ÅÆË™≠„ÅøËæº„ÅøÊñπÂºè„ÇíÈÅ∏Êäû„Åô„Çã„É©„Ç∏„Ç™„Éú„Çø„É≥
                lora_mode = gr.Radio(
                    choices=[translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû"), translate("„Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ")],
                    value=translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû"),
                    label=translate("LoRAË™≠„ÅøËæº„ÅøÊñπÂºè"),
                    visible=False  # ÂàùÊúüÁä∂ÊÖã„Åß„ÅØÈùûË°®Á§∫Ôºàtoggle_lora_settings„ÅßÂà∂Âæ°Ôºâ
                )

                # „Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Ç∞„É´„Éº„Éó - ÂàùÊúüÁä∂ÊÖã„ÅØÈùûË°®Á§∫
                with gr.Group(visible=False) as lora_upload_group:
                    # „É°„Ç§„É≥„ÅÆLoRA„Éï„Ç°„Ç§„É´
                    lora_files = gr.File(
                        label=translate("LoRA„Éï„Ç°„Ç§„É´ (.safetensors, .pt, .bin)"),
                        file_types=[".safetensors", ".pt", ".bin"],
                        visible=True
                    )
                    # ËøΩÂä†„ÅÆLoRA„Éï„Ç°„Ç§„É´1
                    lora_files2 = gr.File(
                        label=translate("LoRA„Éï„Ç°„Ç§„É´2 (.safetensors, .pt, .bin)"),
                        file_types=[".safetensors", ".pt", ".bin"],
                        visible=True
                    )
                    # ËøΩÂä†„ÅÆLoRA„Éï„Ç°„Ç§„É´2ÔºàF1Áâà„Åß„ÇÇ3„Å§ÁõÆ„ÇíËøΩÂä†Ôºâ
                    lora_files3 = gr.File(
                        label=translate("LoRA„Éï„Ç°„Ç§„É´3 (.safetensors, .pt, .bin)"),
                        file_types=[".safetensors", ".pt", ".bin"],
                        visible=True
                    )
                
                # „Éá„Ç£„É¨„ÇØ„Éà„É™ÈÅ∏Êäû„Ç∞„É´„Éº„Éó - ÂàùÊúüÁä∂ÊÖã„ÅØÈùûË°®Á§∫
                with gr.Group(visible=False) as lora_dropdown_group:
                    # „Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„Çâ„Çπ„Ç≠„É£„É≥„Åï„Çå„Åü„É¢„Éá„É´„ÅÆ„Éâ„É≠„ÉÉ„Éó„ÉÄ„Ç¶„É≥
                    lora_dropdown1 = gr.Dropdown(
                        label=translate("LoRA„É¢„Éá„É´ÈÅ∏Êäû 1"),
                        choices=[],
                        value=None,
                        allow_custom_value=False
                    )
                    lora_dropdown2 = gr.Dropdown(
                        label=translate("LoRA„É¢„Éá„É´ÈÅ∏Êäû 2"),
                        choices=[],
                        value=None,
                        allow_custom_value=False
                    )
                    lora_dropdown3 = gr.Dropdown(
                        label=translate("LoRA„É¢„Éá„É´ÈÅ∏Êäû 3"),
                        choices=[],
                        value=None,
                        allow_custom_value=False
                    )
                    # „Çπ„Ç≠„É£„É≥„Éú„Çø„É≥
                    lora_scan_button = gr.Button(translate("LoRA„Éá„Ç£„É¨„ÇØ„Éà„É™„ÇíÂÜç„Çπ„Ç≠„É£„É≥"), variant="secondary")
                
                # „Çπ„Ç±„Éº„É´ÂÄ§„ÅÆÂÖ•Âäõ„Éï„Ç£„Éº„É´„ÉâÔºà‰∏°Êñπ„ÅÆÊñπÂºè„ÅßÂÖ±ÈÄöÔºâ
                lora_scales_text = gr.Textbox(
                    label=translate("LoRAÈÅ©Áî®Âº∑Â∫¶ („Ç´„É≥„ÉûÂå∫Âàá„Çä)"),
                    value="0.8,0.8,0.8",
                    info=translate("ÂêÑLoRA„ÅÆ„Çπ„Ç±„Éº„É´ÂÄ§„Çí„Ç´„É≥„ÉûÂå∫Âàá„Çä„ÅßÂÖ•Âäõ (‰æã: 0.8,0.5,0.3)"),
                    visible=False
                )

                # „ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ„ÅÆÁä∂ÊÖã„Å´„Çà„Å£„Å¶‰ªñ„ÅÆLoRAË®≠ÂÆö„ÅÆË°®Á§∫/ÈùûË°®Á§∫„ÇíÂàá„ÇäÊõø„Åà„ÇãÈñ¢Êï∞
                def toggle_lora_settings(use_lora):
                    """
                    BASIC LORA TOGGLE: Original LoRA visibility control (simplified)
                    
                    This is the simplified version of the original inline function.
                    It only handles basic visibility without the complex config loading logic.
                    
                    PARAMETERS:
                    use_lora (bool): Whether LoRA is enabled
                    
                    RETURNS:
                    List of basic UI updates for visibility control:
                    [lora_mode_visible, lora_upload_group_visible, lora_dropdown_group_visible, lora_scales_visible]
                    """
                    if use_lora:
                        # LoRA‰ΩøÁî®ÊôÇ„ÅØ„Éá„Éï„Ç©„É´„Éà„Åß„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû„É¢„Éº„Éâ„ÇíË°®Á§∫
                        choices = scan_lora_directory()
                        
                        # ÈÅ∏ÊäûËÇ¢„ÅÆÂûã„ÉÅ„Çß„ÉÉ„ÇØ„ÇíËøΩÂä†
                        for i, choice in enumerate(choices):
                            if not isinstance(choice, str):
                                choices[i] = str(choice)
                        
                        # „Éó„É™„Çª„ÉÉ„Éà„ÅØ„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû„É¢„Éº„Éâ„ÅÆÂ†¥Âêà„ÅÆ„ÅøË°®Á§∫
                        preset_visible = True  # „Éá„Éï„Ç©„É´„Éà„ÅØ„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû„Å™„ÅÆ„ÅßË°®Á§∫
                        
                        # „Éâ„É≠„ÉÉ„Éó„ÉÄ„Ç¶„É≥„ÅåÂàùÊúüÂåñÊôÇ„Å´„ÇÇÁ¢∫ÂÆü„Å´Êõ¥Êñ∞„Åï„Çå„Çã„Çà„ÅÜ„Å´„Åô„Çã
                        return [
                            gr.update(visible=True),  # lora_mode
                            gr.update(visible=False),  # lora_upload_group - „Éá„Éï„Ç©„É´„Éà„Åß„ÅØÈùûË°®Á§∫
                            gr.update(visible=True),  # lora_dropdown_group - „Éá„Éï„Ç©„É´„Éà„ÅßË°®Á§∫
                            gr.update(visible=True),  # lora_scales_text
                        ]
                    else:
                        # LoRA‰∏ç‰ΩøÁî®ÊôÇ„ÅØ„Åô„Åπ„Å¶ÈùûË°®Á§∫
                        return [
                            gr.update(visible=False),  # lora_mode
                            gr.update(visible=False),  # lora_upload_group
                            gr.update(visible=False),  # lora_dropdown_group
                            gr.update(visible=False),  # lora_scales_text
                        ]

                # LoRAË™≠„ÅøËæº„ÅøÊñπÂºè„Å´Âøú„Åò„Å¶Ë°®Á§∫„ÇíÂàá„ÇäÊõø„Åà„ÇãÈñ¢Êï∞
                def toggle_lora_mode(mode):
                    if mode == translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû"):
                        # „Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû„É¢„Éº„Éâ„ÅÆÂ†¥Âêà
                        # ÊúÄÂàù„Å´„Éá„Ç£„É¨„ÇØ„Éà„É™„Çí„Çπ„Ç≠„É£„É≥
                        choices = scan_lora_directory()
                        
                        # ÈÅ∏ÊäûËÇ¢„ÅÆÂûã„ÇíÊòéÁ§∫ÁöÑ„Å´Á¢∫Ë™çÔºÜÂ§âÊèõ
                        for i, choice in enumerate(choices):
                            if not isinstance(choice, str):
                                choices[i] = str(choice)
                        
                        # ÊúÄÂàù„ÅÆÈÅ∏ÊäûËÇ¢„Åå„Å°„ÇÉ„Çì„Å®ÊñáÂ≠óÂàó„Å´„Å™„Å£„Å¶„ÅÑ„Çã„ÅãÂÜçÁ¢∫Ë™ç
                        first_choice = choices[0]
                        
                        # ÈÅ∏ÊäûËÇ¢„ÅåÁ¢∫ÂÆü„Å´Êõ¥Êñ∞„Åï„Çå„Çã„Çà„ÅÜ„Å´„Åô„Çã
                        return [
                            gr.update(visible=False),                                # lora_upload_group
                            gr.update(visible=True),                                 # lora_dropdown_group
                            gr.update(choices=choices, value=choices[0]),            # lora_dropdown1
                            gr.update(choices=choices, value=choices[0]),            # lora_dropdown2
                            gr.update(choices=choices, value=choices[0])             # lora_dropdown3
                        ]
                    else:  # „Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ
                        # „Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„ÉâÊñπÂºè„ÅÆÂ†¥Âêà„ÄÅ„Éâ„É≠„ÉÉ„Éó„ÉÄ„Ç¶„É≥„ÅÆÂÄ§„ÅØÊõ¥Êñ∞„Åó„Å™„ÅÑ
                        return [
                            gr.update(visible=True),   # lora_upload_group
                            gr.update(visible=False),  # lora_dropdown_group
                            gr.update(),               # lora_dropdown1 - Â§âÊõ¥„Å™„Åó
                            gr.update(),               # lora_dropdown2 - Â§âÊõ¥„Å™„Åó
                            gr.update()                # lora_dropdown3 - Â§âÊõ¥„Å™„Åó
                        ]
                
                # „Çπ„Ç≠„É£„É≥„Éú„Çø„É≥„ÅÆÂá¶ÁêÜÈñ¢Êï∞
                def update_lora_dropdowns():
                    choices = scan_lora_directory()
                    
                    # „Åô„Åπ„Å¶„ÅÆÈÅ∏ÊäûËÇ¢„ÅåÁ¢∫ÂÆü„Å´ÊñáÂ≠óÂàóÂûã„Åß„ÅÇ„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç
                    for i, choice in enumerate(choices):
                        if not isinstance(choice, str):
                            choices[i] = str(choice)
                    
                    # ÂêÑ„Éâ„É≠„ÉÉ„Éó„ÉÄ„Ç¶„É≥„ÇíÊõ¥Êñ∞
                    return [
                        gr.update(choices=choices, value=choices[0]),  # lora_dropdown1
                        gr.update(choices=choices, value=choices[0]),  # lora_dropdown2
                        gr.update(choices=choices, value=choices[0]),  # lora_dropdown3
                    ]
                
                # ÂâçÂõû„ÅÆLoRA„É¢„Éº„Éâ„ÇíË®òÊÜ∂„Åô„Çã„Åü„ÇÅ„ÅÆÂ§âÊï∞
                previous_lora_mode = translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû")  # „Éá„Éï„Ç©„É´„Éà„ÅØ„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû
                
                # LoRA„É¢„Éº„Éâ„ÅÆÂ§âÊõ¥„ÇíÂá¶ÁêÜ„Åô„ÇãÈñ¢Êï∞
                def toggle_lora_mode_with_memory(mode_value):
                    # „Ç∞„É≠„Éº„Éê„É´Â§âÊï∞„Å´ÈÅ∏Êäû„Çí‰øùÂ≠ò
                    global previous_lora_mode
                    previous_lora_mode = mode_value
                    
                    # Ê®ôÊ∫ñ„ÅÆtoggle_lora_modeÈñ¢Êï∞„ÇíÂëº„Å≥Âá∫„Åó
                    return toggle_lora_mode(mode_value)

                # „ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ„ÅÆÂ§âÊõ¥„Ç§„Éô„É≥„Éà„Å´LoRAË®≠ÂÆöÂÖ®‰Ωì„ÅÆË°®Á§∫/ÈùûË°®Á§∫„ÇíÂàá„ÇäÊõø„Åà„ÇãÈñ¢Êï∞„ÇíÁ¥ê„Å•„Åë
                use_lora.change(
                    fn=toggle_lora_full_update,
                    inputs=[use_lora],
                    outputs=[lora_mode, lora_upload_group, lora_dropdown_group, lora_scales_text,
                             lora_dropdown1, lora_dropdown2, lora_dropdown3]
                )


                # LoRAË™≠„ÅøËæº„ÅøÊñπÂºè„ÅÆÂ§âÊõ¥„Ç§„Éô„É≥„Éà„Å´Ë°®Á§∫ÂàáÊõøÈñ¢Êï∞„ÇíÁ¥ê„Å•„Åë
                lora_mode.change(
                    fn=toggle_lora_mode_with_memory,
                    inputs=[lora_mode],
                    outputs=[lora_upload_group, lora_dropdown_group, lora_dropdown1, lora_dropdown2, lora_dropdown3]
                )
                
                # „Çπ„Ç≠„É£„É≥„Éú„Çø„É≥„ÅÆÂá¶ÁêÜ„ÇíÁ¥ê„Å•„Åë
                lora_scan_button.click(
                    fn=update_lora_dropdowns,
                    inputs=[],
                    outputs=[lora_dropdown1, lora_dropdown2, lora_dropdown3]
                )
                
                # UI„É≠„Éº„ÉâÊôÇ„ÅÆLoRAÂàùÊúüÂåñÈñ¢Êï∞
                def lora_ready_init():
                    """LoRA„Éâ„É≠„ÉÉ„Éó„ÉÄ„Ç¶„É≥„ÅÆÂàùÊúüÂåñ„ÇíË°å„ÅÜÈñ¢Êï∞"""
                    
                    # ÁèæÂú®„ÅÆuse_lora„Å®lora_mode„ÅÆÂÄ§„ÇíÂèñÂæó
                    use_lora_value = getattr(use_lora, 'value', False)
                    lora_mode_value = getattr(lora_mode, 'value', translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû"))
                    
                    # „Ç∞„É≠„Éº„Éê„É´Â§âÊï∞„ÇíÊõ¥Êñ∞
                    global previous_lora_mode
                    previous_lora_mode = lora_mode_value
                    
                    if use_lora_value:
                        # LoRA„ÅåÊúâÂäπ„Å™Â†¥Âêà
                        if lora_mode_value == translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû"):
                            # „Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû„É¢„Éº„Éâ„ÅÆÂ†¥Âêà„ÅØ„Éâ„É≠„ÉÉ„Éó„ÉÄ„Ç¶„É≥„ÇíÂàùÊúüÂåñ
                            choices = scan_lora_directory()
                            return [
                                gr.update(choices=choices, value=choices[0]),  # lora_dropdown1
                                gr.update(choices=choices, value=choices[0]),  # lora_dropdown2
                                gr.update(choices=choices, value=choices[0])   # lora_dropdown3
                            ]
                        else:
                            # „Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„É¢„Éº„Éâ„ÅÆÂ†¥Âêà„ÅØ„Éâ„É≠„ÉÉ„Éó„ÉÄ„Ç¶„É≥„ÇíÊõ¥Êñ∞„Åó„Å™„ÅÑ
                            return [gr.update(), gr.update(), gr.update()]
                    
                    # LoRA„ÅåÁÑ°Âäπ„Å™Â†¥Âêà„ÅØ‰Ωï„ÇÇÊõ¥Êñ∞„Åó„Å™„ÅÑ
                    return [gr.update(), gr.update(), gr.update()]
                
                # ÂàùÊúüÂåñÁî®„ÅÆÈùûË°®Á§∫„Éú„Çø„É≥
                lora_init_btn = gr.Button(visible=False, elem_id="lora_init_btn_f1")
                lora_init_btn.click(
                    fn=lora_ready_init,
                    inputs=[],
                    outputs=[lora_dropdown1, lora_dropdown2, lora_dropdown3]
                )
                
                # UI„É≠„Éº„ÉâÂæå„Å´Ëá™ÂãïÁöÑ„Å´ÂàùÊúüÂåñ„Åô„ÇãJavaScript„ÇíËøΩÂä†
                js_init_code = """
                function initLoraDropdowns() {
                    // UI„É≠„Éº„ÉâÂæå„ÄÅÂ∞ë„ÅóÂæÖ„Å£„Å¶„Åã„Çâ„Éú„Çø„É≥„Çí„ÇØ„É™„ÉÉ„ÇØ
                    setTimeout(function() {
                        // ÈùûË°®Á§∫„Éú„Çø„É≥„ÇíÊé¢„Åó„Å¶Ëá™Âãï„ÇØ„É™„ÉÉ„ÇØ
                        var initBtn = document.getElementById('lora_init_btn_f1');
                        if (initBtn) {
                            console.log('LoRA„Éâ„É≠„ÉÉ„Éó„ÉÄ„Ç¶„É≥ÂàùÊúüÂåñ„Éú„Çø„É≥„ÇíËá™ÂãïÂÆüË°å„Åó„Åæ„Åô');
                            initBtn.click();
                        } else {
                            console.log('LoRA„Éâ„É≠„ÉÉ„Éó„ÉÄ„Ç¶„É≥ÂàùÊúüÂåñ„Éú„Çø„É≥„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì');
                        }
                    }, 1000); // 1ÁßíÂæÖ„Å£„Å¶„Åã„ÇâÂÆüË°å
                }
                
                // „Éö„Éº„Ç∏„É≠„Éº„ÉâÊôÇ„Å´ÂàùÊúüÂåñÈñ¢Êï∞„ÇíÂëº„Å≥Âá∫„Åó
                window.addEventListener('load', initLoraDropdowns);
                """
                
                # JavaScript„Ç≥„Éº„Éâ„ÇíUI„Å´ËøΩÂä†
                gr.HTML(f"<script>{js_init_code}</script>")
            
            # LoRA„Éó„É™„Çª„ÉÉ„ÉàÁî®Â§âÊï∞„ÇíÂàùÊúüÂåñ
            lora_preset_group = None
            
            # LoRA„Éó„É™„Çª„ÉÉ„ÉàÊ©üËÉΩÔºàLoRA„ÅåÊúâÂäπ„Å™Â†¥Âêà„ÅÆ„ÅøÔºâ
            if has_lora_support:
                from eichi_utils.lora_preset_manager import save_lora_preset, load_lora_preset
                
                # LoRA„Éó„É™„Çª„ÉÉ„ÉàÊ©üËÉΩÔºàÂàùÊúüÁä∂ÊÖã„Åß„ÅØÈùûË°®Á§∫Ôºâ
                with gr.Group(visible=False) as lora_preset_group:
                    # „Ç∑„É≥„Éó„É´„Å™1Ë°å„É¨„Ç§„Ç¢„Ç¶„Éà
                    with gr.Row():
                        # „Éó„É™„Çª„ÉÉ„ÉàÈÅ∏Êäû„Éú„Çø„É≥Ôºà1-5Ôºâ
                        preset_buttons = []
                        for i in range(1, 6):
                            preset_buttons.append(
                                gr.Button(
                                    translate("Ë®≠ÂÆö{0}").format(i),
                                    variant="secondary",
                                    scale=1
                                )
                            )
                        
                        # Load/SaveÈÅ∏ÊäûÔºà„É©„Éô„É´„Å™„Åó„ÄÅÊ®™‰∏¶„Å≥Ôºâ
                        with gr.Row(scale=1):
                            load_btn = gr.Button(translate("Load"), variant="primary", scale=1)
                            save_btn = gr.Button(translate("Save"), variant="secondary", scale=1)
                        # ÂÜÖÈÉ®ÁöÑ„Å´‰Ωø„ÅÜRadioÔºàÈùûË°®Á§∫Ôºâ
                        lora_preset_mode = gr.Radio(
                            choices=[translate("Load"), translate("Save")],
                            value=translate("Load"),
                            visible=False
                        )
                    
                    # „Éó„É™„Çª„ÉÉ„ÉàÁä∂ÊÖãË°®Á§∫
                    lora_preset_status = gr.Textbox(
                        label=translate("„Éó„É™„Çª„ÉÉ„ÉàÁä∂ÊÖã"),
                        value="",
                        interactive=False,
                        lines=1
                    )
                
                # LoRA„Çø„Ç§„Éó„Å®„Éó„É™„Çª„ÉÉ„ÉàË°®Á§∫„ÅÆÁµÑ„ÅøÂêà„Çè„Åõ„ÇíÂà∂Âæ°„Åô„ÇãÈñ¢Êï∞
                def toggle_lora_and_preset(use_lora_val, lora_mode_val):
                    # LoRA„ÅåÊúâÂäπ„Åã„Å§„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû„É¢„Éº„Éâ„ÅÆÂ†¥Âêà„ÅÆ„Åø„Éó„É™„Çª„ÉÉ„Éà„ÇíË°®Á§∫
                    preset_visible = use_lora_val and lora_mode_val == translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû")
                    return gr.update(visible=preset_visible)
                
                # LoRA„Éó„É™„Çª„ÉÉ„ÉàÊ©üËÉΩ„ÅÆ„Éè„É≥„Éâ„É©„ÉºÈñ¢Êï∞
                def handle_lora_preset_button(button_index, mode, lora1, lora2, lora3, scales):
                    """LoRA„Éó„É™„Çª„ÉÉ„Éà„Éú„Çø„É≥„ÅÆ„ÇØ„É™„ÉÉ„ÇØ„ÇíÂá¶ÁêÜ„Åô„Çã"""
                    if mode == translate("Load"):  # Load
                        # „É≠„Éº„Éâ„É¢„Éº„Éâ
                        loaded_values = load_lora_preset(button_index)
                        if loaded_values:
                            return (
                                gr.update(value=loaded_values[0]),  # lora_dropdown1
                                gr.update(value=loaded_values[1]),  # lora_dropdown2
                                gr.update(value=loaded_values[2]),  # lora_dropdown3
                                gr.update(value=loaded_values[3]),  # lora_scales_text
                                translate("Ë®≠ÂÆö{0}„ÇíË™≠„ÅøËæº„Åø„Åæ„Åó„Åü").format(button_index + 1)  # status
                            )
                        else:
                            return (
                                gr.update(), gr.update(), gr.update(), gr.update(),
                                translate("Ë®≠ÂÆö{0}„ÅÆË™≠„ÅøËæº„Åø„Å´Â§±Êïó„Åó„Åæ„Åó„Åü").format(button_index + 1)
                            )
                    else:
                        # „Çª„Éº„Éñ„É¢„Éº„Éâ
                        success, message = save_lora_preset(button_index, lora1, lora2, lora3, scales)
                        return (
                            gr.update(), gr.update(), gr.update(), gr.update(),
                            message
                        )
                
                # Load/Save„Éú„Çø„É≥„ÅÆ„Ç§„Éô„É≥„Éà„Éè„É≥„Éâ„É©„Éº
                def set_load_mode():
                    return (
                        gr.update(value=translate("Load")),
                        gr.update(variant="primary"),
                        gr.update(variant="secondary")
                    )
                
                def set_save_mode():
                    return (
                        gr.update(value=translate("Save")),
                        gr.update(variant="secondary"),
                        gr.update(variant="primary")
                    )
                
                # „Ç§„Éô„É≥„Éà„ÅÆË®≠ÂÆö
                # „Éó„É™„Çª„ÉÉ„Éà„Éú„Çø„É≥„ÅÆ„Ç§„Éô„É≥„Éà
                for i, btn in enumerate(preset_buttons):
                    btn.click(
                        fn=lambda mode, lora1, lora2, lora3, scales, idx=i: handle_lora_preset_button(
                            idx, mode, lora1, lora2, lora3, scales
                        ),
                        inputs=[lora_preset_mode, lora_dropdown1, lora_dropdown2, lora_dropdown3, lora_scales_text],
                        outputs=[lora_dropdown1, lora_dropdown2, lora_dropdown3, lora_scales_text, lora_preset_status]
                    )
                
                # Load/Save„Éú„Çø„É≥„ÅÆ„Ç§„Éô„É≥„Éà
                load_btn.click(
                    set_load_mode,
                    outputs=[lora_preset_mode, load_btn, save_btn]
                )
                
                save_btn.click(
                    set_save_mode,
                    outputs=[lora_preset_mode, load_btn, save_btn]
                )
                
                # LoRA‰ΩøÁî®Áä∂ÊÖã„Å®„É¢„Éº„Éâ„ÅÆÂ§âÊõ¥„Åß„Éó„É™„Çª„ÉÉ„ÉàË°®Á§∫„ÇíÊõ¥Êñ∞
                use_lora.change(
                    toggle_lora_and_preset,
                    inputs=[use_lora, lora_mode],
                    outputs=[lora_preset_group]
                )
                
                lora_mode.change(
                    toggle_lora_and_preset,
                    inputs=[use_lora, lora_mode],
                    outputs=[lora_preset_group]
                )
            else:
                # LoRA„Çµ„Éù„Éº„Éà„Åå„Å™„ÅÑÂ†¥Âêà„ÅØ„ÉÄ„Éü„Éº
                lora_preset_group = gr.Group(visible=False)

            # FP8ÊúÄÈÅ©ÂåñË®≠ÂÆö„ÅØÈñãÂßã„ÉªÁµÇ‰∫Ü„Éú„Çø„É≥„ÅÆ‰∏ã„Å´ÁßªÂãïÊ∏à„Åø

            # Âüã„ÇÅËæº„Åø„Éó„É≠„É≥„Éó„Éà„Åä„Çà„Å≥„Ç∑„Éº„Éâ„ÇíË§áÂÜô„Åô„Çã„ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„ÇπÔºàLoRAË®≠ÂÆö„ÅÆ‰∏ã„Å´Ë°®Á§∫Ôºâ
            copy_metadata_visible = gr.Checkbox(
                label=translate("Âüã„ÇÅËæº„Åø„Éó„É≠„É≥„Éó„Éà„Åä„Çà„Å≥„Ç∑„Éº„Éâ„ÇíË§áÂÜô„Åô„Çã"),
                value=False,
                info=translate("„ÉÅ„Çß„ÉÉ„ÇØ„Çí„Ç™„É≥„Å´„Åô„Çã„Å®„ÄÅÁîªÂÉè„ÅÆ„É°„Çø„Éá„Éº„Çø„Åã„Çâ„Éó„É≠„É≥„Éó„Éà„Å®„Ç∑„Éº„Éâ„ÇíËá™ÂãïÁöÑ„Å´ÂèñÂæó„Åó„Åæ„Åô")
            )

            # Ë°®Á§∫Áî®„ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ„Å®ÂÆüÈöõ„ÅÆÂá¶ÁêÜÁî®„ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ„ÇíÂêåÊúü
            copy_metadata_visible.change(
                fn=lambda x: x,
                inputs=[copy_metadata_visible],
                outputs=[copy_metadata]
            )

            # ÂÖÉ„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ„ÅåÂ§âÊõ¥„Åï„Çå„Åü„Å®„Åç„ÇÇË°®Á§∫Áî®„ÇíÂêåÊúü
            copy_metadata.change(
                fn=lambda x: x,
                inputs=[copy_metadata],
                outputs=[copy_metadata_visible],
                queue=False  # È´òÈÄüÂåñ„ÅÆ„Åü„ÇÅ„Ç≠„É•„Éº„Çí„Çπ„Ç≠„ÉÉ„Éó
            )

            # „Éó„É≠„É≥„Éó„ÉàÂÖ•Âäõ
            prompt = gr.Textbox(label=translate("Prompt"), value=get_default_startup_prompt(), lines=6)

            # „Éó„É≠„É≥„Éó„ÉàÁÆ°ÁêÜ„Éë„Éç„É´„ÅÆËøΩÂä†
            with gr.Group(visible=True) as prompt_management:
                gr.Markdown(f"### " + translate("„Éó„É≠„É≥„Éó„ÉàÁÆ°ÁêÜ"))

                # Á∑®ÈõÜÁîªÈù¢„ÇíÂ∏∏ÊôÇË°®Á§∫„Åô„Çã
                with gr.Group(visible=True):
                    # Ëµ∑ÂãïÊôÇ„Éá„Éï„Ç©„É´„Éà„ÅÆÂàùÊúüË°®Á§∫Áî®„Å´ÂèñÂæó
                    default_prompt = ""
                    default_name = ""
                    for preset in load_presets()["presets"]:
                        if preset.get("is_startup_default", False):
                            default_prompt = preset["prompt"]
                            default_name = preset["name"]
                            break

                    with gr.Row():
                        edit_name = gr.Textbox(label=translate("„Éó„É™„Çª„ÉÉ„ÉàÂêç"), placeholder=translate("ÂêçÂâç„ÇíÂÖ•Âäõ..."), value=default_name)

                    edit_prompt = gr.Textbox(label=translate("„Éó„É≠„É≥„Éó„Éà"), lines=5, value=default_prompt)

                    with gr.Row():
                        # Ëµ∑ÂãïÊôÇ„Éá„Éï„Ç©„É´„Éà„Çí„Éá„Éï„Ç©„É´„ÉàÈÅ∏Êäû„Å´Ë®≠ÂÆö
                        default_preset = translate("Ëµ∑ÂãïÊôÇ„Éá„Éï„Ç©„É´„Éà")
                        # „Éó„É™„Çª„ÉÉ„Éà„Éá„Éº„Çø„Åã„ÇâÂÖ®„Éó„É™„Çª„ÉÉ„ÉàÂêç„ÇíÂèñÂæó
                        presets_data = load_presets()
                        choices = [preset["name"] for preset in presets_data["presets"]]
                        default_presets = [name for name in choices if any(p["name"] == name and p.get("is_default", False) for p in presets_data["presets"])]
                        user_presets = [name for name in choices if name not in default_presets]
                        sorted_choices, default_value = fix_prompt_preset_dropdown_initialization()
                        preset_dropdown = gr.Dropdown(
                            label=translate("„Éó„É™„Çª„ÉÉ„Éà"),
                            choices=sorted_choices,
                            value=default_value,
                            type="value"
                        )
                        # sorted_choices = [(name, name) for name in sorted(default_presets) + sorted(user_presets)]
                        # preset_dropdown = gr.Dropdown(label=translate("„Éó„É™„Çª„ÉÉ„Éà"), choices=sorted_choices, value=default_preset, type="value")

                    with gr.Row():
                        save_btn = gr.Button(value=translate("‰øùÂ≠ò"), variant="primary")
                        apply_preset_btn = gr.Button(value=translate("ÂèçÊò†"), variant="primary")
                        clear_btn = gr.Button(value=translate("„ÇØ„É™„Ç¢"))
                        delete_preset_btn = gr.Button(value=translate("ÂâäÈô§"))

                # „É°„ÉÉ„Çª„Éº„Ç∏Ë°®Á§∫Áî®
                result_message = gr.Markdown("")

            # „Éó„É™„Çª„ÉÉ„Éà„ÅÆË™¨ÊòéÊñá„ÇíÂâäÈô§

            # ‰∫íÊèõÊÄß„ÅÆ„Åü„ÇÅ„Å´Quick List„ÇÇÊÆã„Åó„Å¶„Åä„Åè„Åå„ÄÅÈùûË°®Á§∫„Å´„Åô„Çã
            with gr.Row(visible=False):
                example_quick_prompts = gr.Dataset(samples=quick_prompts, label=translate("Quick List"), samples_per_page=1000, components=[prompt])
                example_quick_prompts.click(lambda x: x[0], inputs=[example_quick_prompts], outputs=prompt, show_progress=False, queue=False)

            # ‰ª•‰∏ã„ÅÆË®≠ÂÆö„Éñ„É≠„ÉÉ„ÇØ„ÅØÂè≥„Ç´„É©„É†„Å´ÁßªÂãï„Åó„Åæ„Åó„Åü

                # „Çª„ÇØ„Ç∑„Éß„É≥Ë®≠ÂÆö„ÅÆ„É™„Çπ„Éà„ÅØÊó¢„Å´„Ç¢„Ç≥„Éº„Éá„Ç£„Ç™„É≥ÂÜÖ„ÅßÂàùÊúüÂåñ„Åï„Çå„Å¶„ÅÑ„Åæ„Åô
                # section_number_inputs
                # section_image_inputs
                # section_prompt_inputs
                # section_row_groups

                # collect_section_settingsÈñ¢Êï∞„ÅØÊú™‰ΩøÁî®„ÅÆ„Åü„ÇÅÂâäÈô§

                # „Ç∑„É≥„Éó„É´„Å™‰∫íÊèõÊÄß„ÅÆ„Åü„ÇÅ„ÅÆ„ÉÄ„Éü„Éº„Çπ„ÉÜ„Éº„Éà„Çí‰ΩúÊàê
                section_settings = gr.State([[None, None, ""] for _ in range(max_keyframes)])
                section_inputs = []

                # update_section_settingsÈñ¢Êï∞„ÅØÊú™‰ΩøÁî®„ÅÆ„Åü„ÇÅÂâäÈô§

                # „Éï„É¨„Éº„É†„Çµ„Ç§„Ç∫Â§âÊõ¥ÊôÇ„ÅÆÂá¶ÁêÜ„ÇíËøΩÂä†
                def update_section_calculation(frame_size, mode, length):
                    """„Éï„É¨„Éº„É†„Çµ„Ç§„Ç∫Â§âÊõ¥ÊôÇ„Å´„Çª„ÇØ„Ç∑„Éß„É≥Êï∞„ÇíÂÜçË®àÁÆó„Åó„Å¶Ë°®Á§∫„ÇíÊõ¥Êñ∞"""
                    # ÂãïÁîªÈï∑„ÇíÂèñÂæó
                    seconds = get_video_seconds(length)

                    # latent_window_size„ÇíË®≠ÂÆö
                    latent_window_size = 4.5 if frame_size == translate("0.5Áßí (17„Éï„É¨„Éº„É†)") else 9
                    frame_count = latent_window_size * 4 - 3

                    # „Çª„ÇØ„Ç∑„Éß„É≥Êï∞„ÇíË®àÁÆó
                    total_frames = int(seconds * 30)
                    total_sections = int(max(round(total_frames / frame_count), 1))

                    # Ë®àÁÆóË©≥Á¥∞„ÇíË°®Á§∫„Åô„ÇãHTML„ÇíÁîüÊàê
                    html = f"""<div style='padding: 10px; background-color: #f5f5f5; border-radius: 5px; font-size: 14px;'>
                    {translate('<strong>Ë®àÁÆóË©≥Á¥∞</strong>: „Éï„É¨„Éº„É†„Çµ„Ç§„Ç∫={0}, Á∑è„Éï„É¨„Éº„É†Êï∞={1}, „Çª„ÇØ„Ç∑„Éß„É≥„ÅÇ„Åü„Çä={2}„Éï„É¨„Éº„É†, ÂøÖË¶Å„Çª„ÇØ„Ç∑„Éß„É≥Êï∞={3}').format(frame_size, total_frames, frame_count, total_sections)}
                    <br>
                    {translate('ÂãïÁîª„É¢„Éº„Éâ {0} „Å®„Éï„É¨„Éº„É†„Çµ„Ç§„Ç∫ {1} „ÅßÂøÖË¶Å„Å™„Çª„ÇØ„Ç∑„Éß„É≥Êï∞: <strong>{2}</strong>').format(length, frame_size, total_sections)}
                    </div>"""

                    # „Çª„ÇØ„Ç∑„Éß„É≥Ë®àÁÆó„É≠„Ç∞
                    print(translate("Ë®àÁÆóÁµêÊûú: „É¢„Éº„Éâ=ÈÄöÂ∏∏, „Éï„É¨„Éº„É†„Çµ„Ç§„Ç∫={0}, latent_window_size={1}, Á∑è„Éï„É¨„Éº„É†Êï∞={2}, ÂøÖË¶Å„Çª„ÇØ„Ç∑„Éß„É≥Êï∞={3}").format(frame_size, latent_window_size, total_frames, total_sections))

                    return html

                # ÂàùÊúüÂåñÊôÇ„Å´„ÇÇË®àÁÆó„ÇíÂÆüË°å
                initial_html = update_section_calculation(frame_size_radio.value, mode_radio.value, length_radio.value)
                section_calc_display = gr.HTML(value=initial_html, label="")

                # „Éï„É¨„Éº„É†„Çµ„Ç§„Ç∫Â§âÊõ¥„Ç§„Éô„É≥„Éà - HTMLË°®Á§∫„ÅÆÊõ¥Êñ∞„Å®„Çª„ÇØ„Ç∑„Éß„É≥„Çø„Ç§„Éà„É´„ÅÆÊõ¥Êñ∞„ÇíË°å„ÅÜ
                frame_size_radio.change(
                    fn=update_section_calculation,
                    inputs=[frame_size_radio, mode_radio, length_radio],
                    outputs=[section_calc_display]
                )

                # „Çª„ÇØ„Ç∑„Éß„É≥Ë°®Á§∫Ê©üËÉΩ„Çí„Ç∑„É≥„Éó„É´Âåñ
                def update_section_visibility(mode, length, frame_size=None):
                    """F1„É¢„Éº„Éâ„Åß„ÅØ„Ç∑„É≥„Éó„É´Âåñ„Åï„Çå„ÅüÈñ¢Êï∞"""
                    # ÁßíÊï∞„Å†„ÅëË®àÁÆó„Åó„Å¶Ëøî„Åô
                    seconds = get_video_seconds(length)
                    print(translate("F1„É¢„Éº„ÉâÔºö„Ç∑„É≥„Éó„É´Ë®≠ÂÆöÔºà‰∏çË¶Å„Å™Ê©üËÉΩ„ÇíÂâäÈô§Ê∏à„ÅøÔºâ"))

                    # ÊúÄ‰ΩéÈôê„ÅÆËøîÂÄ§ÔºàÂÖ•Âäõ„Å´ÂØæÂøú„Åô„Çã„Å†„Åë„ÅÆÁ©∫Êõ¥Êñ∞Ôºâ
                    return [gr.update()] * 2 + [] + [gr.update(value=seconds)] + []

                # Ê≥®ÊÑè: „Åì„ÅÆÈñ¢Êï∞„ÅÆ„Ç§„Éô„É≥„ÉàÁôªÈå≤„ÅØ„ÄÅtotal_second_length„ÅÆUI„Ç≥„É≥„Éù„Éº„Éç„É≥„ÉàÂÆöÁæ©Âæå„Å´Ë°å„ÅÜ„Åü„ÇÅ„ÄÅ
                # „Åì„Åì„Åß„ÅØÈñ¢Êï∞„ÅÆÂÆöÁæ©„ÅÆ„ÅøË°å„ÅÑ„ÄÅÂÆüÈöõ„ÅÆ„Ç§„Éô„É≥„ÉàÁôªÈå≤„ÅØUI„Ç≥„É≥„Éù„Éº„Éç„É≥„ÉàÂÆöÁæ©Âæå„Å´Ë°å„ÅÑ„Åæ„Åô„ÄÇ

                # ÂãïÁîªÈï∑Â§âÊõ¥„Ç§„Éô„É≥„Éà„Åß„ÇÇ„Çª„ÇØ„Ç∑„Éß„É≥Êï∞Ë®àÁÆó„ÇíÊõ¥Êñ∞
                length_radio.change(
                    fn=update_section_calculation,
                    inputs=[frame_size_radio, mode_radio, length_radio],
                    outputs=[section_calc_display]
                )

                # F1„É¢„Éº„Éâ„Åß„ÅØ„Çª„ÇØ„Ç∑„Éß„É≥„Çø„Ç§„Éà„É´„ÅØ‰∏çË¶Å

                # „É¢„Éº„ÉâÂ§âÊõ¥ÊôÇ„Å´„ÇÇË®àÁÆó„ÇíÊõ¥Êñ∞
                mode_radio.change(
                    fn=update_section_calculation,
                    inputs=[frame_size_radio, mode_radio, length_radio],
                    outputs=[section_calc_display]
                )

                # F1„É¢„Éº„Éâ„Åß„ÅØ„Çª„ÇØ„Ç∑„Éß„É≥„Çø„Ç§„Éà„É´„ÅØ‰∏çË¶Å

                # „É¢„Éº„ÉâÂ§âÊõ¥ÊôÇ„ÅÆÂá¶ÁêÜ„ÇÇtotal_second_length„Ç≥„É≥„Éù„Éº„Éç„É≥„ÉàÂÆöÁæ©Âæå„Å´Ë°å„ÅÑ„Åæ„Åô

                # ÂãïÁîªÈï∑Â§âÊõ¥ÊôÇ„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥Ë°®Á§∫Êõ¥Êñ∞„ÇÇtotal_second_length„Ç≥„É≥„Éù„Éº„Éç„É≥„ÉàÂÆöÁæ©Âæå„Å´Ë°å„ÅÑ„Åæ„Åô

                # F1„É¢„Éº„Éâ„Åß„ÅØÁµÇÁ´Ø„Éï„É¨„Éº„É†„Å®„É´„Éº„Éó„É¢„Éº„ÉâÈñ¢ÈÄ£„ÅÆÊ©üËÉΩ„Çí„Åô„Åπ„Å¶ÂâäÈô§

                # „Ç≠„Éº„Éï„É¨„Éº„É†Âá¶ÁêÜÈñ¢Êï∞„Å®Zip„Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„ÉâÂá¶ÁêÜÈñ¢Êï∞„ÅØÊú™‰ΩøÁî®„ÅÆ„Åü„ÇÅÂâäÈô§


        with gr.Column():
            result_video = gr.Video(
                label=translate("Finished Frames"),
                key="result_video",
                autoplay=True,
                show_share_button=False,
                height=512,
                loop=True,
                format="mp4",
                interactive=False,
            )
            progress_desc = gr.Markdown('', elem_classes='no-generating-animation')
            progress_bar = gr.HTML('', elem_classes='no-generating-animation')
            preview_image = gr.Image(label="Next Latents", height=200, visible=False)

            # „Éï„É¨„Éº„É†„Çµ„Ç§„Ç∫ÂàáÊõøÁî®„ÅÆUI„Ç≥„É≥„Éà„É≠„Éº„É´„ÅØ‰∏äÈÉ®„Å´ÁßªÂãï„Åó„Åü„Åü„ÇÅÂâäÈô§

            # Ë®àÁÆóÁµêÊûú„ÇíË°®Á§∫„Åô„Çã„Ç®„É™„Ç¢
            section_calc_display = gr.HTML("", label="")

            use_teacache = gr.Checkbox(
                label=translate('Use TeaCache'), 
                value=saved_app_settings.get("use_teacache", True) if saved_app_settings else True, 
                info=translate('Faster speed, but often makes hands and fingers slightly worse.'),
                elem_classes="saveable-setting"
            )

            # Use Random Seed„ÅÆÂàùÊúüÂÄ§
            use_random_seed_default = True
            seed_default = random.randint(0, 2**32 - 1) if use_random_seed_default else 1

            use_random_seed = gr.Checkbox(label=translate("Use Random Seed"), value=use_random_seed_default)

            n_prompt = gr.Textbox(label=translate("Negative Prompt"), value="", visible=False)  # Not used
            seed = gr.Number(label=translate("Seed"), value=seed_default, precision=0)

            # „Åì„Åì„Åß„ÄÅ„É°„Çø„Éá„Éº„ÇøÂèñÂæóÂá¶ÁêÜ„ÅÆÁôªÈå≤„ÇíÁßªÂãï„Åô„Çã
            # „Åì„Åì„Åß„ÅØ„ÄÅprompt„Å®seed„ÅÆ‰∏°Êñπ„ÅåÂÆöÁæ©Ê∏à„Åø
            input_image.change(
                fn=update_from_image_metadata,
                inputs=[input_image, copy_metadata],
                outputs=[prompt, seed]
            )

            # „ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ„ÅÆÂ§âÊõ¥ÊôÇ„Å´ÂÜçË™≠„ÅøËæº„Åø„ÇíË°å„ÅÜ
            def check_metadata_on_checkbox_change(copy_enabled, image_path):
                if not copy_enabled or image_path is None:
                    return [gr.update()] * 2
                # „ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ„Ç™„É≥ÊôÇ„Å´„ÄÅÁîªÂÉè„Åå„ÅÇ„Çå„Å∞ÂÜçÂ∫¶„É°„Çø„Éá„Éº„Çø„ÇíË™≠„ÅøËæº„ÇÄ
                return update_from_image_metadata(image_path, copy_enabled)

            # update_section_metadata_on_checkbox_changeÈñ¢Êï∞„ÅØÊú™‰ΩøÁî®„ÅÆ„Åü„ÇÅÂâäÈô§

            copy_metadata.change(
                fn=check_metadata_on_checkbox_change,
                inputs=[copy_metadata, input_image],
                outputs=[prompt, seed]
            )


            def set_random_seed(is_checked):
                if is_checked:
                    return random.randint(0, 2**32 - 1)
                else:
                    return gr.update()
            use_random_seed.change(fn=set_random_seed, inputs=use_random_seed, outputs=seed)

            total_second_length = gr.Slider(label=translate("Total Video Length (Seconds)"), minimum=1, maximum=120, value=1, step=1)
            latent_window_size = gr.Slider(label=translate("Latent Window Size"), minimum=1, maximum=33, value=9, step=1, visible=False)  # Should not change
            steps = gr.Slider(
                label=translate("Steps"), 
                minimum=1, 
                maximum=100, 
                value=saved_app_settings.get("steps", 25) if saved_app_settings else 25, 
                step=1, 
                info=translate('Changing this value is not recommended.'),
                elem_classes="saveable-setting"
            )

            cfg = gr.Slider(
                label=translate("CFG Scale"), 
                minimum=1.0, 
                maximum=32.0, 
                value=saved_app_settings.get("cfg", 1.0) if saved_app_settings else 1.0, 
                step=0.01, 
                visible=False,  # Should not change
                elem_classes="saveable-setting"
            )
            gs = gr.Slider(
                label=translate("Distilled CFG Scale"), 
                minimum=1.0, 
                maximum=32.0, 
                value=saved_app_settings.get("gs", 10) if saved_app_settings else 10, 
                step=0.01, 
                info=translate('Changing this value is not recommended.'),
                elem_classes="saveable-setting"
            )
            rs = gr.Slider(label=translate("CFG Re-Scale"), minimum=0.0, maximum=1.0, value=0.0, step=0.01, visible=False)  # Should not change

            available_cuda_memory_gb = round(torch.cuda.get_device_properties(0).total_memory / (1024**3))
            default_gpu_memory_preservation_gb = 6 if available_cuda_memory_gb >= 20 else (8 if available_cuda_memory_gb > 16 else 10)
            gpu_memory_preservation = gr.Slider(label=translate("GPU Memory to Preserve (GB) (smaller = more VRAM usage)"), minimum=6, maximum=128, value=saved_app_settings.get("gpu_memory_preservation", default_gpu_memory_preservation_gb) if saved_app_settings else default_gpu_memory_preservation_gb, step=0.1, info=translate("Á©∫„Åë„Å¶„Åä„ÅèGPU„É°„É¢„É™Èáè„ÇíÊåáÂÆö„ÄÇÂ∞è„Åï„ÅÑÂÄ§=„Çà„ÇäÂ§ö„Åè„ÅÆVRAM„Çí‰ΩøÁî®ÂèØËÉΩ=È´òÈÄü„ÄÅÂ§ß„Åç„ÅÑÂÄ§=„Çà„ÇäÂ∞ë„Å™„ÅÑVRAM„Çí‰ΩøÁî®=ÂÆâÂÖ®"), elem_classes="saveable-setting")

            # MP4ÂúßÁ∏ÆË®≠ÂÆö„Çπ„É©„Ç§„ÉÄ„Éº„ÇíËøΩÂä†
            mp4_crf = gr.Slider(
                label=translate("MP4 Compression"), 
                minimum=0, 
                maximum=100, 
                value=saved_app_settings.get("mp4_crf", 16) if saved_app_settings else 16, 
                step=1, 
                info=translate("Êï∞ÂÄ§„ÅåÂ∞è„Åï„ÅÑ„Åª„Å©È´òÂìÅË≥™„Å´„Å™„Çä„Åæ„Åô„ÄÇ0„ÅØÁÑ°ÂúßÁ∏Æ„ÄÇÈªíÁîªÈù¢„ÅåÂá∫„ÇãÂ†¥Âêà„ÅØ16„Å´Ë®≠ÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"),
                elem_classes="saveable-setting"
            )

            # „Çª„ÇØ„Ç∑„Éß„É≥„Åî„Å®„ÅÆÂãïÁîª‰øùÂ≠ò„ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ„ÇíËøΩÂä†Ôºà„Éá„Éï„Ç©„É´„ÉàOFFÔºâ
            keep_section_videos = gr.Checkbox(label=translate("ÂÆå‰∫ÜÊôÇ„Å´„Çª„ÇØ„Ç∑„Éß„É≥„Åî„Å®„ÅÆÂãïÁîª„ÇíÊÆã„Åô - „ÉÅ„Çß„ÉÉ„ÇØ„Åå„Å™„ÅÑÂ†¥Âêà„ÅØÊúÄÁµÇÂãïÁîª„ÅÆ„Åø‰øùÂ≠ò„Åï„Çå„Åæ„ÅôÔºà„Éá„Éï„Ç©„É´„ÉàOFFÔºâ"), value=saved_app_settings.get("keep_section_videos", False) if saved_app_settings else False, elem_classes="saveable-setting")

            # „ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø‰øùÂ≠ò„ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„ÇπËøùÂä†
            save_tensor_data = gr.Checkbox(
                label=translate("ÂÆå‰∫ÜÊôÇ„Å´„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø(.safetensors)„ÇÇ‰øùÂ≠ò - „Åì„ÅÆ„Éá„Éº„Çø„ÇíÂà•„ÅÆÂãïÁîª„ÅÆÂæå„Å´ÁµêÂêàÂèØËÉΩ"),
                value=saved_app_settings.get("save_tensor_data", False) if saved_app_settings else False,
                info=translate("„ÉÅ„Çß„ÉÉ„ÇØ„Åô„Çã„Å®„ÄÅÁîüÊàê„Åï„Çå„Åü„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„Çí‰øùÂ≠ò„Åó„Åæ„Åô„ÄÇ„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åï„Çå„Åü„ÉÜ„É≥„ÇΩ„É´„Åå„ÅÇ„Çå„Å∞„ÄÅÁµêÂêà„Åó„Åü„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„ÇÇ‰øùÂ≠ò„Åï„Çå„Åæ„Åô„ÄÇ"),
                elem_classes="saveable-setting"
            )

            # „Çª„ÇØ„Ç∑„Éß„É≥„Åî„Å®„ÅÆÈùôÊ≠¢Áîª‰øùÂ≠ò„ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ„ÇíËøΩÂä†Ôºà„Éá„Éï„Ç©„É´„ÉàOFFÔºâ
            save_section_frames = gr.Checkbox(label=translate("Save Section Frames"), value=saved_app_settings.get("save_section_frames", False) if saved_app_settings else False, info=translate("ÂêÑ„Çª„ÇØ„Ç∑„Éß„É≥„ÅÆÊúÄÁµÇ„Éï„É¨„Éº„É†„ÇíÈùôÊ≠¢Áîª„Å®„Åó„Å¶‰øùÂ≠ò„Åó„Åæ„ÅôÔºà„Éá„Éï„Ç©„É´„ÉàOFFÔºâ"), elem_classes="saveable-setting")
            
            # „Éï„É¨„Éº„É†ÁîªÂÉè‰øùÂ≠ò„ÅÆ„É©„Ç∏„Ç™„Éú„Çø„É≥„ÇíËøΩÂä†Ôºà„Éá„Éï„Ç©„É´„Éà„ÅØ„Äå‰øùÂ≠ò„Åó„Å™„ÅÑ„ÄçÔºâ
            # gr.Group„ÅßÂõ≤„ÇÄ„Åì„Å®„ÅßÁÅ∞Ëâ≤ËÉåÊôØ„ÅÆ„Çπ„Çø„Ç§„É´„Å´Áµ±‰∏Ä
            with gr.Group():
                gr.Markdown(f"### " + translate("„Éï„É¨„Éº„É†ÁîªÂÉè‰øùÂ≠òË®≠ÂÆö"))
                frame_save_mode = gr.Radio(
                    label=translate("„Éï„É¨„Éº„É†ÁîªÂÉè‰øùÂ≠ò„É¢„Éº„Éâ"),
                    choices=[
                        translate("‰øùÂ≠ò„Åó„Å™„ÅÑ"),
                        translate("ÂÖ®„Éï„É¨„Éº„É†ÁîªÂÉè‰øùÂ≠ò"),
                        translate("ÊúÄÁµÇ„Çª„ÇØ„Ç∑„Éß„É≥„ÅÆ„ÅøÂÖ®„Éï„É¨„Éº„É†ÁîªÂÉè‰øùÂ≠ò")
                    ],
                    # value=saved_app_settings.get("frame_save_mode", translate("‰øùÂ≠ò„Åó„Å™„ÅÑ")) if saved_app_settings else translate("‰øùÂ≠ò„Åó„Å™„ÅÑ"),
                    value=translate(saved_app_settings.get("frame_save_mode", "‰øùÂ≠ò„Åó„Å™„ÅÑ") if saved_app_settings else "‰øùÂ≠ò„Åó„Å™„ÅÑ"),
                    info=translate("„Éï„É¨„Éº„É†ÁîªÂÉè„ÅÆ‰øùÂ≠òÊñπÊ≥ï„ÇíÈÅ∏Êäû„Åó„Åæ„Åô„ÄÇÈÅéÂéª„Çª„ÇØ„Ç∑„Éß„É≥ÂàÜ„ÇÇÂê´„ÇÅ„Å¶‰øùÂ≠ò„Åó„Åæ„Åô„ÄÇÂÖ®„Çª„ÇØ„Ç∑„Éß„É≥„ÅãÊúÄÁµÇ„Çª„ÇØ„Ç∑„Éß„É≥„ÅÆ„Åø„ÅãÈÅ∏Êäû„Åß„Åç„Åæ„Åô„ÄÇ"),
                    elem_classes="saveable-setting"
                )

            # UI„Ç≥„É≥„Éù„Éº„Éç„É≥„ÉàÂÆöÁæ©Âæå„ÅÆ„Ç§„Éô„É≥„ÉàÁôªÈå≤
            # F1„É¢„Éº„Éâ„Åß„ÅØ„Çª„ÇØ„Ç∑„Éß„É≥Ê©üËÉΩ„ÇíÂâäÈô§Ê∏à„Åø - „Ç∑„É≥„Éó„É´Âåñ„Åó„Åü„Ç§„Éô„É≥„Éà„Éè„É≥„Éâ„É©
            mode_radio.change(
                fn=update_section_visibility,
                inputs=[mode_radio, length_radio, frame_size_radio],
                outputs=[input_image, input_image, total_second_length]
            )

            # „Éï„É¨„Éº„É†„Çµ„Ç§„Ç∫Â§âÊõ¥ÊôÇ„ÅÆÂá¶ÁêÜÔºà„Ç∑„É≥„Éó„É´ÂåñÔºâ
            frame_size_radio.change(
                fn=update_section_visibility,
                inputs=[mode_radio, length_radio, frame_size_radio],
                outputs=[input_image, input_image, total_second_length]
            )

            # ÂãïÁîªÈï∑Â§âÊõ¥ÊôÇ„ÅÆÂá¶ÁêÜÔºà„Ç∑„É≥„Éó„É´ÂåñÔºâ
            length_radio.change(
                fn=update_section_visibility,
                inputs=[mode_radio, length_radio, frame_size_radio],
                outputs=[input_image, input_image, total_second_length]
            )


            # ImageÂΩ±ÈüøÂ∫¶Ë™øÊï¥„Çπ„É©„Ç§„ÉÄ„Éº
            with gr.Group():
                gr.Markdown("### " + translate("ImageÂΩ±ÈüøÂ∫¶Ë™øÊï¥"))
                image_strength = gr.Slider(
                    label=translate("ImageÂΩ±ÈüøÂ∫¶"),
                    minimum=1.00,
                    maximum=1.02,
                    value=saved_app_settings.get("image_strength", 1.00) if saved_app_settings else 1.00,
                    step=0.001,
                    info=translate("ÈñãÂßã„Éï„É¨„Éº„É†(Image)„ÅåÂãïÁîª„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÅÆÂº∑„Åï„ÇíË™øÊï¥„Åó„Åæ„Åô„ÄÇ1.00„ÅåÈÄöÂ∏∏„ÅÆÂãï‰ΩúÔºà100%Ôºâ„Åß„Åô„ÄÇÂÄ§„ÇíÂ§ß„Åç„Åè„Åô„Çã„Å®ÂßãÁÇπ„ÅÆÂΩ±Èüø„ÅåÂº∑„Åæ„Çä„ÄÅÂ§âÂåñ„ÅåÂ∞ë„Å™„Åè„Å™„Çä„Åæ„Åô„ÄÇ100%-102%„ÅÆÁØÑÂõ≤„Åß0.1%Âàª„Åø„ÅÆÂæÆË™øÊï¥„ÅåÂèØËÉΩ„Åß„Åô„ÄÇ"),
                    elem_classes="saveable-setting"
                )

            # Âá∫Âäõ„Éï„Ç©„É´„ÉÄË®≠ÂÆö
            gr.Markdown(translate("‚Äª Âá∫ÂäõÂÖà„ÅØ `webui` ÈÖç‰∏ã„Å´ÈôêÂÆö„Åï„Çå„Åæ„Åô"))
            with gr.Row(equal_height=True):
                with gr.Column(scale=4):
                    # „Éï„Ç©„É´„ÉÄÂêç„Å†„Åë„ÇíÂÖ•ÂäõÊ¨Ñ„Å´Ë®≠ÂÆö
                    output_dir = gr.Textbox(
                        label=translate("Âá∫Âäõ„Éï„Ç©„É´„ÉÄÂêç"),
                        value=output_folder_name,  # Ë®≠ÂÆö„Åã„ÇâË™≠„ÅøËæº„Çì„Å†ÂÄ§„Çí‰ΩøÁî®
                        info=translate("ÂãïÁîª„ÇÑ„Ç≠„Éº„Éï„É¨„Éº„É†ÁîªÂÉè„ÅÆ‰øùÂ≠òÂÖà„Éï„Ç©„É´„ÉÄÂêç"),
                        placeholder="outputs"
                    )
                with gr.Column(scale=1, min_width=100):
                    open_folder_btn = gr.Button(value=translate("üìÇ ‰øùÂ≠ò„Åä„Çà„Å≥Âá∫Âäõ„Éï„Ç©„É´„ÉÄ„ÇíÈñã„Åè"), size="sm")

            # ÂÆüÈöõ„ÅÆÂá∫Âäõ„Éë„Çπ„ÇíË°®Á§∫
            with gr.Row(visible=False):
                path_display = gr.Textbox(
                    label=translate("Âá∫Âäõ„Éï„Ç©„É´„ÉÄ„ÅÆÂÆåÂÖ®„Éë„Çπ"),
                    value=os.path.join(base_path, output_folder_name),
                    interactive=False
                )

            # „Éï„Ç©„É´„ÉÄ„ÇíÈñã„Åè„Éú„Çø„É≥„ÅÆ„Ç§„Éô„É≥„Éà
            def handle_open_folder_btn(folder_name):
                """„Éï„Ç©„É´„ÉÄÂêç„Çí‰øùÂ≠ò„Åó„ÄÅ„Åù„ÅÆ„Éï„Ç©„É´„ÉÄ„ÇíÈñã„Åè"""
                if not folder_name or not folder_name.strip():
                    folder_name = "outputs"

                # „Éï„Ç©„É´„ÉÄ„Éë„Çπ„ÇíÂèñÂæó
                folder_path = get_output_folder_path(folder_name)

                # Ë®≠ÂÆö„ÇíÊõ¥Êñ∞„Åó„Å¶‰øùÂ≠ò
                settings = load_settings()
                old_folder_name = settings.get('output_folder')

                if old_folder_name != folder_name:
                    settings['output_folder'] = folder_name
                    save_result = save_settings(settings)
                    if save_result:
                        # „Ç∞„É≠„Éº„Éê„É´Â§âÊï∞„ÇÇÊõ¥Êñ∞
                        global output_folder_name, outputs_folder
                        output_folder_name = folder_name
                        outputs_folder = folder_path
                    print(translate("Âá∫Âäõ„Éï„Ç©„É´„ÉÄË®≠ÂÆö„Çí‰øùÂ≠ò„Åó„Åæ„Åó„Åü: {folder_name}").format(folder_name=folder_name))

                # „Éï„Ç©„É´„ÉÄ„ÇíÈñã„Åè
                open_output_folder(folder_path)

                # Âá∫Âäõ„Éá„Ç£„É¨„ÇØ„Éà„É™ÂÖ•ÂäõÊ¨Ñ„Å®„Éë„ÇπË°®Á§∫„ÇíÊõ¥Êñ∞
                return gr.update(value=folder_name), gr.update(value=folder_path)

            open_folder_btn.click(fn=handle_open_folder_btn, inputs=[output_dir], outputs=[output_dir, path_display])

            # „Éó„É≠„É≥„Éó„ÉàÁÆ°ÁêÜ„Éë„Éç„É´ÔºàÂè≥„Ç´„É©„É†„Åã„ÇâÂ∑¶„Ç´„É©„É†„Å´ÁßªÂãïÊ∏à„ÅøÔºâ
            
            # „Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥Ë®≠ÂÆöÁÆ°ÁêÜUI
            with gr.Group():
                gr.Markdown(f"### " + translate("„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥Ë®≠ÂÆö"))
                with gr.Row():
                    with gr.Column(scale=1):
                        save_current_settings_btn = gr.Button(value=translate("üíæ ÁèæÂú®„ÅÆË®≠ÂÆö„Çí‰øùÂ≠ò"), size="sm")
                    with gr.Column(scale=1):
                        reset_settings_btn = gr.Button(value=translate("üîÑ Ë®≠ÂÆö„Çí„É™„Çª„ÉÉ„Éà"), size="sm")
                
                # Ëá™Âãï‰øùÂ≠òË®≠ÂÆö
                save_settings_default_value = saved_app_settings.get("save_settings_on_start", False) if saved_app_settings else False
                save_settings_on_start = gr.Checkbox(
                    label=translate("ÁîüÊàêÈñãÂßãÊôÇ„Å´Ëá™Âãï‰øùÂ≠ò"),
                    value=save_settings_default_value,
                    info=translate("„ÉÅ„Çß„ÉÉ„ÇØ„Çí„Ç™„É≥„Å´„Åô„Çã„Å®„ÄÅÁîüÊàêÈñãÂßãÊôÇ„Å´ÁèæÂú®„ÅÆË®≠ÂÆö„ÅåËá™ÂãïÁöÑ„Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åô„ÄÇË®≠ÂÆö„ÅØÂÜçËµ∑ÂãïÊôÇ„Å´ÂèçÊò†„Åï„Çå„Åæ„Åô„ÄÇ"),
                    elem_classes="saveable-setting",
                    interactive=True
                )
                
                # ÂÆå‰∫ÜÊôÇ„ÅÆ„Ç¢„É©„Éº„É†Ë®≠ÂÆö
                alarm_default_value = saved_app_settings.get("alarm_on_completion", True) if saved_app_settings else True
                alarm_on_completion = gr.Checkbox(
                    label=translate("ÂÆå‰∫ÜÊôÇ„Å´„Ç¢„É©„Éº„É†„ÇíÈ≥¥„Çâ„Åô(Windows)"),
                    value=alarm_default_value,
                    info=translate("„ÉÅ„Çß„ÉÉ„ÇØ„Çí„Ç™„É≥„Å´„Åô„Çã„Å®„ÄÅÁîüÊàêÂÆå‰∫ÜÊôÇ„Å´„Ç¢„É©„Éº„É†Èü≥„ÇíÈ≥¥„Çâ„Åó„Åæ„ÅôÔºàWindowsÔºâ"),
                    elem_classes="saveable-setting",
                    interactive=True
                )
                
                # „É≠„Ç∞Ë®≠ÂÆö
                gr.Markdown("### " + translate("„É≠„Ç∞Ë®≠ÂÆö"))
                
                # Ë®≠ÂÆö„Åã„Çâ„É≠„Ç∞Ë®≠ÂÆö„ÇíË™≠„ÅøËæº„ÇÄ
                all_settings = load_settings()
                log_settings = all_settings.get('log_settings', {'log_enabled': False, 'log_folder': 'logs'})
                
                # „É≠„Ç∞ÊúâÂäπ/ÁÑ°ÂäπË®≠ÂÆö
                log_enabled = gr.Checkbox(
                    label=translate("„Ç≥„É≥„ÇΩ„Éº„É´„É≠„Ç∞„ÇíÂá∫Âäõ„Åô„Çã"),
                    value=log_settings.get('log_enabled', False),
                    info=translate("„ÉÅ„Çß„ÉÉ„ÇØ„Çí„Ç™„É≥„Å´„Åô„Çã„Å®„ÄÅ„Ç≥„É≥„ÇΩ„Éº„É´Âá∫Âäõ„Çí„É≠„Ç∞„Éï„Ç°„Ç§„É´„Å´„ÇÇ‰øùÂ≠ò„Åó„Åæ„Åô"),
                    elem_classes="saveable-setting",
                    interactive=True
                )
                
                # „É≠„Ç∞Âá∫ÂäõÂÖàË®≠ÂÆö
                log_folder = gr.Textbox(
                    label=translate("„É≠„Ç∞Âá∫ÂäõÂÖà"),
                    value=log_settings.get('log_folder', 'logs'),
                    info=translate("„É≠„Ç∞„Éï„Ç°„Ç§„É´„ÅÆ‰øùÂ≠òÂÖà„Éï„Ç©„É´„ÉÄ„ÇíÊåáÂÆö„Åó„Åæ„Åô"),
                    elem_classes="saveable-setting",
                    interactive=True
                )
                
                # „É≠„Ç∞„Éï„Ç©„É´„ÉÄ„ÇíÈñã„Åè„Éú„Çø„É≥
                open_log_folder_btn = gr.Button(value=translate("üìÇ „É≠„Ç∞„Éï„Ç©„É´„ÉÄ„ÇíÈñã„Åè"), size="sm")
                
                # „É≠„Ç∞„Éï„Ç©„É´„ÉÄ„ÇíÈñã„Åè„Éú„Çø„É≥„ÅÆ„ÇØ„É™„ÉÉ„ÇØ„Ç§„Éô„É≥„Éà
                open_log_folder_btn.click(fn=open_log_folder)
                
                # Ë®≠ÂÆöÁä∂ÊÖã„ÅÆË°®Á§∫
                settings_status = gr.Markdown("")
            
            # „Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥Ë®≠ÂÆö„ÅÆ‰øùÂ≠òÊ©üËÉΩ
            def save_app_settings_handler(
                # Basic settings
                resolution_val,
                mp4_crf_val,
                steps_val,
                cfg_val,
                # Performance settings
                use_teacache_val,
                gpu_memory_preservation_val,
                # Detail settings
                gs_val,
                # F1 specific settings
                image_strength_val,
                # Save settings
                keep_section_videos_val,
                save_section_frames_val,
                save_tensor_data_val,
                frame_save_mode_val,
                # Auto-save settings
                save_settings_on_start_val,
                alarm_on_completion_val,
                # Log settings
                log_enabled_val,
                log_folder_val,
                # ADD NEW CONFIG QUEUE SETTING
                add_timestamp_to_config_val
            ):
                """ÁèæÂú®„ÅÆË®≠ÂÆö„Çí‰øùÂ≠ò"""
                from eichi_utils.settings_manager import save_app_settings_f1
                
                # „Ç¢„Éó„É™Ë®≠ÂÆöÁî®„ÅÆË®≠ÂÆöËæûÊõ∏„Çí‰ΩúÊàê
                current_settings = {
                    # Âü∫Êú¨Ë®≠ÂÆö
                    "resolution": resolution_val,
                    "mp4_crf": mp4_crf_val,
                    "steps": steps_val,
                    "cfg": cfg_val,
                    # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπË®≠ÂÆö
                    "use_teacache": use_teacache_val,
                    "gpu_memory_preservation": gpu_memory_preservation_val,
                    # Ë©≥Á¥∞Ë®≠ÂÆö
                    "gs": gs_val,
                    # F1Áã¨Ëá™Ë®≠ÂÆö
                    "image_strength": image_strength_val,
                    # ‰øùÂ≠òË®≠ÂÆö
                    "keep_section_videos": keep_section_videos_val,
                    "save_section_frames": save_section_frames_val,
                    "save_tensor_data": save_tensor_data_val,
                    "frame_save_mode": frame_save_mode_val,
                    # Ëá™Âãï‰øùÂ≠ò„Éª„Ç¢„É©„Éº„É†Ë®≠ÂÆö
                    "save_settings_on_start": save_settings_on_start_val,
                    "alarm_on_completion": alarm_on_completion_val,
                    # CONFIG QUEUEË®≠ÂÆö - NEW
                    "add_timestamp_to_config": bool(add_timestamp_to_config_val)
                }
                
                # „Ç¢„Éó„É™Ë®≠ÂÆö„Çí‰øùÂ≠ò
                try:
                    app_success = save_app_settings_f1(current_settings)
                except Exception as e:
                    return translate("Ë®≠ÂÆö„ÅÆ‰øùÂ≠ò„Å´Â§±Êïó„Åó„Åæ„Åó„Åü: {0}").format(str(e))
                
                # „É≠„Ç∞Ë®≠ÂÆö„ÇÇ‰øùÂ≠ò - ÂÄ§„ÅÆÂûã„ÇíÁ¢∫Ë™ç
                # log_enabled„ÅØboolean„Å´Á¢∫ÂÆü„Å´Â§âÊèõ
                is_log_enabled = False
                if isinstance(log_enabled_val, bool):
                    is_log_enabled = log_enabled_val
                elif hasattr(log_enabled_val, 'value'):
                    is_log_enabled = bool(log_enabled_val.value)
                
                # log_folder„ÅØÊñáÂ≠óÂàó„Å´Á¢∫ÂÆü„Å´Â§âÊèõ
                log_folder_path = "logs"
                if log_folder_val and isinstance(log_folder_val, str):
                    log_folder_path = log_folder_val
                elif hasattr(log_folder_val, 'value') and log_folder_val.value:
                    log_folder_path = str(log_folder_val.value)
                
                log_settings = {
                    "log_enabled": is_log_enabled,
                    "log_folder": log_folder_path
                }
                
                # ÂÖ®‰ΩìË®≠ÂÆö„ÇíÂèñÂæó„Åó„ÄÅ„É≠„Ç∞Ë®≠ÂÆö„ÇíÊõ¥Êñ∞
                all_settings = load_settings()
                all_settings['log_settings'] = log_settings
                log_success = save_settings(all_settings)
                
                # „É≠„Ç∞Ë®≠ÂÆö„ÇíÈÅ©Áî®ÔºàË®≠ÂÆö‰øùÂ≠òÂæå„ÄÅ„Åô„Åê„Å´Êñ∞„Åó„ÅÑ„É≠„Ç∞Ë®≠ÂÆö„ÇíÂèçÊò†Ôºâ
                if log_success:
                    # ‰∏ÄÊó¶„É≠„Ç∞„ÇíÁÑ°ÂäπÂåñ
                    disable_logging()
                    # Êñ∞„Åó„ÅÑË®≠ÂÆö„Åß„É≠„Ç∞„ÇíÂÜçÈñãÔºàÊúâÂäπ„Å™Â†¥ÂêàÔºâ
                    apply_log_settings(log_settings, source_name="endframe_ichi_f1")
                    print(translate("„É≠„Ç∞Ë®≠ÂÆö„ÇíÊõ¥Êñ∞„Åó„Åæ„Åó„Åü: ÊúâÂäπ={0}, „Éï„Ç©„É´„ÉÄ={1}").format(
                        log_enabled_val, log_folder_val))
                
                if app_success and log_success:
                    return translate("Ë®≠ÂÆö„Çí‰øùÂ≠ò„Åó„Åæ„Åó„Åü")
                else:
                    return translate("Ë®≠ÂÆö„ÅÆ‰∏ÄÈÉ®‰øùÂ≠ò„Å´Â§±Êïó„Åó„Åæ„Åó„Åü")

            def reset_app_settings_handler():
                """Ë®≠ÂÆö„Çí„Éá„Éï„Ç©„É´„Éà„Å´Êàª„Åô"""
                from eichi_utils.settings_manager import get_default_app_settings_f1
                from locales import i18n
                
                # ÁèæÂú®„ÅÆË®ÄË™ûË®≠ÂÆö„ÇíÂèñÂæó„Åó„Å¶„ÄÅ„Åù„ÅÆË®ÄË™ûÁî®„ÅÆ„Éá„Éï„Ç©„É´„ÉàË®≠ÂÆö„ÇíÂèñÂæó
                current_lang = i18n.lang
                
                # Ë®ÄË™ûË®≠ÂÆö„ÇíËÄÉÊÖÆ„Åó„Åü„Éá„Éï„Ç©„É´„ÉàË®≠ÂÆö„ÇíÂèñÂæó
                default_settings = get_default_app_settings_f1(current_lang)
                updates = []
                
                # ÂêÑUI„Ç≥„É≥„Éù„Éº„Éç„É≥„Éà„ÅÆ„Éá„Éï„Ç©„É´„ÉàÂÄ§„ÇíË®≠ÂÆöÔºàF1„ÅÆÈ†ÜÂ∫è„Å´Âêà„Çè„Åõ„ÇãÔºâ
                updates.append(gr.update(value=default_settings.get("resolution", 640)))  # 1
                updates.append(gr.update(value=default_settings.get("mp4_crf", 16)))  # 2
                updates.append(gr.update(value=default_settings.get("steps", 25)))  # 3
                updates.append(gr.update(value=default_settings.get("cfg", 1.0)))  # 4
                updates.append(gr.update(value=default_settings.get("use_teacache", True)))  # 5
                updates.append(gr.update(value=default_settings.get("gpu_memory_preservation", 6)))  # 6
                updates.append(gr.update(value=default_settings.get("gs", 10)))  # 7
                # F1Áã¨Ëá™
                updates.append(gr.update(value=default_settings.get("image_strength", 1.0)))  # 8
                updates.append(gr.update(value=default_settings.get("keep_section_videos", False)))  # 9
                updates.append(gr.update(value=default_settings.get("save_section_frames", False)))  # 10
                updates.append(gr.update(value=default_settings.get("save_tensor_data", False)))  # 11
                updates.append(gr.update(value=default_settings.get("frame_save_mode", translate("‰øùÂ≠ò„Åó„Å™„ÅÑ"))))  # 12
                updates.append(gr.update(value=default_settings.get("save_settings_on_start", False)))  # 13
                updates.append(gr.update(value=default_settings.get("alarm_on_completion", True)))  # 14
                
                # „É≠„Ç∞Ë®≠ÂÆö (15Áï™ÁõÆ„ÇÅ16Áï™ÁõÆ„ÅÆË¶ÅÁ¥†)
                # „É≠„Ç∞Ë®≠ÂÆö„ÅØÂõ∫ÂÆöÂÄ§„Çí‰ΩøÁî® - Áµ∂ÂØæ„Å´ÊñáÂ≠óÂàó„Å®boolean„Çí‰ΩøÁî®
                updates.append(gr.update(value=False))  # log_enabled (15)
                updates.append(gr.update(value="logs"))  # log_folder (16)
                
                # „É≠„Ç∞Ë®≠ÂÆö„Çí„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„Å´ÈÅ©Áî®
                default_log_settings = {
                    "log_enabled": False,
                    "log_folder": "logs"
                }

                # CONFIG QUEUEË®≠ÂÆö - NEW (17Áï™ÁõÆ„ÅÆË¶ÅÁ¥†)
                updates.append(gr.update(value=default_settings.get("add_timestamp_to_config", True)))  # 17
                
                # Ë®≠ÂÆö„Éï„Ç°„Ç§„É´„ÇíÊõ¥Êñ∞
                all_settings = load_settings()
                all_settings['log_settings'] = default_log_settings
                save_settings(all_settings)
                
                # „É≠„Ç∞Ë®≠ÂÆö„ÇíÈÅ©Áî® (Êó¢Â≠ò„ÅÆ„É≠„Ç∞„Éï„Ç°„Ç§„É´„ÇíÈñâ„Åò„Å¶„ÄÅË®≠ÂÆö„Å´Âæì„Å£„Å¶ÂÜçË®≠ÂÆö)
                disable_logging()  # Êó¢Â≠ò„ÅÆ„É≠„Ç∞„ÇíÈñâ„Åò„Çã
                
                # Ë®≠ÂÆöÁä∂ÊÖã„É°„ÉÉ„Çª„Éº„Ç∏ (18Áï™ÁõÆ„ÅÆË¶ÅÁ¥†)
                updates.append(translate("Ë®≠ÂÆö„Çí„Éá„Éï„Ç©„É´„Éà„Å´Êàª„Åó„Åæ„Åó„Åü"))
                
                return updates

    # ÂÆüË°åÂâç„ÅÆ„Éê„É™„Éá„Éº„Ç∑„Éß„É≥Èñ¢Êï∞
    def validate_and_process(*args):
        """ÂÖ•ÂäõÁîªÂÉè„Åæ„Åü„ÅØÊúÄÂæå„ÅÆ„Ç≠„Éº„Éï„É¨„Éº„É†ÁîªÂÉè„ÅÆ„ÅÑ„Åö„Çå„Åã„ÅåÊúâÂäπ„Åã„Å©„ÅÜ„Åã„ÇíÁ¢∫Ë™ç„Åó„ÄÅÂïèÈ°å„Åå„Å™„Åë„Çå„Å∞Âá¶ÁêÜ„ÇíÂÆüË°å„Åô„Çã"""
        # „Ç∞„É≠„Éº„Éê„É´Â§âÊï∞„ÅÆÂÆ£Ë®Ä
        global batch_stopped, queue_enabled, queue_type, prompt_queue_file_path, image_queue_files

        input_img = args[0]  # ÂÖ•Âäõ„ÅÆÊúÄÂàù„ÅåÂÖ•ÂäõÁîªÂÉè

        # UI„ÅÆ„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó„Å®ipsÈÖçÂàó (ÂÆüÈöõ„ÅÆipsÈÖçÂàó„ÅÆÈ†ÜÂ∫è):
        # [0]input_image, [1]prompt, [2]n_prompt, [3]seed, [4]total_second_length, [5]latent_window_size,
        # [6]steps, [7]cfg, [8]gs, [9]rs, [10]gpu_memory_preservation, [11]use_teacache, [12]use_random_seed,
        # [13]mp4_crf, [14]all_padding_value, [15]image_strength, [16]frame_size_radio, [17]keep_section_videos,
        # [18]lora_files, [19]lora_files2, [20]lora_files3, [21]lora_scales_text, [22]output_dir, [23]save_section_frames,
        # [24]use_all_padding, [25]use_lora, [26]lora_mode, [27]lora_dropdown1, [28]lora_dropdown2, [29]lora_dropdown3,
        # [30]save_tensor_data, [31]section_settings, [32]tensor_data_input, [33]fp8_optimization, [34]resolution,
        # [35]batch_count, [36]frame_save_mode, [37]use_queue, [38]prompt_queue_file, [39]save_settings_on_start, [40]alarm_on_completion
        
        # ÂêÑÂºïÊï∞„ÇíÊòéÁ§∫ÁöÑ„Å´ÂèñÂæó - „Ç≥„É°„É≥„Éà„Å´Âü∫„Å•„ÅÑ„Å¶Ê≠£Á¢∫„Å™„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Çí‰ΩøÁî®
        output_dir = args[22] if len(args) > 22 else None
        save_section_frames = args[23] if len(args) > 23 else False
        use_all_padding = args[24] if len(args) > 24 else False
        use_lora = args[25] if len(args) > 25 else False
        lora_mode = args[26] if len(args) > 26 else translate("„Éá„Ç£„É¨„ÇØ„Éà„É™„Åã„ÇâÈÅ∏Êäû")
        lora_dropdown1 = args[27] if len(args) > 27 else None
        lora_dropdown2 = args[28] if len(args) > 28 else None
        lora_dropdown3 = args[29] if len(args) > 29 else None
        save_tensor_data = args[30] if len(args) > 30 else False
        # F1Áâà„Åß„ÅØsection_settings„ÅØÂ∏∏„Å´Âõ∫ÂÆöÂÄ§„Çí‰ΩøÁî®ÔºàÁÑ°Âç∞Áâà„ÅÆÈÉ®ÂàÜ„ÅØ‰∏çË¶ÅÔºâ
        # F1ÁâàÁî®„ÅÆsection_settings - ‰∏ÄË≤´ÊÄß„ÅÆ„Åü„ÇÅ„Å´ÈÖçÂàó„Çí‰ΩúÊàê
        # section_settings„ÅåÂ≠òÂú®„Åô„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ„Åô„ÇãÔºàargs[31]Ôºâ
        section_settings = [[None, None, ""] for _ in range(50)]
        if len(args) > 31 and args[31] is not None:
            # „Åô„Åß„Å´ÈÖçÂàó„Å™„Çâ‰ΩøÁî®„ÄÅ„Åù„ÅÜ„Åß„Å™„Åë„Çå„Å∞ÂàùÊúüÂåñ„Åó„ÅüÈÖçÂàó„Çí‰ΩøÁî®
            if isinstance(args[31], list):
                section_settings = args[31]
        tensor_data_input = args[32] if len(args) > 32 else None
        fp8_optimization = args[33] if len(args) > 33 else True
        resolution_value = args[34] if len(args) > 34 else 640
        batch_count = args[35] if len(args) > 35 else 1
        frame_save_mode = args[36] if len(args) > 36 else translate("‰øùÂ≠ò„Åó„Å™„ÅÑ")
        # Êñ∞„Åó„ÅÑ„Ç≠„É•„ÉºÈñ¢ÈÄ£„ÅÆÂºïÊï∞„ÇíÂèñÂæó
        use_queue_ui = args[37] if len(args) > 37 else False
        prompt_queue_file_ui = args[38] if len(args) > 38 else None
        
        # Ëá™Âãï‰øùÂ≠ò„Éª„Ç¢„É©„Éº„É†Ë®≠ÂÆö„ÅÆÂºïÊï∞„ÇíÂèñÂæó
        save_settings_on_start_ui = args[39] if len(args) > 39 else False
        alarm_on_completion_ui = args[40] if len(args) > 40 else False
        
        # ÂÄ§„ÅÆÂèñÂæóÂá¶ÁêÜ
        actual_save_settings_value = save_settings_on_start_ui
        if hasattr(save_settings_on_start_ui, 'value'):
            actual_save_settings_value = save_settings_on_start_ui.value
        
        # „Ç¢„É©„Éº„É†Ë®≠ÂÆöÂÄ§„ÇíÂèñÂæó
        actual_alarm_value = False  # „Éá„Éï„Ç©„É´„Éà„ÅØ„Ç™„Éï
        
        # Gradio„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ„Åã„ÇâÂÄ§„ÇíÈÅ©Âàá„Å´ÂèñÂæó
        if isinstance(alarm_on_completion_ui, bool):
            # boolean„ÅÆÂ†¥Âêà„ÅØ„Åù„ÅÆ„Åæ„Åæ‰ΩøÁî®
            actual_alarm_value = alarm_on_completion_ui
        elif hasattr(alarm_on_completion_ui, 'value'):
            # Gradio„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„ÅÆÂ†¥Âêà„ÅØvalueÂ±ûÊÄß„ÇíÂèñÂæó
            if isinstance(alarm_on_completion_ui.value, bool):
                actual_alarm_value = alarm_on_completion_ui.value

        # „Ç≠„É•„ÉºË®≠ÂÆö„ÅÆÂá∫Âäõ
        print(translate("„Ç≠„É•„Éº„Çø„Ç§„Éó: {0}").format(queue_type))

        # „Ç≠„É•„ÉºÊ©üËÉΩ„ÅÆÁä∂ÊÖã„ÇíÊõ¥Êñ∞ÔºàUI„ÉÅ„Çß„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ„Åã„Çâ„ÅÆÂÄ§„ÇíÁõ¥Êé•ÂèçÊò†Ôºâ
        queue_enabled = use_queue_ui

        # section_settingsÂûã„ÉÅ„Çß„ÉÉ„ÇØ - „Ç®„É©„Éº‰øÆÊ≠£
        if len(args) > 31 and args[31] is not None and not isinstance(args[31], list):
            print(translate("section_settings„ÅåÊ≠£„Åó„ÅÑÂûã„Åß„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì: {0}. ÂàùÊúüÂåñ„Åó„Åæ„Åô„ÄÇ").format(type(args[31]).__name__))
            section_settings = [[None, None, ""] for _ in range(50)]

        # „Éê„ÉÉ„ÉÅÊï∞„ÅÆ‰∏äÈôê„ÇíË®≠ÂÆö
        batch_count = max(1, min(int(batch_count), 100))  # 1„Äú100„ÅÆÁØÑÂõ≤„Å´Âà∂Èôê

        # „Ç§„É°„Éº„Ç∏„Ç≠„É•„Éº„ÅÆÂ†¥Âêà„ÅØ„ÄÅ‰∫ãÂâç„Å´ÁîªÂÉè„Éï„Ç°„Ç§„É´„É™„Çπ„Éà„ÇíÊõ¥Êñ∞
        if queue_enabled and queue_type == "image":
            # ÂÖ•Âäõ„Éï„Ç©„É´„ÉÄ„Åã„ÇâÁîªÂÉè„Éï„Ç°„Ç§„É´„É™„Çπ„Éà„ÇíÊõ¥Êñ∞
            get_image_queue_files()
            image_queue_count = len(image_queue_files)
            print(translate("„Ç§„É°„Éº„Ç∏„Ç≠„É•„Éº‰ΩøÁî®: ÂÖ•Âäõ„Éï„Ç©„É´„ÉÄ„ÅÆÁîªÂÉè {0} ÂÄã„Çí‰ΩøÁî®„Åó„Åæ„Åô").format(image_queue_count))

            # „Éê„ÉÉ„ÉÅÊï∞„ÇíÁîªÂÉèÊï∞+1ÔºàÂÖ•ÂäõÁîªÂÉè„ÇíÂê´„ÇÄÔºâ„Å´Âêà„Çè„Åõ„Çã
            if image_queue_count > 0:
                # ÂÖ•ÂäõÁîªÂÉè„Çí‰Ωø„ÅÜ1Âõû + ÁîªÂÉè„Éï„Ç°„Ç§„É´ÂàÜ„ÅÆ„Éê„ÉÉ„ÉÅÊï∞
                total_needed_batches = 1 + image_queue_count

                # Ë®≠ÂÆö„Åï„Çå„Åü„Éê„ÉÉ„ÉÅÊï∞„Çà„ÇäÂøÖË¶ÅÊï∞„ÅåÂ§ö„ÅÑÂ†¥Âêà„ÅØË™øÊï¥
                if total_needed_batches > batch_count:
                    print(translate("ÁîªÂÉè„Ç≠„É•„ÉºÊï∞+1„Å´Âêà„Çè„Åõ„Å¶„Éê„ÉÉ„ÉÅÊï∞„ÇíËá™ÂãïË™øÊï¥: {0} ‚Üí {1}").format(batch_count, total_needed_batches))
                    batch_count = total_needed_batches

        # „Éó„É≠„É≥„Éó„Éà„Ç≠„É•„Éº„ÅÆÂ†¥Âêà„ÅØ„Éï„Ç°„Ç§„É´„ÅÆÂÜÖÂÆπ„ÇíÁ¢∫Ë™ç
        if queue_enabled and queue_type == "prompt":
            # „Ç∞„É≠„Éº„Éê„É´Â§âÊï∞„Åã„Çâ„Éï„Ç°„Ç§„É´„Éë„Çπ„ÇíÂèñÂæó
            if prompt_queue_file_path is not None:
                queue_file_path = prompt_queue_file_path
                print(translate("„Éó„É≠„É≥„Éó„Éà„Ç≠„É•„Éº„Éï„Ç°„Ç§„É´: {0}").format(queue_file_path))

                # „Éï„Ç°„Ç§„É´„Éë„Çπ„ÅåÊúâÂäπ„Åã„ÉÅ„Çß„ÉÉ„ÇØ
                if os.path.exists(queue_file_path):
                    print(translate("„Éó„É≠„É≥„Éó„Éà„Ç≠„É•„Éº„Éï„Ç°„Ç§„É´„ÅÆÂÜÖÂÆπ„ÇíË™≠„ÅøËæº„Åø„Åæ„Åô: {0}").format(queue_file_path))
                    try:
                        with open(queue_file_path, 'r', encoding='utf-8') as f:
                            lines = [line.strip() for line in f.readlines() if line.strip()]
                            queue_prompts_count = len(lines)
                            print(translate("ÊúâÂäπ„Å™„Éó„É≠„É≥„Éó„ÉàË°åÊï∞: {0}").format(queue_prompts_count))

                            if queue_prompts_count > 0:
                                # „Çµ„É≥„Éó„É´„Å®„Åó„Å¶ÊúÄÂàù„ÅÆÊï∞Ë°å„ÇíË°®Á§∫
                                sample_lines = lines[:min(3, queue_prompts_count)]
                                print(translate("„Éó„É≠„É≥„Éó„Éà„Çµ„É≥„Éó„É´: {0}").format(sample_lines))

                                # „Éê„ÉÉ„ÉÅÊï∞„Çí„Éó„É≠„É≥„Éó„ÉàÊï∞„Å´Âêà„Çè„Åõ„Çã
                                if queue_prompts_count > batch_count:
                                    print(translate("„Éó„É≠„É≥„Éó„ÉàÊï∞„Å´Âêà„Çè„Åõ„Å¶„Éê„ÉÉ„ÉÅÊï∞„ÇíËá™ÂãïË™øÊï¥: {0} ‚Üí {1}").format(batch_count, queue_prompts_count))
                                    batch_count = queue_prompts_count
                            else:
                                print(translate("„Éó„É≠„É≥„Éó„Éà„Ç≠„É•„Éº„Éï„Ç°„Ç§„É´„Å´ÊúâÂäπ„Å™„Éó„É≠„É≥„Éó„Éà„Åå„ÅÇ„Çä„Åæ„Åõ„Çì"))
                    except Exception as e:
                        print(translate("„Éó„É≠„É≥„Éó„Éà„Ç≠„É•„Éº„Éï„Ç°„Ç§„É´Ë™≠„ÅøËæº„Åø„Ç®„É©„Éº: {0}").format(str(e)))
                else:
                    print(translate("„Éó„É≠„É≥„Éó„Éà„Ç≠„É•„Éº„Éï„Ç°„Ç§„É´„ÅåÂ≠òÂú®„Åó„Å™„ÅÑ„ÅãÁÑ°Âäπ„Åß„Åô: {0}").format(queue_file_path))
            else:
                print(translate("„Éó„É≠„É≥„Éó„Éà„Ç≠„É•„ÉºÁÑ°Âäπ: „Éï„Ç°„Ç§„É´„ÅåÊ≠£„Åó„Åè„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì"))
        
        # Gradio„ÅÆ„É©„Ç∏„Ç™„Éú„Çø„É≥„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„ÅåÁõ¥Êé•Ê∏°„Åï„Çå„Å¶„ÅÑ„Çã„Åã„ÄÅÊñáÂ≠óÂàóÂÄ§„ÅåÊ∏°„Åï„Çå„Å¶„ÅÑ„Çã„Åã„ÇíÁ¢∫Ë™ç
        if hasattr(frame_save_mode, 'value'):
            # Gradio„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„ÅÆÂ†¥Âêà„ÅØÂÄ§„ÇíÂèñÂæó
            frame_save_mode_value = frame_save_mode.value
        else:
            # ÊñáÂ≠óÂàó„Å™„Å©„ÅÆÈÄöÂ∏∏„ÅÆÂÄ§„ÅÆÂ†¥Âêà„ÅØ„Åù„ÅÆ„Åæ„Åæ‰ΩøÁî®
            frame_save_mode_value = frame_save_mode
        
        # „Éï„É¨„Éº„É†‰øùÂ≠ò„É¢„Éº„Éâ„ÅØworkerÈñ¢Êï∞ÂÜÖ„ÅßÂá¶ÁêÜ„Åï„Çå„Çã„Åü„ÇÅ„ÄÅ„Åì„Åì„Åß„ÅÆË®≠ÂÆö„ÅØ‰∏çË¶Å
        # frame_save_mode „ÅØ workerÈñ¢Êï∞„Å´Áõ¥Êé•Ê∏°„Åï„Çå„Çã
        # „Éê„ÉÉ„ÉÅÂõûÊï∞„ÇíÊúâÂäπ„Å™ÁØÑÂõ≤„Å´Âà∂Èôê
        batch_count = max(1, min(int(batch_count), 100))

        # F1„É¢„Éº„Éâ„Åß„ÅØÂõ∫ÂÆö„ÅÆ„ÉÄ„Éü„Éº„Çª„ÇØ„Ç∑„Éß„É≥Ë®≠ÂÆö„Çí‰ΩøÁî®
        section_settings = [[None, None, ""] for _ in range(50)]

        # ÁèæÂú®„ÅÆÂãïÁîªÈï∑Ë®≠ÂÆö„Å®„Éï„É¨„Éº„É†„Çµ„Ç§„Ç∫Ë®≠ÂÆö„ÇíÊ∏°„Åô
        is_valid, error_message = validate_images(input_img, section_settings, length_radio, frame_size_radio)

        if not is_valid:
            # ÁîªÂÉè„ÅåÁÑ°„ÅÑÂ†¥Âêà„ÅØ„Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏„ÇíË°®Á§∫„Åó„Å¶ÁµÇ‰∫Ü
            yield None, gr.update(visible=False), translate("„Ç®„É©„Éº: ÁîªÂÉè„ÅåÈÅ∏Êäû„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì"), error_message, gr.update(interactive=True), gr.update(interactive=False), gr.update()
            return

        # ÁîªÂÉè„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØÈÄöÂ∏∏„ÅÆÂá¶ÁêÜ„ÇíÂÆüË°å
        # ÂÖÉ„ÅÆ„Éë„É©„É°„Éº„Çø„Çí‰ΩøÁî®
        new_args = list(args)
        
        # ÂºïÊï∞„ÇíÊ≠£„Åó„ÅÑ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÅßË®≠ÂÆö (LoRAÈñ¢ÈÄ£„Éë„É©„É°„Éº„ÇøËøΩÂä†„Å´‰º¥„ÅÑË™øÊï¥)
        if len(new_args) > 25:
            new_args[25] = use_lora  # use_lora„ÇíÁ¢∫ÂÆü„Å´Ê≠£„Åó„ÅÑÂÄ§„Å´
        if len(new_args) > 26:
            new_args[26] = lora_mode  # lora_mode„ÇíË®≠ÂÆö
        if len(new_args) > 27:
            new_args[27] = lora_dropdown1  # lora_dropdown1„ÇíË®≠ÂÆö
        if len(new_args) > 28:
            new_args[28] = lora_dropdown2  # lora_dropdown2„ÇíË®≠ÂÆö
        if len(new_args) > 29:
            new_args[29] = lora_dropdown3  # lora_dropdown3„ÇíË®≠ÂÆö
        # ===========================================================
        # ÈáçË¶Å: save_tensor_data„ÅØÊ≠£Á¢∫„Å´„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ30„Å´Ë®≠ÂÆö„Åô„Çã„Åì„Å®
        # ÂæåÁ∂ö„ÅÆ„Ç≥„Éº„Éâ„Åß„Åì„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Åå‰∏äÊõ∏„Åç„Åï„Çå„Å™„ÅÑ„Çà„ÅÜÊ≥®ÊÑè
        # ===========================================================
        if len(new_args) > 30:
            new_args[30] = save_tensor_data  # save_tensor_data„ÇíÁ¢∫ÂÆü„Å´Ê≠£„Åó„ÅÑÂÄ§„Å´
        
        # F1„É¢„Éº„Éâ„Åß„ÅØÂõ∫ÂÆö„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥Ë®≠ÂÆö„Çí‰ΩøÁî®
        if len(new_args) > 31:
            new_args[31] = section_settings
        
        # „Åù„ÅÆ‰ªñ„ÅÆÂºïÊï∞„ÇÇÂøÖË¶Å„Å´Âøú„Åò„Å¶Ë®≠ÂÆö
        if len(new_args) <= 37:  # ÂºïÊï∞„ÅÆÊúÄÂ§ß„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Å´Âêà„Çè„Åõ„Å¶Ë™øÊï¥
            # ‰∏çË∂≥„Åó„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅØÊã°Âºµ
            new_args.extend([None] * (37 - len(new_args)))
            if len(new_args) <= 31:
                if len(new_args) <= 30:
                    if len(new_args) <= 29:
                        # resolution„ÇÇ„Å™„ÅÑÂ†¥Âêà
                        new_args.append(resolution_value)  # resolution„ÇíËøΩÂä†
                    new_args.append(batch_count)  # batch_count„ÇíËøΩÂä†
        else:
            # Êó¢„Å´Â≠òÂú®„Åô„ÇãÂ†¥Âêà„ÅØÊõ¥Êñ∞
            # =============================================================================
            # ÈáçË¶Å: save_tensor_data(index 30)„ÅØ3507Ë°å„ÅßÊó¢„Å´Ë®≠ÂÆöÊ∏à„Åø„ÅÆ„Åü„ÇÅ„ÄÅ‰∏äÊõ∏„Åç„Åó„Å™„ÅÑ„Åì„Å®
            # ‰ª•Ââç„ÅØ„Åì„Åì„Åßnew_args[30] = batch_count„Å®„Å™„Å£„Å¶„Åä„Çä„ÄÅ„ÉÜ„É≥„ÇΩ„É´„Éá„Éº„Çø„ÅåÂ∏∏„Å´‰øùÂ≠ò„Åï„Çå„Çã
            # „Éê„Ç∞„ÅåÁô∫Áîü„Åó„Å¶„ÅÑ„Åü„ÄÇ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÇíÈñìÈÅï„Åà„Å™„ÅÑ„Çà„ÅÜÊ≥®ÊÑè„Åô„Çã„Åì„Å®„ÄÇ
            # =============================================================================
            new_args[34] = resolution_value  # resolution
            new_args[35] = batch_count  # batch_count
            # save_tensor_data„ÅØ‰∏äÈÉ®„ÅßÊó¢„Å´Ë®≠ÂÆöÊ∏à„Åø (new_args[30])
            new_args[36] = frame_save_mode  # frame_save_mode
            new_args[37] = use_queue_ui  # use_queue
            new_args[38] = prompt_queue_file_ui  # prompt_queue_file
            new_args[39] = actual_save_settings_value  # save_settings_on_start
            new_args[40] = actual_alarm_value  # alarm_on_completion

        # processÈñ¢Êï∞„Å´Ê∏°„ÅôÂâç„Å´ÈáçË¶Å„Å™ÂÄ§„ÇíÁ¢∫Ë™ç
        # Ê≥®ÊÑè: „Åì„Åì„Åß„ÅØ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ25„Å®Êõ∏„Åã„Çå„Å¶„ÅÑ„Åæ„Åô„Åå„ÄÅ„Åì„Çå„ÅØË™§„Çä„Åß„Åô
        # Ê≠£„Åó„Åè„ÅØnew_args[30]„Ååsave_tensor_data„ÅÆÂÄ§„Åß„Åô
        
        # new_args„ÅÆÂºïÊï∞„ÇíÂá∫ÂäõÔºàÁâπ„Å´section_settingsÔºâ
        # section_settings„ÅØÈÖçÂàó„Åß„ÅÇ„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç
        section_settings_index = 31  # section_settings„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ
        if len(new_args) > section_settings_index:
            if not isinstance(new_args[section_settings_index], list):
                print(translate("section_settings„Åå„É™„Çπ„Éà„Åß„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ‰øÆÊ≠£„Åó„Åæ„Åô„ÄÇ"))
                new_args[section_settings_index] = [[None, None, ""] for _ in range(50)]

        # processÈñ¢Êï∞„ÅÆ„Ç∏„Çß„Éç„É¨„Éº„Çø„ÇíËøî„Åô
        yield from process(*new_args)

    # Ë®≠ÂÆö‰øùÂ≠ò„Éú„Çø„É≥„ÅÆ„ÇØ„É™„ÉÉ„ÇØ„Ç§„Éô„É≥„Éà
    save_current_settings_btn.click(
        fn=save_app_settings_handler,
        inputs=[
            resolution,
            mp4_crf,
            steps,
            cfg,
            use_teacache,
            gpu_memory_preservation,
            gs,
            image_strength,
            keep_section_videos,
            save_section_frames,
            save_tensor_data,
            frame_save_mode,
            save_settings_on_start,
            alarm_on_completion,
            # „É≠„Ç∞Ë®≠ÂÆö„ÇíËøΩÂä†
            log_enabled,
            log_folder,
            # ADD CONFIG QUEUE SETTING
            config_queue_components['add_timestamp_to_config']  # NEW INPUT
        ],
        outputs=[settings_status]
    )

    # Ë®≠ÂÆö„É™„Çª„ÉÉ„Éà„Éú„Çø„É≥„ÅÆ„ÇØ„É™„ÉÉ„ÇØ„Ç§„Éô„É≥„Éà
    reset_settings_btn.click(
        fn=reset_app_settings_handler,
        inputs=[],
        outputs=[
            resolution,           # 1
            mp4_crf,              # 2
            steps,                # 3
            cfg,                  # 4
            use_teacache,         # 5
            gpu_memory_preservation, # 6
            gs,                   # 7
            image_strength,       # 8
            keep_section_videos,  # 9
            save_section_frames,  # 10
            save_tensor_data,     # 11
            frame_save_mode,      # 12
            save_settings_on_start, # 13
            alarm_on_completion,  # 14
            log_enabled,          # 15
            log_folder,           # 16
            config_queue_components['add_timestamp_to_config'], # 17 - NEW OUTPUT
            settings_status       # 18
        ]
    )

    # ÂÆüË°å„Éú„Çø„É≥„ÅÆ„Ç§„Éô„É≥„Éà
    # ===================================================================================================
    # ÈáçË¶Å: ipsÈÖçÂàó„ÅÆÂºïÊï∞„ÅÆÈ†ÜÂ∫è„Å®„ÄÅvalidate_and_process/process/workerÈñ¢Êï∞„ÅÆÂºïÊï∞„ÅÆÈ†ÜÂ∫è„ÇíÊ≠£Á¢∫„Å´‰∏ÄËá¥„Åï„Åõ„Çã
    # „Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÇíÂ§âÊõ¥„Åô„ÇãÂ†¥Âêà„ÅØ„ÄÅÂÖ®„Å¶„ÅÆÈñ¢ÈÄ£ÁÆáÊâÄÔºàvalidate_and_processÂÜÖ„ÅÆÂá¶ÁêÜÔºâ„ÇÇÂêà„Çè„Åõ„Å¶Â§âÊõ¥„Åô„Çã„Åì„Å®
    # Áâπ„Å´ÈáçË¶Å: [30]save_tensor_data„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÅØÂ§âÊõ¥„Åó„Å™„ÅÑ„Åì„Å®„ÄÇÂ§âÊõ¥„Åô„Çã„Å®Ë™§‰ΩúÂãï„ÅÆÂéüÂõ†„Å®„Å™„Çä„Åæ„Åô„ÄÇ
    # 5/13‰øÆÊ≠£: save_tensor_data(„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ30)„ÅØ„Éê„ÉÉ„ÉÅ„Ç´„Ç¶„É≥„Éà„Å´‰∏äÊõ∏„Åç„Åï„Çå„ÇãÂïèÈ°å„Çí‰øÆÊ≠£„Åó„Åæ„Åó„Åü„ÄÇ
    # ===================================================================================================
    # Ê≥®ÊÑè: ‰ª•‰∏ã„ÅåÂÆüÈöõ„ÅÆipsÈÖçÂàó„ÅÆÈ†ÜÂ∫è„Åß„Åô
    #  [0]input_image, [1]prompt, [2]n_prompt, [3]seed, [4]total_second_length, [5]latent_window_size,
    #  [6]steps, [7]cfg, [8]gs, [9]rs, [10]gpu_memory_preservation, [11]use_teacache, [12]use_random_seed,
    #  [13]mp4_crf, [14]all_padding_value, [15]image_strength, [16]frame_size_radio, [17]keep_section_videos,
    #  [18]lora_files, [19]lora_files2, [20]lora_files3, [21]lora_scales_text, [22]output_dir, [23]save_section_frames,
    #  [24]use_all_padding, [25]use_lora, [26]lora_mode, [27]lora_dropdown1, [28]lora_dropdown2, [29]lora_dropdown3,
    #  [30]save_tensor_data, [31]section_settings, [32]tensor_data_input, [33]fp8_optimization, [34]resolution,
    #  [35]batch_count, [36]frame_save_mode, [37]use_queue, [38]prompt_queue_file, [39]save_settings_on_start, [40]alarm_on_completion
    ips = [input_image, prompt, n_prompt, seed, total_second_length, latent_window_size, steps, cfg, gs, rs, gpu_memory_preservation, use_teacache, use_random_seed, mp4_crf, all_padding_value, image_strength, frame_size_radio, keep_section_videos, lora_files, lora_files2, lora_files3, lora_scales_text, output_dir, save_section_frames, use_all_padding, use_lora, lora_mode, lora_dropdown1, lora_dropdown2, lora_dropdown3, save_tensor_data, section_settings, tensor_data_input, fp8_optimization, resolution, batch_count, frame_save_mode, use_queue, prompt_queue_file, save_settings_on_start, alarm_on_completion]

    start_button.click(fn=validate_and_process_with_queue_check, inputs=ips, outputs=[result_video, preview_image, progress_desc, progress_bar, start_button, end_button, queue_start_button, seed])
    end_button.click(fn=end_process_enhanced, outputs=[end_button,queue_start_button])

    # F1„É¢„Éº„Éâ„Åß„ÅØ„Çª„ÇØ„Ç∑„Éß„É≥Ê©üËÉΩ„Å®„Ç≠„Éº„Éï„É¨„Éº„É†„Ç≥„Éî„ÉºÊ©üËÉΩ„ÇíÂâäÈô§Ê∏à„Åø

    # Ê≥®: create_single_keyframe_handlerÈñ¢Êï∞„ÅØ„Éï„É¨„Éº„É†„Çµ„Ç§„Ç∫„ÇÑÂãïÁîªÈï∑„Å´Âü∫„Å•„ÅÑ„ÅüÂãïÁöÑ„Çª„ÇØ„Ç∑„Éß„É≥Êï∞„ÇíË®àÁÆó„Åó„Åæ„Åô
    # UI„Åß„Éï„É¨„Éº„É†„Çµ„Ç§„Ç∫„ÇÑÂãïÁîªÈï∑„ÇíÂ§âÊõ¥„Åô„Çã„Å®„ÄÅÂãïÁöÑ„Å´Ë®àÁÆó„Åï„Çå„Åü„Çª„ÇØ„Ç∑„Éß„É≥Êï∞„Å´Âæì„Å£„Å¶„Ç≥„Éî„ÉºÂá¶ÁêÜ„ÅåË°å„Çè„Çå„Åæ„Åô

    # „Éó„É™„Çª„ÉÉ„Éà‰øùÂ≠ò„Éú„Çø„É≥„ÅÆ„Ç§„Éô„É≥„Éà
    def save_button_click_handler(name, prompt_text):
        """‰øùÂ≠ò„Éú„Çø„É≥„ÇØ„É™„ÉÉ„ÇØÊôÇ„ÅÆ„Éè„É≥„Éâ„É©Èñ¢Êï∞"""

        # ÈáçË§á„ÉÅ„Çß„ÉÉ„ÇØ„Å®Ê≠£Ë¶èÂåñ
        if "A character" in prompt_text and prompt_text.count("A character") > 1:
            sentences = prompt_text.split(".")
            if len(sentences) > 0:
                prompt_text = sentences[0].strip() + "."
                # ÈáçË§á„ÇíÊ§úÂá∫„Åó„Åü„Åü„ÇÅÊ≠£Ë¶èÂåñ

        # „Éó„É™„Çª„ÉÉ„Éà‰øùÂ≠ò
        result_msg = save_preset(name, prompt_text)

        # „Éó„É™„Çª„ÉÉ„Éà„Éá„Éº„Çø„ÇíÂèñÂæó„Åó„Å¶„Éâ„É≠„ÉÉ„Éó„ÉÄ„Ç¶„É≥„ÇíÊõ¥Êñ∞
        presets_data = load_presets()
        choices = [preset["name"] for preset in presets_data["presets"]]
        default_presets = [n for n in choices if any(p["name"] == n and p.get("is_default", False) for p in presets_data["presets"])]
        user_presets = [n for n in choices if n not in default_presets]
        sorted_choices = [(n, n) for n in sorted(default_presets) + sorted(user_presets)]

        # „É°„Ç§„É≥„Éó„É≠„É≥„Éó„Éà„ÅØÊõ¥Êñ∞„Åó„Å™„ÅÑÔºà‰øùÂ≠ò„ÅÆ„Åø„ÇíË°å„ÅÜÔºâ
        return result_msg, gr.update(choices=sorted_choices), gr.update()

    # ‰øùÂ≠ò„Éú„Çø„É≥„ÅÆ„ÇØ„É™„ÉÉ„ÇØ„Ç§„Éô„É≥„Éà„ÇíÊé•Á∂ö
    save_btn.click(
        fn=save_button_click_handler,
        inputs=[edit_name, edit_prompt],
        outputs=[result_message, preset_dropdown, prompt]
    )

    # „ÇØ„É™„Ç¢„Éú„Çø„É≥Âá¶ÁêÜ
    def clear_fields():
        return gr.update(value=""), gr.update(value="")

    clear_btn.click(
        fn=clear_fields,
        inputs=[],
        outputs=[edit_name, edit_prompt]
    )

    # „Éó„É™„Çª„ÉÉ„ÉàË™≠ËæºÂá¶ÁêÜ
    def load_preset_handler(preset_name):
        # „Éó„É™„Çª„ÉÉ„ÉàÈÅ∏ÊäûÊôÇ„Å´Á∑®ÈõÜÊ¨Ñ„ÅÆ„Åø„ÇíÊõ¥Êñ∞
        for preset in load_presets()["presets"]:
            if preset["name"] == preset_name:
                return gr.update(value=preset_name), gr.update(value=preset["prompt"])
        return gr.update(), gr.update()

    # „Éó„É™„Çª„ÉÉ„ÉàÈÅ∏ÊäûÊôÇ„Å´Á∑®ÈõÜÊ¨Ñ„Å´ÂèçÊò†
    def load_preset_handler_wrapper(preset_name):
        # „Éó„É™„Çª„ÉÉ„ÉàÂêç„Åå„Çø„Éó„É´„ÅÆÂ†¥Âêà„ÇÇÂá¶ÁêÜ„Åô„Çã
        if isinstance(preset_name, tuple) and len(preset_name) == 2:
            preset_name = preset_name[1]  # ÂÄ§ÈÉ®ÂàÜ„ÇíÂèñÂæó
        return load_preset_handler(preset_name)

    preset_dropdown.change(
        fn=load_preset_handler_wrapper,
        inputs=[preset_dropdown],
        outputs=[edit_name, edit_prompt]
    )

    # ÂèçÊò†„Éú„Çø„É≥Âá¶ÁêÜ - Á∑®ÈõÜÁîªÈù¢„ÅÆÂÜÖÂÆπ„Çí„É°„Ç§„É≥„Éó„É≠„É≥„Éó„Éà„Å´ÂèçÊò†
    def apply_to_prompt(edit_text):
        """Á∑®ÈõÜÁîªÈù¢„ÅÆÂÜÖÂÆπ„Çí„É°„Ç§„É≥„Éó„É≠„É≥„Éó„Éà„Å´ÂèçÊò†„Åô„ÇãÈñ¢Êï∞"""
        # Á∑®ÈõÜÁîªÈù¢„ÅÆ„Éó„É≠„É≥„Éó„Éà„Çí„É°„Ç§„É≥„Å´ÈÅ©Áî®
        return gr.update(value=edit_text)

    # „Éó„É™„Çª„ÉÉ„ÉàÂâäÈô§Âá¶ÁêÜ
    def delete_preset_handler(preset_name):
        # „Éó„É™„Çª„ÉÉ„ÉàÂêç„Åå„Çø„Éó„É´„ÅÆÂ†¥Âêà„ÇÇÂá¶ÁêÜ„Åô„Çã
        if isinstance(preset_name, tuple) and len(preset_name) == 2:
            preset_name = preset_name[1]  # ÂÄ§ÈÉ®ÂàÜ„ÇíÂèñÂæó

        result = delete_preset(preset_name)

        # „Éó„É™„Çª„ÉÉ„Éà„Éá„Éº„Çø„ÇíÂèñÂæó„Åó„Å¶„Éâ„É≠„ÉÉ„Éó„ÉÄ„Ç¶„É≥„ÇíÊõ¥Êñ∞
        presets_data = load_presets()
        choices = [preset["name"] for preset in presets_data["presets"]]
        default_presets = [name for name in choices if any(p["name"] == name and p.get("is_default", False) for p in presets_data["presets"])]
        user_presets = [name for name in choices if name not in default_presets]
        sorted_names = sorted(default_presets) + sorted(user_presets)
        updated_choices = [(name, name) for name in sorted_names]

        return result, gr.update(choices=updated_choices)

    # F1„É¢„Éº„Éâ„Åß„ÅØ„Ç≠„Éº„Éï„É¨„Éº„É†„Ç≥„Éî„ÉºÊ©üËÉΩ„ÇíÂâäÈô§Ê∏à„Åø
    
    # =============================================================================
    # SETUP CONFIG QUEUE EVENT HANDLERS - MOVED INSIDE BLOCKS CONTEXT
    # =============================================================================
    
    # Setup config queue event handlers
    ui_components = {
        'input_image': input_image,
        'prompt': prompt,
        'use_lora': use_lora,
        'lora_mode': lora_mode,
        'lora_dropdown1': lora_dropdown1,
        'lora_dropdown2': lora_dropdown2,
        'lora_dropdown3': lora_dropdown3,
        'lora_files': lora_files,
        'lora_files2': lora_files2,
        'lora_files3': lora_files3,
        'lora_scales_text': lora_scales_text,
        'progress_desc': progress_desc,
        'progress_bar': progress_bar,
        'preview_image': preview_image,
        'result_video': result_video
    }

    setup_enhanced_config_queue_events(config_queue_components, ui_components)

    apply_preset_btn.click(
        fn=apply_to_prompt,
        inputs=[edit_prompt],
        outputs=[prompt]
    )

    delete_preset_btn.click(
        fn=delete_preset_handler,
        inputs=[preset_dropdown],
        outputs=[result_message, preset_dropdown]
    )

# F1„É¢„Éº„Éâ„Åß„ÅØ„Ç≠„Éº„Éï„É¨„Éº„É†„Ç≥„Éî„ÉºÊ©üËÉΩ„ÇíÂâäÈô§Ê∏à„Åø

allowed_paths = [os.path.abspath(os.path.realpath(os.path.join(os.path.dirname(__file__), './outputs')))]

# Ëµ∑Âãï„Ç≥„Éº„Éâ
try:
    block.launch(
        server_name=args.server,
        server_port=args.port,
        share=args.share,
        allowed_paths=allowed_paths,
        inbrowser=args.inbrowser,
    )
except OSError as e:
    if "Cannot find empty port" in str(e):
        print("======================================================")
        print(translate("„Ç®„É©„Éº: FramePack-eichi„ÅØÊó¢„Å´Ëµ∑Âãï„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ"))
        print(translate("ÂêåÊôÇ„Å´Ë§áÊï∞„ÅÆ„Ç§„É≥„Çπ„Çø„É≥„Çπ„ÇíÂÆüË°å„Åô„Çã„Åì„Å®„ÅØ„Åß„Åç„Åæ„Åõ„Çì„ÄÇ"))
        print(translate("ÁèæÂú®ÂÆüË°å‰∏≠„ÅÆ„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÇíÂÖà„Å´ÁµÇ‰∫Ü„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"))
        print("======================================================")
        input(translate("Á∂öË°å„Åô„Çã„Å´„ÅØ‰Ωï„Åã„Ç≠„Éº„ÇíÊäº„Åó„Å¶„Åè„Å†„Åï„ÅÑ..."))
    else:
        # „Åù„ÅÆ‰ªñ„ÅÆOSError„ÅÆÂ†¥Âêà„ÅØÂÖÉ„ÅÆ„Ç®„É©„Éº„ÇíË°®Á§∫
        print(translate("„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {e}").format(e=e))
        input(translate("Á∂öË°å„Åô„Çã„Å´„ÅØ‰Ωï„Åã„Ç≠„Éº„ÇíÊäº„Åó„Å¶„Åè„Å†„Åï„ÅÑ..."))
